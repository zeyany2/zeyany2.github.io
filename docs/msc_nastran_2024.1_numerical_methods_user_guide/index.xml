<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DOC REPO – MSC Nastran manuals</title>
    <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/</link>
    <description>Recent content in MSC Nastran manuals on DOC REPO</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	  <atom:link href="//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>User&#39;s Manual P1</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_001/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_001/</guid>
      <description>
        
        
        &lt;p&gt;MSC Nastran 2024.1
Numerical Methods User’s Guide
(Last updated for version 2001)MSC Nastran Numerical Methods User’s Guide&lt;/p&gt;
&lt;p&gt;Worldwide Web
&lt;a href=&#34;https://www.hexagon.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.hexagon.com&lt;/a&gt;
Support
&lt;a href=&#34;https://simcompanion.hexagon.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://simcompanion.hexagon.com&lt;/a&gt;
Disclaimer
Hexagon reserves the right to make changes in specifications and other information contained in this document without prior notice.
The concepts, methods, and examples presented in this text are for illustrative and educational purposes only, and are not intended
to be exhaustive or to apply to any particular engineering problem or design. Hexagon assumes no liability or responsibility to any
person or company for direct or indirect damages resulting from the use of any information contained herein.
User Documentation: Copyright 2024 Hexagon AB and/or its subsidiaries. All Rights Reserved.
This notice shall be marked on any reproduction of this documentation, in whole or in part. Any reproduction or distribution of this
document, in whole or in part, without the prior written consent of Hexagon is prohibited.
This software may contain certain third-party software that is protected by copyright and licensed from Hexagon suppliers. Additional
terms and conditions and/or notices may apply for certain third party software. Such additional third party software terms and
conditions and/or notices may be set forth in documentation and/or at third-party software information  (or successor website designated
by Hexagon from time to time).
PCGLSS 8.0, Copyright © 1992-2016, Computational Applications and System Integration Inc. All rights reserved. PCGLSS 8.0 is
licensed from Computational Applications and System Integration Inc.
The Hexagon logo, Hexagon, MSC Software logo, MSC, Dytran, Marc, MSC Nastran, Patran, e -Xstream, Digimat, and Simulating
Reality are trademarks or registered trademarks of Hexagon AB and/or its subsidiaries in the United States and/or other countries.
NASTRAN is a registered trademark of NASA. FLEXlm and FlexNet Publisher are trademarks or registered trademarks of Flexera
Software. All other trademarks are the property of their respective owners.
Use, duplicate, or disclosure by the U.S. Government is subjected to restrictions as set forth in FAR 12.212 (Commercial Computer
Software) and DFARS 227.7202 (Commercial Computer Software and Commercial Computer Software Documentation), as
applicable.
U.S. Patent 9,361,413
May 16, 2024
NA:V2024.1:Z:Z:Z:DC-NUMERICAL-PDFCorporate Office
Hexagon Manufacturing Intelligence, Inc. UK
78 Portsmouth Road Cedar House
Cobham, Surrey KT11 1HY
Telephone: (+44) 02070686555&lt;/p&gt;
&lt;p&gt;Documentation Feedback
At Hexagon Manufacturing Intelligence, we strive to produce the highest quality documentation and
welcome your feedback. If you have comments or suggestions about our documentation, write to us .
Please include the following information with your feedback:
Document name
Release/Version number
Chapter/Section name
Topic title (for Online Help)
Brief description of the content (for example, incomplete/incorrect information, grammatical
errors, information that requires clarification or more details and so on).
Your suggestions for correcting/improving documentation
Note:   The above mentioned e-mail address is only for providing documentation specific
feedback. If you have any technical problems, issues, or queries, please contact Technical
Support .&lt;/p&gt;
&lt;p&gt;C o n t e n t s
MSC Nastran Numerical Method User’s Guide
Contents
Main Index
Contents
Preface
About this Book  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12
Introduction  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12
Using This Guide  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  12
List of MSC Nastran Guides . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13
Technical Support  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  14
Training and Internet Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  14
1 Utility Tools and Functions
Utility Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16
System Cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  16
Diagnostic (DIAG) Flags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19
Matrix Trailers  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  20
Indexed Matrix File Structure  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  23
Kernel Functions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  24
Timing Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  26
Time Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  28
Storage Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  29
Performance Analysis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  30
Parallel Processing  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  31
2 Matrix Multiply-Add Module
Multiply-Add Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  34
Theory of Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  34
Method One (Dense x Dense)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  35
Method Two (Sparse x Dense)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  38
Method Three (Sparse x Sparse) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  39
Method Four (Dense x Sparse)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  40&lt;/p&gt;
&lt;p&gt;MSC Nastran Numerical Method User’s Guide6
Main Index
Sparse Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  41
Triple Multiply Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  42
Parallel Multiply Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  45
MPYAD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  46
DMAP User Interface  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  47
Method Selection/Deselection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  48
Automatic Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  48
Automatic Deselection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  48
User Deselection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49
User Selection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  50
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51
Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51
Performance Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  51
Submethod Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  52
Error Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53
MPYAD Estimates and Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53
3 Matrix Decomposition
Decomposition Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  56
Theory of Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57
Symmetric Decomposition Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57
Mathematical Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  57
Symbolic Phase  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  58
Numeric Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59
Numerical Reliability of Symmetric Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  60
Partial Decomposition  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  61
User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  61
Method Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  63
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  64
Minimum Front Option  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  64
Reordering Options  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  64
Compression Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  64
Perturbation Option  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  64
High Rank Options  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  65
Diagnostic Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  65
Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  66
Numerical Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  66
Performance Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  67
Statistical Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  68
Error Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  68&lt;/p&gt;
&lt;p&gt;7 Contents
Main Index
Decomposition Estimates and Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69
References  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  70
4 Direct Solution of Linear Systems
Solution Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  74
Theory of Forward-Backward Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75
Right-Handed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75
Left-Handed Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75
Sparse Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  75
Parallel Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  76
User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  76
Method Selection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77
FBS Method Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77
Right-handed FBS Options  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  77
Left-handed FBS Option  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  78
Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  78
Numerical Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  78
Performance Messages  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79
Error Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79
FBS Estimates and Requirements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80
Sparse FBS Estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80
References  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  80
5 Iterative Solution of Systems of Linear Systems
Iterative Solver Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82
Theory of the Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82
Convergence Control  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  83
Block Conjugate Gradient Method (BIC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  84
Real and Complex BIC  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85
Preconditioning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  88
Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  89
Numerical Reliability of Equation Solutions  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  89
User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  90
Iterative Method Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  92
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  93&lt;/p&gt;
&lt;p&gt;MSC Nastran Numerical Method User’s Guide8
Main Index
Preconditioner Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  93
Convergence Criterion Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  94
Diagnostic Output Option  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95
Incomplete Cholesky Density Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95
Extraction Level Option for Incomplete Cholesky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  96
Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  96
Iterative Solution Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97
Accuracy Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  97
Performance Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  99
Iterative Solver Estimates and Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  100
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  102
6 Real Symmetric Eigenvalue Analysis
Real Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  104
Theory of Real Eigenvalue Analysis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  104
Reduction (Tridiagonal) Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  106
Solution Method Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147
DMAP User Interface  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147
Method Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  150
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151
Normalization Options  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151
Frequency and Mode Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  151
Performance Options  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  152
Miscellaneous Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  153
Mass Matrix Analysis Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  154
Real Symmetric Eigenvalue Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155
Execution Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155
Table of Shifts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155
Numerical Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  157
Performance Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  158
Lanczos Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  159
Real Lanczos Estimates and Requirements  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  161
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  162
7 Complex Eigenvalue Analysis
Damped Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  164
Theory of Complex Eigenvalue Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  164
Canonical Transformation to Mathematical Form  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  164
Dynamic Matrix Multiplication  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  169&lt;/p&gt;
&lt;p&gt;9 Contents
Main Index
Physical Solution Diagnosis  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  171
Hessenberg Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  172
QR Iteration Using the Householder Matrices  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  177
Eigenvector Computation  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  180
The Complex Lanczos Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  183
The Single Vector Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  184
The Adaptive, Block Lanczos Method  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  195
Singular Value Decomposition (SVD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  204
Solution Method Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  204
User Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  206
Method Selection  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  207
Option Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  209
Normalization Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  210
Hessenberg and Lanczos Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  210
Complex Eigenvalue Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  212
Hessenberg Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  212
Complex Lanczos Internal Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  213
Performance Diagnostics  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  216
Complex Lanczos Estimates and Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  218
References  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219
Glossary
Bibliography&lt;/p&gt;
&lt;p&gt;MSC Nastran Numerical Method User’s Guide10
Main Index&lt;/p&gt;
&lt;p&gt;Main Index
Preface
Preface
About this Book      12
List of MSC Nastran Guides      13
Technical Support     14
Training and Internet Resources      14&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
About this Book12
About this Book
MSC Nastran is a general-purpose finite element program which solves a wide variety of engineering
problems. MSC Nastran is developed, marketed, and supported by Hexagon AB and/or its subsidiaries.
The MSC Nastran Numerical Methods User&amp;rsquo;s Guide  is intended to help users of the program choose among
the different numerical methods and to tune these methods for optimal performance. Another goal for this
guide is to provide information about the accuracy, time, and space requirements of these methods.
This edition covers the major numerical methods presented in MSC Nastran Version 2001. The main new
material in this guide covers parallel execution of the eigenvalue analysis for the fast solution of normal modes
analyses.
The major change of this edition is related to the elimination of the descriptions of old methods such as the
active column decomposition or the inverse power method of eigenvalue analysis. As in each edition, several
known typographical errors from the earlier editions were corrected and some of the comments were clarified.
This guide will continue to be revised to reflect future enhancements to MSC Nastran&amp;rsquo;s numerical methods.
Therefore, readers who would like to suggest changes or additions are encouraged to send them to me.
Introduction
This guide is designed to assist the users of MSC Nastran with the method selection and time estimations for
the most important numerical modules. The guide is separated into seven chapters:
Utility Tools and Functions , 17
Matrix Multiply-Add Module, 33
Matrix Decomposition, 55
Direct Solution of Linear Systems, 73
Iterative Solution of Systems of Linear Systems, 81
Real Symmetric Eigenvalue Analysis, 103
Complex Eigenvalue Analysis, 163
These topics are selected because they have the biggest impact on the performance of MSC Nastran. To
obtain the most accurate solutions, an analyst should read this guide carefully. Some of the numerical
solutions exhibit different characteristics with different problems. This guide provides tools and
recommendations for the user regarding how to select the best solution.
Using This Guide
The reader of this guide should be familiar with the basic structure of MSC Nastran. Experience with the
methods of linear statics and normal modes is helpful. A first-time reader of this guide should read Chapter
1 to become familiar with the utility tools and functions. After that it is possible to move directly to the
chapters containing the topic the user is trying to apply and tune (see Chapters 2 through 7). Each chapter
contains general time estimates and performance analysis information as well as resource estimation formulae
for some of the methods discussed in the chapter.&lt;/p&gt;
&lt;p&gt;13 Preface
List of MSC Nastran Guides
Since this guide also discusses the theory of numerical methods, it is a stand-alone document except for a few
references to the MSC Nastran Quick Reference Guide .
List of MSC Nastran Guides
A list of some of the MSC Nastran guides is as follows:
Installation and Release Guides
Installation and Operations Guide
Release Guide
Reference Guides
Quick Reference Guide
DMAP Programmer’s Guide
Reference Guide
Utilities Guide
Getting Started Guide
SOL 400 Getting Started Guide
MSC Nastran Error Messages Guide
Demonstration Guides
Linear Analysis
Implicit Nonlinear (SOL 400)
Explicit Nonlinear (SOL 700)
MSC Nastran Verification Guide
User’s Guides
Automated Component Modal Synthesis (ACMS)
Access Manual
Aeroelastic Analysis
Design Sensitivity and Optimization
DEMATD
Dynamic Analysis
Embedded Fatigue
Embedded Vibration Fatigue
Explicit Nonlinear (SOL 700)
High Performance Computing
Linear Static Analysis
Nonlinear (SOL 400)&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Technical Support14
You may find any of these documents from Hexagon at:
&lt;a href=&#34;https://simcompanion.hexagon.com/customers/s/article/MSC-Nastran-Support-Home-Page&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://simcompanion.hexagon.com/customers/s/article/MSC-Nastran-Support-Home-Page&lt;/a&gt;
Technical Support
For technical support phone numbers and contact information, please visit:
&lt;a href=&#34;https://simcompanion.hexagon.com/customers/s/article/support-contact-information-kb8019304&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://simcompanion.hexagon.com/customers/s/article/support-contact-information-kb8019304&lt;/a&gt;
Support Center ( &lt;a href=&#34;http://simcompanion.hexagon.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://simcompanion.hexagon.com&lt;/a&gt;)
The SimCompanion link above gives you access to the wealth of resources for Hexagon products. Here you
will find product and support contact information, product documentations, knowledge base articles,
product error list, knowledge base articles and SimAcademy Webinars. It is a searchable database which allows
you to find articles relevant to your inquiry. Valid MSC customer entitlement and login is required to access
the database and documents. It is a single sign-on that gives you access to product documentation for
complete list of products from Hexagon, allows you to manage your support cases, and participate in our
discussion forums.
Training and Internet Resources
The Hexagon corporate site has the information on the latest events, products, and services for the
CAD/CAE/CAM marketplace.
Design and Engineering e-Learning
The above link will point you to schedule and description of seminars. Following courses are recommended
for beginning MSC Nastran users.
NAS120 - Linear Static Analysis using MSC Nastran and Patran
This seminar introduces basic finite element analysis techniques for linear static, normal modes, and buckling
analysis of structures using MSC Nastran and Patran. MSC Nastran data structure, the element library,
modeling practices, model validation, and guidelines for efficient solutions are discussed and illustrated with
examples and workshops. Patran will be an integral part of the examples and workshops and will be used to
generate and verify illustrative MSC Nastran models, manage analysis submission requests, and visualize
results. This seminar provides the foundation required for intermediate and advanced MSC Nastran
applications.Numerical Methods
Rotordynamics
Superelements and Modules
Thermal Analysis
User Defined Services&lt;/p&gt;
&lt;p&gt;Main Index
Chapter 1: Utility Tools and Functions
1 Utility Tools and Functions
Utility Tools
System Cells
Diagnostic (DIAG) Flags
Matrix Trailers
Kernel Functions
Timing Constants
Time Estimates
Storage Requirements
Performance Analysis
Parallel Processing&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions18
Utility Tools
In this chapter the following utility tools are discussed:
System cells
DIAG flags
Matrix trailers
Kernel functions
Timing constants
Since these utilities are of a general nature, they are used in the same way on different computers and solution
sequences. They are also used to select certain numerical methods and request diagnostics information and
timing data. For these reasons, the utility tools are overviewed here before any specific numerical method is
discussed.
System Cells
One of the most important communication utilities in MSC Nastran is the SYSTEM common block.&lt;br&gt;
Elements of this common block are called system cells . Some of the system cells have names associated with
them.  In those cases, the system  cell can be referred to by this name (commonly called a keyword).&lt;br&gt;
Performance Cells .  Some of the system cells related to general performance and timing issues are
Method Cells.  System cells directly related to some numerical methods are
Execution Cells .  System cells related to execution types areBUFFSIZE = SYSTEM(1)
HICORE = SYSTEM(57)
REAL = SYSTEM(81)
IORATE = SYSTEM(84)
BUFFPOOL = SYSTEM(119)
SOLVE = SYSTEM(69) - mixed
MPYAD = SYSTEM(66) - binary
FBSOPT = SYSTEM(70) - decimal
SHARED PARALLEL = SYSTEM(107) - mixed
SPARSE = SYSTEM(126) - mixed
DISTRIBUTED PARALLEL = SYSTEM(231) - decimal
USPARSE = SYSTEM(209) - decimal&lt;/p&gt;
&lt;p&gt;19 Chapter 1: Utility Tools and Functions
System Cells
The binary system cells are organized so that the options are selected by the decimal values of the powers of
2.  This organization makes the selection of multiple options possible by adding up the specific option values.&lt;br&gt;
The decimal cells use integer numbers.  The mixed cells use both decimal and binary values.
The following several system cells are related to machine and solution accuracy:
where MCHEPSS and MCHEPD are the machine epsilons for single- and double-precision, respectively,
and MCHINF is the exponent of the machine infinity.
Setting System Cells .  The following are several ways a user can set a system cell to a certain value:
The first pair of techniques is used on the NASTRAN entry, and the effect of these techniques is global to
the run.  The second pair of techniques is used for local settings and can be used anywhere in the DMAP
program; PUTSYS is the recommended way.
To read the value of cell, use:
VARIABLE = GETSYS (TYPE, CELL)
or
VARIABLE = GETSYS (VARIABLE, CELL)
Sparse Keywords .  The setting of the SPARSE keyword (SYSTEM(126)) is detailed below:MCHEPSS = SYSTEM(102)
MCHEPSD = SYSTEM(103)
MCHINF = SYSTEM(98)
NASTRAN SYSTEM (CELL) = value
NASTRAN KEYWORD = value
PUTSYS (value, CELL)
PARAM //’STSR’/value/  CELLNASTRAN Entry
DMAP Program&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions20
Combinations of values are valid. For example, SPARSE = 24 invokes a sparse run, except for SPMPYAD.
In the table below, the following naming conventions are used
:
The default value for SPARSE is 25.
Another keyword (USPARSE) is used to control the unsymmetric sparse decomposition and FBS. By setting
USPARSE = 0 (the default is 1, meaning on), the user can deselect sparse operation in the unsymmetric
decomposition and forward-backward substitution (FBS).
Shared Memory Parallel Keyword .  The SMPARALLEL (or PARALLEL) keyword controls the
shared memory (low level) parallel options of various numerical modules.
The setting of the SMPARALLEL keyword (SYSTEM(107)) is as follows:Value Meaning
1 Enable SPMPYAD T and NT
2 Deselect SPMPYAD NT
3 Force SPMPYAD NT
4 Deselect SPMPYAD T
5 Force SPMPYAD T
6 Deselect SMPMYAD T and NT
7 Force SPMPYAD T and NT
8 Force SPDCMP
16 Force SPFBS
SPMPYAD SPARSE matrix multiply
SPDCMP SPARSE decomposition (symmetric)&lt;/p&gt;
&lt;p&gt;21 Chapter 1: Utility Tools and Functions
Diagnostic (DIAG) Flags
Combinations are valid.  For example, PARALLEL = 525314 means a parallel run with two CPUs, except
with FBS methods.
Module Naming Conventions .  In the table above, the following naming conventions are used:
Distributed Parallel Keyword .  For distributed memory (high level) parallel processing the
DISTRIBUTED PARALLEL or DMP (SYSTEM (231)) keyword is used. In general, this keyword describes
the number of subdivisions or subdomains (in geometry or frequency) used in the solution. Since the value
of DMP in the distributed memory parallel execution of MSC Nastran defines the number of parallel Nastran
jobs spawn on the computer or over the network, its value may not be modified locally in some numerical
modules.
Diagnostic (DIAG) Flags
To request internal diagnostics information from MSC Nastran, the user can use DIAG  flags. The DIAG
statement is an Executive Control statement.&lt;br&gt;
DIAG Flags for Numerical Methods .  The DIAG flags used in the numerical and performance areas
are:Value Meaning
1  1023 No. of Process es
1024 Deselect FBS
2048 Deselect PDCOMP
4096 Deselect MPYAD
8192 Deselect MHOUS
16384 Unused
32768 Deselect READ
262144 Deselect SPDCMP
524288 Deselect SPFBS
FBS Forward-backward substitution
PDCOMP Parallel symmetric decomposition
MHOUS Parallel modified Householder method
READ Real eigenvalue module
SPFBS Sparse FBS
MPYAD Multiply-Add
SPDCMP Sparse decomposition&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions22
For other DIAG flags and solution sequence numbers, see the Executive Control Statements  (p.107) in the MSC
Nastran Quick Reference Guide .
Always use DIAG 8, as it helps to trace the evolution of the matrices throughout the MSC Nastran run,
culminating in the final matrices given to the numerical solution modules.
The module-related DIAGs 12, 16, 19 are useful depending on the particular solution sequence; for example,
12 on SOL 107, 111s, 16 on SOL 103 and 19 on SOL 200 jobs.
DIAG 58 is to be used only at the time of installation and it helps the performance timing of large jobs.
Matrix Trailers
The matrix trailer is an information record following (i.e., trailing) a matrix containing the main
characteristics of the matrix.
Matrix Trailer Content .  The matrix  trailer  of every matrix created during an MSC Nastran run is printed
by requesting DIAG 8. The format of the basic trailer is as follows:
Name of matrix
Number of columns: (COLS)
Number of rows: (ROWS)
Matrix form (F)
= 1 square matrix
= 2 rectangular
= 3 diagonal
= 4 lower triangular
= 5 upper triangular
= 6 symmetric
= 8 identity matrix
= 10 Cholesky factor
= 11 partial lower triangular factor
= 13 sparse symmetric factorDIAG 8 Print matrix trailers
12 Diagnostics from CEAD
13 Open core length
16 Diagnostics from READ
19 Multiply-Add time estimates
58 Print timing data&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P10</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_010/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_010/</guid>
      <description>
        
        
        &lt;p&gt;183 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
The practical implementation of the above procedure takes care of the cases where decoupling occurs, i.e.,
when . This process is still very unstable and an alternative iterative solution of (7-101) exists in
MSC Nastran, which is similar to the procedure detailed in Theory of Real Eigenvalue Analysis , 106.
Transformation of the Vectors .  If (7-72) is extended, the following is obtained:
(7-108)
By introducing
(7-109)
the following is obtained:
(7-110)
or
(7-111)
If (7-111) is multiplied by an eigenvector of the  matrix, the following is obtained:
(7-112)
because
(7-113)
Finally, the result is the following equation:
(7-114)
which indicates that the  eigenvectors of  can be obtained by the following:
(7-115)
This calculation is also straightforward if the  matrix is accumulated during the transformation from  to
. However, it is not practical to explicitly form the  matrix when the  matrices are not formed. An
effective implicit formulation follows.
Implicit Vector Transformation .  Write (7-115) as follows:
(7-116)
when  represents the Householder transformation matrices. Taking advantage of their matrix structure
using , and , write the following:hii1–0=
H Pn2–Pn1–P1 A1 P1Pn2–   =
Z P1  Pn2–=
H ZTAZ =
AZ ZH=
H
AZy ZHy =
Hyy=
AZyZy =
x A
x Zy=
Z A
H Z Pr
x P1 P2Pn2– y  =
Pi
y yn2–= x y0=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis184
Main Index
(7-117)
and
(7-118)
With this formulation only the  vectors need to be accumulated during the  transformation and
saved in the secondary storage for the vector transformations.
QZ Hessenberg Method
The QZ method is a robust algorithm for computing eigenvalues and eigenvectors of the eigenvalue problem
and, as such, applicable to the  case. If , the appropriate canonical transformation is executed
to linear form. There are no restrictions on  or . For details, see Golub and Van Loan, p. 375.
Hessenberg-Triangular Form
The first stage of the QZ algorithm is to determine unitary matrices  and  so that the matrix
is upper triangular and  is upper Hessenberg.
This is accomplished by first computing a unitary matrix  such that  is upper triangular. Next, the
matrix  is reduced to an upper Hessenberg matrix  by premultiplying by a series of unitary matrices
and postmultiplying by a series of unitary matrices .
The matrices  and  are carefully chosen so that
is upper Hessenberg, while
remains upper triangular.yr1–Pr yr=
yr1–yrurT yr
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-ur– =
urAH–
Ax Cx =
B 0= B0
A C
QˆZˆ
T QˆHCZˆ= S QˆHAZˆ=
U UHC
UHA S
QjHj1=kZjj1=H
QjHZj
S QjHQj1–HQ1HUHAZ1Z2ZjQˆHAZˆ=  =
T QHQj1–HQ1HUHCZ1Z2ZjQˆHCZˆ=  =&lt;/p&gt;
&lt;p&gt;185 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
The QZ Step
The derivation of the QZ step is motivated by the case where  is nonsingular. If  is nonsingular, we could
form  and apply the Francis QR algorithm. Rather than forming  explicitly, the QZ algorithm
updates  and  using unitary matrices  and :
The matrices  and  are chosen so that the matrix  is essentially the same as if a QR step had been
explicitly applied to . However, since we operate with  and  rather than , it is not necessary
for  to be invertible.
The QZ iteration continues until the matrix  converges to upper triangular form.
Eigenvalue Computation
Once  has converged to upper triangular form, the QZ algorithm will have determined unitary matrices
and  so that both  and  are upper triangular. Denote the diagonal entries
of  by , and the diagonal entries of  by .
Then, for each , the matrix  is singular. It follows that there exists a vector&lt;br&gt;
so that .
Substituting for  and  and multiplying on the right by , we have .
Hence, if , we set  and  to get , as desired. However, if
, then we have two cases to consider:
The Complex Lanczos Method
The main steps of the complex Lanczos Method are: reduction to tridiagonal form, solution of the tridiagonal
problem, and eigenvector computation.
MSC Nastran currently supports two complex Lanczos methods: the single vector and the adaptive, block
method.1. In this case,  is said to be an infinite eigenvalue.
2. Here  is indeterminate.T T
ST1–ST1–
S T Q Z
S QHSZ =
T QHTZ =
Q Z ST1–
ST1–S T ST1–
T
S
S
Q Z S QHAZ = T QHCZ =
S12Qn T12n
j1= 2n j SjT– uj
jSujjTuj=
S T Q jAZujjCZuj=
j0 jjj = xjZuj= AxjjCxj=
j0=
j0j0= j
j0j 0 = = j&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis186
Main Index
The Single Vector Method
Reduction to Tridiagonal Form .  The recurrence procedure introduced in Theory of Real Eigenvalue
Analysis , 106 can also be used to reduce a general matrix to tridiagonal form. This is the so-called biorthogonal
Lanczos method. Find a nonsingular  matrix such that:
(7-119)
or
(7-120)
Then with , the following relation holds:
(7-121)
Upon comparing columns of the right- and left-hand sides of (7-119) and (7-121), the following is obtained:
(7-122)
where  and  and  are complex rows and columns of the&lt;br&gt;
and  matrices.
Reordering (7-122) results in the following recurrence relations:
(7-123)
There is some flexibility in choosing the scale factors of . One possibility is to select , which
results in an unsymmetric  matrix with ones on the subdiagonal. This form is advantageous for a direct
eigenvector generation scheme.V
AV VT=
V1–AV T12&lt;br&gt;
223&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;n
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bnn = = &amp;hellip;
U VT–=
ATU UTT=
Avjj vj1–j vjj1+ vj1++ + =
ATujj uj1–j ujj1+ uj1++ + =
1v01 u00= = j12n 1uj– = vjU
V
j1+ vj1+Avjj vjj vj1–– – =
j1+ uj1+ATvjj ujj uj1–– – =
jj j1=
T&lt;/p&gt;
&lt;p&gt;187 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Another possibility is to select , which results  in a complex but symmetric matrix. This matrix
has overwhelming advantages in the eigenvalue extraction from the tridiagonal form, and MSC Nastran uses
this method in  the implementation. Thus, the new form of (7-123) is
(7-124)
which is in matrix form as follows:
(7-125)
From the biorthonormality of vector  and , derive the explicit form of the  coefficients.
Biorthonormality means that
(7-126)
The premultiplication of (7-124) by , and , respectively, results in the following (using
(7-126)):
and
(7-127)
The two versions of  and  should be the same in exact arithmetic. In the actual implementation,
the average of the two versions is used.
The algorithm can be developed as follows:jj= T
j1+ vj1+Avjj vjj vj1–– – =
j1+ uj1+ATujj ujj uj1–– – =
UTAV T12&lt;br&gt;
22&lt;/p&gt;
&lt;p&gt;n1–n
nn = =&amp;hellip;
u v q
uiTvj0 if ij
1 if ij==
ujTuj1+T vjTvj1+T
juuiTAvj or jvvjTAuj= =
j1+uuj1+TAvj or j1+vvj1+TATuj= =
jj1+&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis188
Main Index
(7-128)
where . The algorithm starts with  as well as biorthonormal
random starting vectors for  and .
The breakdown of the procedure occurs when  becomes equal to zero. Then the process is restarted
with new  vectors. In our implementation this is done when
(7-129)
where  is a small number (related to the machine accuracy).
Solution of Tridiagonal Problem .  The method for extracting the eigenvalues of the complex
tridiagonal form is a variant of the basic  procedure, mentioned previously in Theory of Real Eigenvalue
Analysis , 106. At each iteration the basic  procedure factors a shifted version of the current iterate as
follows:
(7-130)
where  is a lower triangular matrix,  is orthogonal and obeys the following:
(7-131)
The next iterate  is obtained as follows:
(7-132)juujT= vj ; jvvjTAuj ; jjujv+2 = =
vj1+Avjj vjj vj1–– – =
uj1+ATujj uj– j  uj1–– =
j1+uuj1+TAvj ; j1+vvj1+TATuj ; j1+j1+uj1+v+ 2 = = =
vj1+vj1+
j1+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
uj1+uj1+
j1+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
j1=mn u0v0010= = =
u1v1
j1+
uj1+vj1+
j1+j

QL
QL
TlQL= –
L Q
QTQ I=
Ti1+
T1QL=l+&lt;/p&gt;
&lt;p&gt;189 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Premultiplying by  and postmultiplying by  in (7-130) gives
(7-133)
By repeatedly applying (7-130) and (7-133),  is finally converted into a diagonal matrix whose elements are
the eigenvalues of the original tridiagonal matrix; that is,
(7-134)
Note that if the  matrix is complex, then the  matrix is also complex.
The computation of  is performed by a sequence of complex Givens transformation matrices .   Each
is the identity matrix except for the entries  where . These terms are defined as
follows:
(7-135)
and  are complex scalars. Each  matrix satisfies
(7-136)
The  matrix is built as follows:
(7-137)
First,  is determined by the n-th column of  with the diagonal entry shifted by . Applying this
transformation matrix to , a new nonzero term is introduced on the  position. Subsequent
values of  are defined so that the new nonzero introduced by  is forced up and out of the matrix,
thereby preserving the complex symmetric tridiagonal structure.
The algorithm below does not perform the explicit shifting, factorization, and recombination of (7-130) and
(7-132). Explicit shifting can result in significant loss of accuracy. Instead, within a given iteration (generating
from ), intermediate matrices are produced as follows:
(7-138)QTQ
T1QTTQ =
Ti
Tn=
T Q
Q Pk
PkPkij kijk1+
Pk– kk Pkk1+k1+  Ck= =
Pkk1+k Pkkk1+ Sk= =
Ck2Sk2+ 1=
CkSkPk
PkTPkPk1–= =
Q
Q Pn1–Pn2–P1 =
Pn1–T 
T n2n–
PjPn1–
Ti1+Ti
TikPk Tik1–Pkkn11 –= =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis190
Main Index
where , the original matrix.
Finally, the following is obtained:
(7-139)
The process is repeated for  or until the matrix  becomes diagonal. Note that this
diagonalization occurs by successive decoupling of 1  1 and 2  2 submatrices of . Decoupling occurs
when an off-diagonal term is less than the product of an  and the sum of the absolute values of the two
corresponding diagonal entries.
The selection of the  parameters can now be described. For , we must first determine the
shift that is chosen to be the one eigenvalue of the upper 2  2 submatrix of  closest to .
Then the parameters are as follows:
(7-140)
(7-141)
where
(7-142)
Note again that by executing this transformation the term  becomes nonzero. For the
subsequent rotations , the parameters are selected as follows:
(7-143)
where
This step is reviewed in the following figure by using  (the sub- and superscripts of the  are
ignored).T0kT=
Ti1+nTi1=
i1= 2m Ti
Ti

CkSk k n1–=
 TinTin11
Cn1–Tinnn–
an1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
Sn1–Tinn1–n
an1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
an1–Tinnn–  2Tinn1–n2+ =
Tinn2–n
k21–
CkTik1+k1+k2+ 
ak&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
SkTik1+kk2+
ak&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
akTik1+k1+k2+   2Tik1+kk2+  2+ =
k n2–= T&lt;/p&gt;
&lt;p&gt;191 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Figure 7-1  Chasing of Nonzero Offdiagonals.
As shown in Figure 7-1, the k-th transformation zeroes out , which was introduced by the
th transformation, but generates a new nonzero term in .
Using (7-143) the new terms of the matrix  are calculated as follows. Let
(7-144)
ThenNew term introduced
by the  step kn2– =
T erm to be eliminated by
the  step k n2–=
New term introduced
by the  step k n1–=Tk1–k Tk1–k1+ 
Tkk 1+
orTkk2+
Tn2–n&lt;/p&gt;
&lt;p&gt;orTk1+k1+ 
Tn1–n1– 
orTk1+k2+ 
Tn1–n
Tnn
Tkk2+
k1+ Tk1–k1+ 
Tik
bk2CkTik1+kk1+Sk+    Tik1+kkTik1+k1+k1+  –   =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis192
Main Index
(7-145)
Finishing the calculation of (7-145), set  and repeat from (7-144). When (7-145) is finished for
, then use (7-142) and start the process from (7-144) again.
Breakdown of the Procedure .  The calculation of  and  is performed using a denominator with
the following form:
(7-146)
which can be very small without either  or  being small. For example, for  and
. When this situation occurs, the process breaks down. The condition of this problem
can be formulated as follows. If
(7-147)
then the procedure will terminate.  in (7-147) is a small number related to the machine accuracy.
The storage and computational efficiency of this algorithm is excellent since it needs  storage and
operations.
Error Bounds .  Theoretically, the solutions of the following reduced eigenproblem
(7-148)
are approximations to the solutions of the original problem.
T o estimate the accuracy of the solution, we use
(7-149)
(7-149) is a generalization of such an error bound in the current symmetric Lanczos code.
The error bound for the original eigenvalues of (7-2) can be found as follows:Tikkk Tik1+kkSk bk– =
Tikk1+k1+  Tik1+k1+k1+ Sk bk– =
Tikkk1+ Tik1+kk1+Ck bk– =
Tikk1–k CkTikk1–k  =
Tikk1–k1+  SkTikk1–k  =
k k1–=
k k=
CkSk
a2b2+
a b a 1=
b ia2b20= +=
a2b2+a2b2+  

On
On2
Tsii si=
ii–m1+ sim &lt;/p&gt;
&lt;p&gt;193 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
(7-150)
Eigenvector Computation .  T o calculate the eigenvectors of the tridiagonal form, an inverse power
iteration procedure is used. First, a random right-hand side is generated for the inverse iteration. Then an
decomposition of the tridiagonal matrix is performed by Gaussian elimination with partial pivoting.
After a back substitution pass, the convergence of the approximate eigenvector is checked by its norm. If the
norm is greater than one, then the eigenvector is accepted. This norm shows sufficient growth, considering
that the procedure began with a random right-hand side vector with elements less than one. Otherwise, the
eigenvector is normalized, and the process is repeated with this vector as the right-hand side. The iteration is
repeated up to three times.
Practice indicates that most of the time one iteration pass is sufficient. The computation is very stable,
especially with partial pivoting. The process can be summarized as follows.
1.Decomposition
(7-151)
where:
2.Iteration
(7-152)
where:=the tridiagonal matrix
=an eigenvalue
=permutation matrix (row interchanges)
=factors
=random starting vector
=approximate eigenvectorii–
i&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;m1+ sim
i&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; 
for  B0pi0–
pi0–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-1–m1+ sim
i&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- 
for  B 0pi20–
pi20–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;1–m1+ sim
i&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;  =
LU
PTi I – LU=
T
i
P
LU
LUu2u1=
u1
u2&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis194
Main Index
If  and the iteration count is less than 3, then
(7-153)
and the iteration continues; otherwise,  is the eigenvector.
3.Converting the Solution
The conversion of eigenvalues is performed by the following equations:
(7-154)
The eigenvector conversion requires the computation of the following:
, for the right vectors
or
, for the left vectors
where:
4.Initialization
The Lanczos recurrence is started with orthonormalized random vectors. Any numerical procedure
can be used that generates long enough pseudorandom number sequences for the random number
generation. Suppose the random vectors are as follows:
Orthonormalization can be performed in the following form:
(7-155)=eigenvector of (7-162) corresponding to&lt;/h1&gt;&lt;p&gt;=
=number of roots found by the procedure at the particular shiftu21
u1u2
u2&amp;mdash;&amp;mdash;&amp;mdash;- =
u2
pii0B0+ =
pi2i0B0=+ =
xiVsi=
yiUsi=
si
U u1um 
V v1vm 
m
u1v1
u1u1
v1Tu112&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
v1v1
u1Tv112&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =&lt;/p&gt;
&lt;p&gt;195 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
These vectors are now suitable to start the Lanczos procedures.
5.Outer Orthogonalization
It is well known that the repeated occurrence of the eigenvectors can be prevented by assuring
orthogonality between the already accepted eigenvectors and the Lanczos vectors. This outer
orthogonality can be maintained using the following iteration process (i.e., the modified Gram-
Schmidt):
(7-156)
This process should be executed similarly with the  vectors.
The respective orthogonalization formula is as follows:
In the above formulae,  are already accepted eigenvectors and .
6.Inner Orthogonalization
When the Lanczos procedure is carried out in practice using finite precision arithmetic, then the strict
biorthogonality of the  sequences is lost. The biorthogonality can be maintained if  is
reorthogonalized with respect to , and  is reothogonalized with respect to
using the following formulae:
(7-157)
For numerical stability, the subtraction is performed immediately after a term is calculated, and the
sum is not accumulated.vijvijxkT vixk
k1=c
– =
for   j12l =     until
xkT vil for max k1c =
ui
uijuijykTuiyk
k1=c
– =
xkyk k 12c =
uv uj1+
v1vj1+ vj1+
u1uj1+
vj1+vj1+uiTvj1+vi
i1=j
– =
uj1+uj1+viTuj1+ui
i1=j
– =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis196
Main Index
Inner orthogonality monitoring is performed by the following:
(7-158)
No orthogonalization is necessary when
If necessary, the orthogonalization is performed against all previous vectors. If the orthogonalizations
are kept in the core, this fact implies a limit on .
7.Shift Strategy
The default logic begins with a shift at (0.1, 1.0). The Lanczos recurrence process is performed until
breakdown. Then the eigensolutions of the reduced problem are evaluated as approximations. If the
user desires, then the shifts are executed at user-specified locations as shown.
Figure 7-2  Recommended Shift Points for Complex Lanczosjuuj1+Tvj=
jvvj1+Tuj=
max jujv
m
Default ShiftReIm
11
22
 
kk&lt;/p&gt;
&lt;p&gt;197 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
The shifts are used until the required number of eigenvalues are found. Unfortunately, there is no
analogue to the Sturm sequence theory used for the real eigensolution. As a result, there is no
assurance that no gaps exist between the eigenvalues found.
The Adaptive, Block Lanczos Method
The solution of the mathematical eigenproblem in its canonical form shown in (7-13) will be more efficiently
accomplished with the block Lanczos method.
The block Lanczos method (see Bai, et al., 1996) generates two sets of biorthonormal blocks of vectors&lt;br&gt;
and  such that:
(7-159)
when  and zero otherwise. Note that we are using superscript H to denote the complex
conjugate transpose. These vector sets reduce the  system matrix to  block tridiagonal matrix form:
(7-160)
where the
(7-161)
and
(7-162)
matrices are the collections of the Lanczos blocks. The structure of the tridiagonal matrix is:
(7-163)
The block Lanczos process is manifested in the following three term recurrence matrix equations:
(7-164)
andPj
Qj
PiHQjI=
ijijn =
A Tj
TjPjHAQj =
PjP1P2Pj =
QjQ1Q2Qj =
TjA1B2&lt;br&gt;
C2A2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Bj
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;CjBj=&amp;hellip;&amp;hellip;
Bj1+Pj1+HPjHAAj PjHCj Pj1–H– – =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis198
Main Index
(7-165)
Note that in both of these equations the transpose of the system matrix  is avoided.
In order to find the mathematical eigenvalues and eigenvectors we solve the block tridiagonal eigenvalue
problems posed as:
(7-166)
and
(7-167)
where in above equations the size of the reduced tridiagonal matrix is  times , assuming a fixed block size
for now. The  eigenvalues of the tridiagonal problem (the so-called Ritz values) are approximations of
the  eigenvalues of the mathematical problem stated in Equation (7-13)(7-13). The approximations to the
eigenvectors of the original problem are calculated from the eigenvectors of the tridiagonal problem (Ritz
vectors) by:
(7-168)
and
(7-169)
where  are the matrices containing the first  Lanczos blocks of vectors and  are the left and right
eigenvectors of the tridiagonal problem. Finally,  are the right and left approximated eigenvectors of the
mathematical problem.
A beautiful aspect of the Lanczos method (exploited also in the READ module) is that the error norm of the
original problem may be calculated from the tridiagonal solution, without calculating the eigenvectors. Let
us introduce a rectangular matrix  having an identity matrix as bottom square. Using this, a residual vector
for the left-handed solution is:
(7-170)
which means that only the bottom  (if the current block size is ) terms of the new Ritz vector  are
required due to the structure of . Similarly for the right-handed vectors:
(7-171)
An easy acceptance criterion (an extension of the one used in the real case) may be based on the norm of the
above residual vectors as:Qj1+ Cj1+AQjQj AjQj1– Bj– – =
A
wHTjwH=
Tj zz=
j p
p 

y Pj w =
x Qj z =
PjQj j wz
xy
Ej
sHyHAyH– wHEjBj1+ Pj1+H= =
p p w
Ej
r AxxQj1+ Cj1+EjHz = – =&lt;/p&gt;
&lt;p&gt;199 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
(7-172)
where the  value to accept convergence is either user given or related to an automatically calculated
machine epsilon.
Based on a detailed error analysis of these quantities, we modify this criterion by considering a gap:
(7-173)
where . With this the recommended criterion is
(7-174)
The  denotes the Euclidean norm.
In the above, we assumed that the Lanczos blocks have a uniform size of . It is possible to generalize this to
allow for the j-th iteration to have  variable block size. Such flexibility may be advantageous in the case of
clustered eigenvalues or to avoid the breakdown of the Lanczos process.
Let us assume at the -st iteration, the block size is increased by  and the -st Lanczos vectors
are augmented as:
(7-175)
and
(7-176)
where the * vectors are the yet undefined augmentations. It is easy to see that appropriate augmentations will
maintain the validity of the three member recurrence of (7-164) and (7-165) as follows:
(7-177)
and
(7-178)min sH
2r2 con
con
gapTj min i– =
i
min sH
2r2 sH
2r2
gapTj&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;con
.2
p
pj
j1+ k j1+
Pj1+ Pj1+ 
Qj1+ Qj1+ 
Bj1+0Pj1+H
Pj1+HPjHAAj PjHCj Pj1–H– – =
Qj1+Qj1+ Cj1+
0AQjQj Aj– Qj1– Bj– =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis200
Main Index
The Lanczos process can therefore formally continue with:
(7-179)
(7-180)
and
(7-181)
(7-182)
The conditions of successful continuation with augmented blocks are the orthogonality requirements of:
(7-183)
and
(7-184)
It is of course necessary that the newly created, augmented pair of Lanczos vector blocks’ inner product
is not singular, since its decomposition will be needed by the algorithm. Specifically, we need the smallest
singular values of the inner product matrix to be larger than a certain small number. A possible choice for the
augmentations is to have  pairs of random vectors and orthogonalize them against the earlier vectors by
using a modified Gram-Schmidt procedure. The orthogonalization may be repeated several times to assure
that the smallest value is above the threshold.
The most prevalent usage of the block size adaption is to cover existing clusters of eigenvalues. In the real
block implementation, we were not able to change the block size on the fly (adapt). Therefore, an estimate
of the largest possible cluster was needed a priori. For typical structural applications, the default block size of
7 (anticipating 6 as the highest multiplicity) was used.
The multiplicity of a cluster may be found on the fly with the help of the Ritz values. The number of expected
multiplicities in a cluster is the number of elements of the set satisfying:
(7-185)
where  is the user specified cluster threshold. The order of the largest cluster of the Ritz values is
calculated every time a convergence test is made and the block size is appropriately adjusted. This procedure
is not very expensive, since it is done on the tridiagonal problem.Bj1+Bj1+0 =
Cj1+Cj1+
0=
Pj1+Pj1+ Pj1+  =
Qj1+Qj1+ 
PH
j1+ Qj0=
PjH Qj1+0=
PjH Qj1+
k
ik–clu max ik 
clu&lt;/p&gt;
&lt;p&gt;201 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Preventing Breakdown .  It is easy to see that the Lanczos process breaks down in some circumstances.
These are:
Either  or  or both are rank deficient.
Neither are rank deficient, but  is rank deficient.
The breakdown of the first kind prevents the execution of the QR decomposition of the j-th blocks. This is
fairly easy to overcome by an orthogonalization procedure also applied in the current complex Lanczos
implementation.
Specifically, if  is deficient, then we restart the Lanczos process with a random  made orthogonal to
all previous left Lanczos vectors  as:
(7-186)
If  is just nearly rank deficient (detected by the QR decomposition of ), then we
reorthogonalize this  to the previous left Lanczos vectors, as shown in the above equation.
Rank deficiency (full or near) of  is treated similarly with respect to the right Lanczos vectors.
The breakdown of the second type (a serious breakdown) manifests itself in the singular value decomposition
of . In this type of breakdown, some or all of the singular values are zero, as follows:
(7-187)
where  is nonsingular if it exists. Using the augmentation techniques shown earlier, this problem may also
be overcome. First, calculate and partition as follows:
(7-188)
and
(7-189)
where the number of columns in the second partition is the number of zero singular values. Create the
following projector matrix:
(7-190)RjSj
RjH Sj
SjQj1+
Pj
PjH Qj1+0=
SjSjQj1+ Cj1+=
Qj1+
Rj
Pj1+H Qj1+
Pj1+H Qj1+Uj0
00VjH=

Pj1+Uj P1 P2=
Qj1+Vj Q1 Q2=
jQj PjH=&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis202
Main Index
Bai et al., 1996 proves that the vectors of:
(7-191)
and
(7-192)
in the following augmentation:
(7-193)
and
(7-194)
will result in an always nonsingular  product.
It is possible to extend this procedure to the case of near-breakdown when the singular values may not be
exact zeroes, but smaller than desired. In this case, it is recommended to increase the block size for those
singular values that are below a specified threshold. Finally, one may only use random vectors instead of the
projection matrix.
Maintaining Biorthonormality .  The maintenance of the biorthonormality of the  and  vectors
is the cornerstone of the Lanczos algorithm. Local biorthonormality, that is, maintaining the condition
between the consecutive Lanczos vectors, is fairly simple by executing the following steps:
(7-195)
(7-196)
These steps use data only available in memory, therefore they are cost effective even when executed repeatedly.
Unfortunately, this method does not ensure that multiple eigenvectors will not reoccur. This may be
prevented by a full reorthonormalization scheme using a modified Gram-Schmidt process. It is implemented
that way in the single vector complex Lanczos method of MSC Nastran. A measure of the orthogonality of
the current Lanczos vectors, with respect to the already accepted eigenvector, is:
(7-197)P2Ij–H Q2=
Q2Ij–H P2=
Pj1+ P1 P2 P2=
Qj1+Q1Q2Q2=
Pj1+H Qj1+
PjQj
RjRjPjQjHRj – =
SjSjQjSjHRj – =
dj1+max PjHQj1+1
Pj1Pj1+1&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;QjHPj1+1
Qj1Pj1+1&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; 
    &lt;/h1&gt;
      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P11</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_011/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_011/</guid>
      <description>
        
        
        &lt;p&gt;203 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
where  is the matrix column norm. This quantity is usually compared to a machine computational
accuracy indicator as:
(7-198)
where  is the automatically calculated machine epsilon. The appropriateness of the choice of the square
root was proven in the real Lanczos method (called partial orthogonality there) and was already successfully
employed in the READ module.
That measure, however, is very expensive, both in CPU and I/O regards. Specifically, the numerator requires
the retrieval of the  and  vector blocks from secondary storage and a multiplication by them. A method
of maintaining partial orthogonality with a limited access of the  and  matrices, uses
(7-199)
where  is now the matrix row norm. The norms of the denominator may be updated in every iteration
step without retrieving the eigenvectors. This method is calculating the numerator terms of
(7-200)
and
(7-201)
utilizing the fact that these also satisfy the three term recurrence equations perturbed by rounding errors as
follows:
(7-202)
where
(7-203)
and
(7-204)
The superscript  refers to the left side. Similarly, to the right side:.1
dj1+mac
mac
Pj Qj
Pj Qj
dj1+max Xj1+1
Pj1Qj1+1&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;Yj1+inf
Qj1Pj1+1&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; 
   =
.inf
Xj1+Pj Qj1+=
Yj1+Pj1+ Qj=
Xj1+TjXj
0Xj
0AjXj1–
W1lBj0
Bj1+W2l+ – – =
W1lPj Qj1+=
W2lPj1+ Qj=
l&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis204
Main Index
(7-205)
where
(7-206)
and
(7-207)
with superscript  referring to the right side. After these steps, the matrices are perturbed to simulate the
round-off effect as:
(7-208)
and
(7-209)
where  is a random matrix having a norm of . If the test
(7-210)
fails, then a retroactive modified Gram-Schmidt procedure (similar to the local one above) is needed. The
cost of this procedure is , the dominant cost being the inner products of the Lanczos blocks producing
the  vector blocks. This assumes that the block structure of  is taken into consideration.
Mathematical Algorithm.  The simplified mathematical algorithm not containing all the procedures
developed in the preceding sections follows:Yj1+ Xj 0TjAjYj 0CjYj1– W1r0
W2r Cj1++ – – =
W1rPjHQj1–=
W2rPj1+H Qj=
r
Xj1+Xj1+Fj+ Cj1+1–=
Yj1+Bj1+1–Yj1+FjH+  =
Fjmac
dj1+mac
On
W Tj&lt;/p&gt;
&lt;p&gt;205 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Figure 7-3  Block Lanczos Logic1.Initialization
a.Choose starting vector blocks  such that
b.Calculate  and
2.Iteration for
a.Compute recurrence:
b.QR decompose:
c.SV decompose:
d.Compute recurrence:
e.Start recurrence:P1Q1 P1HQ1 I=
R1P1HAH= S1AQ1=
j12  =
AjPjHSj=
RjRjPjAjH– =
SjSjQjAj– =
Rj1+Pj1+Bj1+H=
SjQj1+Cj1+=
PjHQj1+UjjVjH=
Bj1+Bj1+Ujj12=
Cj1+j12VjHCj1+=
Pj1+Pj1+Ujj12–=
Qj1+Qj1+Vjj12–=
Rj1+Pj1+HACj1+PjH– H=
Sj1+AQj1+QjBj1+– =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Solution Method Characteristics206
Main Index
Singular Value Decomposition (SVD)
Since a crucial element of the algorithm in Figure 7-3 is the singular value decomposition, (c), this section gives
more detail.
Given any  matrix ,  there exist unitary matrices  and  such that
where  has the same order as , and  has the form
and  is a diagonal matrix with real diagonal entries . The diagonal entries of&lt;br&gt;
are the singular values of , and are mathematically defined to be the positive square roots of the eigenvalues
of .
The singular value decomposition of  is computed without forming . The first step is to compute
unitary matrices  and  such that
is bidiagonal.
The main step of the SVD algorithm implicitly performs the Francis QR iteration on the matrix .
It is also possible to define an &amp;ldquo;economy&amp;rdquo; size SVD. Let  be the matrix consisting of the first  columns
of . Then
is an alternate form of the SVD.
The method is also available directly to the user as shown in the next section.
Solution Method Characteristics
Available methods in MSC Nastran are the Hessenberg  methods and the complex Lanczos  methods. The
Hessenberg method is a reduction method, while the Lanczos method is an iterative method. The
characteristics of these methods are:nx Anm U V
A UVH=
 A
ˆ
0=
ˆ12m0  ˆ
A
AHA
A AHA
U1V1
B U1HAV1=
BHB
Uˆm
U
A UˆˆVH=&lt;/p&gt;
&lt;p&gt;207 Chapter 7: Complex Eigenvalue Analysis
Solution Method Characteristics
Main Index
Method Type Identifier Application Restriction
Hessenberg Reduction HESS All roots, few
vectors
QZ Hessenberg Reduction QZHESS All roots, few
vectorsNone
Complex
LanczosIterative CLAN Few roots
SVD Reduction SVD Singular value and/or
vectors must be purgedM1–0
KsBs2M0 + +
BM&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
User Interface208
Main Index
User Interface
Input Data Blocks:
Output Data Blocks:
Parameters:CEAD KXX,BXX,MXX,DYNAMIC,CASECC,VDXC,VDXR/
CPHX,CLAMA,OCEIG,LCPHX,CLAMMAT/
S,N,NEIGV/UNUSED2/SID/METH/EPS/ND1/ALPHAJ/OMEGAJ/
MAXBLK/IBLK/KSTEP $
KXX Stiffness matrix.
BXX Viscous damping matrix.
MXX Mass matrix.
DYNAMIC Table of Bulk Data entry images related to dynamics.
CASECC Table of Case Control command images.
VDXC Partitioning vector with 1.0 at rows corresponding to null columns in K, B,
and M.
VDXR Partitioning vector with 1.0 at rows corresponding to null rows in K, B, and M.
CPHX Complex eigenvector matrix.
CLAMA Complex eigenvalue summary table.
OCEIG Complex eigenvalue extraction report.
LCPHX Left-handed complex eigenvector matrix.
CLAMMAT Diagonal matrix with complex eigenvalues on the diagonal.
NEIGV Output-integer-no default. NEIGV indicates the number of eigenvalues found. If none
were found, NEIGV is set to -1.
UNUSED2 Input-integer-default=1. Unused.&lt;/p&gt;
&lt;p&gt;209 Chapter 7: Complex Eigenvalue Analysis
Method Selection
Main Index
Method Selection
Defines data needed to perform complex eigenvalue analysis.
Format:
The following continuation is repeated for each desired search region. (J =1 to n, where n is the number of
search regions.)SID Input-integer-default=0. Alternate set identification number.
If SID=0, the set identification number is obtained from the CMETHOD command in
CASECC and used to select the EIGC entry in DYNAMIC.
If SID&amp;gt;0, then the CMETHOD command is ignored and the EIGC entry is selected by
this parameter value. Applicable for all methods
If SID&amp;lt;0, then both the CMETHOD command and all EIGC entries are ignored and
the subsequent parameter values (E, ND1, etc.) will be used to control the eigenvalue
extraction. Applicable for single vector Lanczos, block Lanczos, QZ Hessenberg, QR
Hessenberg, and SVD (Singular Value Decomposition).
METH Input-character-default=&amp;lsquo;CLAN&amp;rsquo;. If SID&amp;lt;0, then METH specifies the method of
eigenvalue extraction
CLAN Complex Lanczos (block or single vector),
HESS QZ Hessenberg or QR Hessenberg,
SVD Singular Value Decomposition.
EPS Input-real-default=1.E-5. Used only when SID&amp;lt;0.
ND1 Input-integer-default=0. The number of desired eigenvectors. Used only when SID&amp;lt;0.
ALPHAJ Input-real-default=0.0. Real part of shift point for the Lanczos method. Used only when
SID&amp;lt;0.
OMEGAJ Input-real-default=0.0. Imaginary part of shift point for the Lanczos method. Used only
when SID&amp;lt;0.
MAXBLK Input-real-default=0.0. Maximum block size. Used only when SID&amp;lt;0.
IBLK Input-real-default=0.0. Initial block size. Used only when SID&amp;lt;0.
KSTEP Input-real-default=0.0. Frequency of solve. Used only when SID&amp;lt;0.
EIGC Complex Eigenvalue Extraction Data
1 2 3 4 5 6 7 8 9 10
EIGC SID METHOD NORM G C E ND0
ALPHAAJ OMEGAAJ MBLKSZ IBLKSZ KSTEPS NDJ&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Method Selection210
Main Index
Examples:
Comments:
1.The EIGC entry may or may not require continuations as noted below.
•For the &amp;ldquo;HESS&amp;rdquo; method, continuations are not required; and their contents are ignored when
present, except for ND0. However, it is recommended that continuations are not used.
•For the &amp;ldquo;CLAN&amp;rdquo; method when the continuation entry is not used a shift is calculated
automatically. When a shift is input on the first continuation entry it is used as the initial shift.
Only one shift is used. Data on other continuation entries is ignored.
2.The MBLKSZ and IBKLSZ parameters are integers in concept, but must be input at real numbers
(that is, with a decimal point.) They represent maximum sizes, and may be reduced internally for
small size problems.
Alternate EIGC Bulk Data Entry
The following alternate format is valid for all existing methods except inverse power:EIGC 14 CLAN
+5.6 4
-5.5 3
EIGC 15 HESS 6
Field Contents
SID Set identification number. (Unique Integer &amp;gt;0) Referred to by CMETHOD =SID.
METHOD Method of complex eigenvalue extraction. (Character: &amp;ldquo;HESS&amp;rdquo; or &amp;ldquo;CLAN&amp;rdquo;)
NORM Method for normalizing eigenvectors. See Option Selection , 211.
ND0,J Number of eigenvalues desired.
G Grid or scalar point identification number. Required if and only if NORM =&amp;ldquo;POINT&amp;rdquo;.
(Integer &amp;gt;0)
C Component number. Required if and only if NORM =&amp;ldquo;POINT&amp;rdquo; and G is a geometric
grid point. (0 &amp;lt;Integer &amp;lt;6)
E Convergence criterion. (Real &amp;gt;0.0. Default values are: 10-15 for METHOD =&amp;ldquo;HESS&amp;rdquo;,
E is machine dependent for METHOD =&amp;ldquo;CLAN&amp;rdquo;.)
MBLKSZ Maximum block size. (Default =7, Real &amp;gt;0.0) Block Lanczos only.
IBLKSZ Initial block size. (Default =2, Real &amp;gt;0.0) Block Lanczos only.
KSTEPS Frequency of solve. (Default =5, Integer &amp;gt;0) Block Lanczos only.
1 2 3 4 5 6 7 8 9 10
EIGC SID METHOD NORM G C EPS ND1
KEYWORD1=&lt;!-- raw HTML omitted --&gt; KEYWORD2=&lt;!-- raw HTML omitted --&gt; KEYWORD3=&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;211 Chapter 7: Complex Eigenvalue Analysis
Option Selection
Main Index
where KEYWORD may be any of the parameters from the original entry except SID, as well as:
Examples:
Remarks:
1.The first field of the keyword-driven continuation entry must be blank.
2.If any of the parameters METHOD, NORM, G, C, EPS, or ND1 are specified     on the continuation
entry, the corresponding field on the original entry must be blank.
3.A maximum of 10 shifts may be specified.
Option Selection
Complex eigenvalue analysis in MSC Nastran supports general damping and normalization options as well
as specific Hessenberg and Lanczos options.
Damping  Options
The presence of the  matrix indicates the viscous damping option when the following equation is solved:NDj Number of desired roots at shift j. (Integer &amp;gt; 0)
SHIFTRj The real part of shift j. (Real)
SHIFTIj The imaginary part of shift j. (Real)
KSTEPSj Block tridiagonal solution frequency at shift j; (only block Lanczos).
(Integer &amp;gt; 0)
MBLKSZj Maximum block size at shift j; (only block Lanczos). (Integer &amp;gt; 0)
IBLKSZj Initial block size at shift j; (only block Lanczos). (Integer &amp;gt; 0)
Note:    In the parameters above, the value of j ranges from 1 to 10.
EIGC 1 CLAN
EPS=1.E-12 ND1=12 SHIFTR1=0. SHIFTI1=2.4E2
EIGC 2 HESS
ND1=10
EIGC 3 CLAN
shiftr1=0.0, shifti1=20., nd1=5, iblksz1=2, mblksz1=5
shiftr2=0.0, shifti2=50., nd2=5, iblksz2=2, mblksz2=5
shiftr3=0.0, shifti3=100., nd3=5, iblksz3=1, mblksz3=5
B&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection212
Main Index
(7-211)
This problem is transformed into a linear problem that is twice the size of the original matrices and provides
eigenvalue solutions in complex conjugate pairs.
When the  matrix is not present and damping is introduced via imaginary stiffness terms (structural
damping), then the following problem is solved:
(7-212)
In this case the roots are not complex conjugate pairs. The mixed case of having both viscous and structural
damping is also possible.
Normalization  Options
The default normalization (and the only method available for the Hessenberg and Lanczos methods) is MAX.
This option normalizes the component (of the eigenvector with the largest magnitude) to one as the real part
and zero as the imaginary part.
The POINT normalization option uses G for a grid and C for a component to set the component to a value
of (1.0, 0.0). This option is not currently available for the complex eigenvalue methods.
Hessenberg and Lanczos Options
Hessenberg Spill Option .  The spill  option of the Hessenberg method (with QR) is less robust than the
default (with QZ), but the latter has no spill. Since this option requires a much smaller amount of memory
to run a problem, larger problems can be solved on a given computer.
Single Vector Lanczos Option .  The adaptive block option of the Lanczos method is very robust and
efficient even for large, direct complex eigenvalue analysis jobs and is the default. The single vector option
must be forced.
These options are selected via SYSTEM(108).
SYSTEM(108)
Bit Decimal EIGC Entry Selection
0 0 HESS QZ Hessenberg without spill (default)
1 1 HESS QR Hessenberg with spill
2 2 CLAN Single vector complex Lanczos
3 4 CLAN Adaptive, block complex Lanczos (default)M2B+ K+  u0=
B
2M K+ u0+&lt;/p&gt;
&lt;p&gt;213 Chapter 7: Complex Eigenvalue Analysis
Option Selection
Main Index
Since the cell is binary, appropriate combinations are also valid. For example, SYSTEM(108) =12 is a proper
setting for the block method with debug output.
Internal Block Lanczos Options .  There are several internal detailed options available for the block
Lanczos method (also in SYSTEM(108)) as shown in the following table:
SVD Option .  The singular value decomposition of a  matrix is produced if  and  are purged.
The SVD method is provided for DMAP applications. If used in SOLs 107 or 110, and mass or damping
terms are present, a user fatal exit is taken. The SVD operation decomposes the input stiffness matrix K into
the factors U, , and V as described in Solution Method Characteristics , 206. The ND1 value has a meaning for
the SVD functions which differs from eigensolution.4 8 CLAN Debug output for both complex Lanczos
9 256 HESS Force LR Hess (aka old Hessenberg without
spill)
10 512 HESS Force QZ Hess
Option Action
16 T urn off block size reduction in block Lanczos
32 T urn off block size augmentation in block Lanczos
64 Enforce full orthogonality in block Lanczos
128 T urn off initial vector preprocessing in block Lanczos
1024 Override defaults for small problems in block Lanczos
2048 Output XORTH matrix in place of ROOTS matrix
4096 T urn off block FBS for complex matrices in block Lanczos
8192 T urn off symmetric decomposition in block Lanczos
16384 T urn off real reduction phase (always use complex arithmetic)
32768 Force spill of Lanczos vectors (testing purposes only)
65536 Old semi-algebraic sorting criterion
131072 Force spill during eigenvector computation (testing purposes only)
262144 T urn off autoshift logicSYSTEM(108)
Bit Decimal EIGC Entry Selection
K B M
&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Complex Eigenvalue Diagnostics214
Main Index
Complex Eigenvalue Diagnostics
Hessenberg Diagnostics
The Hessenberg method has no internal diagnostics when the no spill option is used. The spill option has
the following diagnostics:
NEW HESSENBERG ROUTINE
TIME OF TRANSFORMATION TO HESS FORM: X (REDUCTION TIME)
STARTING QR ITERATION WITH NINC = X
The NINC value is the number of columns that are in memory.
INFINITY NORM = X
This is the maximum term (in magnitude) in the Hessenberg matrix.
FINAL EPSILON =
The E convergence criterion  (from the EIGC entry) is adjusted for the problem.
TIME OF FINDING EIGENVALUES =X (QR ITERATION TIME)
VECTOR GENERATION WITH NINC =X
The number of columns of the Hessenberg matrix held in core.
TIME OF VECTOR GENERATION =X
VECTOR REVERSE TRANSFORMATION WITH NINC =X
The number of eigenvectors held in core is NINC.
TIME OF VECTOR TRANSFORMATION = X (MULTIPLICATION BY HOUSEHOLDER
VECTORS).
Hessenberg diagnostics contain both numerical and performance information. The performance is low if
NINC N at any phase where N is the problem size.
The adjusted convergence criterion isND1 Output&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;0 All vectors of U and V are output.
=0 U and V are returned in a purged state, that is, only the singular value
matrix  is computed..
&amp;lt;0  is returned as a square matrix whose number of columns is equal to the
minimum number of rows or columns of the input matrix. U and V are
truncated to be commensurate with . This is a method to reduce the costs
of solving very rectangular input matrices by providing a partial solution
for the most interesting vectors.



«&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;215 Chapter 7: Complex Eigenvalue Analysis
Complex Eigenvalue Diagnostics
Main Index
(7-213)
where  is the infinity norm of the Hessenberg matrix and  is the user-given (or default) convergence
criterion.
Complex Lanczos Internal Diagnostics
Here, the newer block method diagnostics is shown.
Complex Lanczos Diagnostics DIAG 12.  The two levels of internal diagnostics of complex Lanczos
are requested via DIAG 12 and SYSTEM(108) = 8. The structure of the DIAG 12 diagnostics is as follows:
The accuracy required is an echo of  on the EIGC entry or the default if  is not used. Default .
The number of shifts equals to the number of the continuation entries. The damping mode flag is 0 when
damping matrix is present; otherwise, it is 1.
CURRENT SHIFT IS AT X,Y
CURRENT BLOCK SIZE IS X
NUMBER OF MODES REQUIRED AT THIS SHIFT IS XX
The most important parts of the debugging diagnostics (requested by SYSTEM(108) = 8) are as follows:
This message could appear any number of times. It indicates the end of an internal Lanczos process.
At the end of the Lanczos run, the following table is printed:*** USER INFORMATION MESSAGE 6361 - COMPLEX LANCZOS DIAGNOSTICS
THIS DIAGNOSTICS IS REQUESTED BY DIAG 12.
INITIAL PROBLEM SPECIFICATION
DEGREES OF FREEDOM = XXX ACCURACY REQUIRED = XXX
REQUESTED MODES = XXX NUMBER OF SHIFTS = XXX
DAMPING MODE FLAG = XXX SIZE OF WORKSPACE = XXX
BLOCK SIZE = XXX STEP SIZE = XXX
MATHEMATICAL EIGENVALUE ESTIMATED ACCURACY
EIGENVALUE # REAL IMAGINARY LEFT RIGHT
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . NHE =
HE
E E 106–=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Complex Eigenvalue Diagnostics216
Main Index
(7-214)
Mathematical Solution .  This table contains the shifted solutions where  is the reduced size at shift
.
Besides the diagnostics detailed above, additional information is printed for debugging purposes, such as the
dynamic matrix calculation and the eigenvectors of the tridiagonal form. These outputs are not explained
here. This part of the diagnostics may be extensively long, therefore the user should not use SYSTEM(108)
= 8 on large problems.
The acceptance of these approximate roots is documented in the following table, again requested by
DIAG12:
When the error value is less than the  convergence criterion set by the user on the EIGC entry, then the i-
th approximate eigenvalue is accepted as a physical eigenvalue. These accepted eigenvalues are printed in the
following table:
(7-215)
where  is the number of acceptable roots found at shift .
The final acceptance is documented by the state equation summary table as follows:
Any rejected roots are printed in a similar table preceded by the following message:
THE FOLLOWING ROOTS HAD AN UNACCEPTABLY LARGE
STATE EQUATION RESIDUALMATHEMATICAL SOLUTION DIRECT RESIDUALS
REAL IMAGINARY LEFT RIGHT
PHYSICAL SOLUTION STATE EQUATION RESIDUALS
REAL IMAGINARY LEFT RIGHTireal iimag i1NRj =MATHEMATICAL EIGENVALUES
NRj
j
ireal iimag yiTAiI– yiTAiI–
E
kreal kimag k 1NFj =PHYSICAL EIGENV ALUES
NFjj
ireal iimag iTMi2BiK + +   Mi2Bi2K + +  i&lt;/p&gt;
&lt;p&gt;217 Chapter 7: Complex Eigenvalue Analysis
Complex Eigenvalue Diagnostics
Main Index
The accepted physical eigenvalues are also printed in the regular complex eigenvalue summary output of the
CEAD module: CLAMA. Finally, an eigenvalue summary table is always printed as follows:
EIGENVALUE ANALYSIS SUMMARY (COMPLEX LANCZOS METHOD)
Complex Lanczos Messages and Errors
To monitor the process (especially in the case of multiple shifts), the complex Lanczos method may issue the
following messages:
UWM 5451 :
NO ROOTS FOUND AT THIS SHIFT.
UIM 5453:
FEWER ROOTS THAN REQUIRED HAVE BEEN FOUND.
UWM 5452 :
NO ROOTS ACCEPTED AT THIS SHIFT.
UIM 5445:
NO ROOTS FOUND AT ALL.
UIM 5444:
ALL ROOTS HAVE BEEN FOUND.
The following message may be repeated up to three times:
UIM 5443:
DYNAMIC MATRIX IS SINGULAR AT THE SHIFT OF X, Y.
The program attempts to perturb the shift point (up to three times) to obtain an acceptable decomposition.
SWM 6938,* :
BREAKDOWN IN BLOCK LANCZOS METHOD.
This message is given on the various (*) breakdown conditions of the Lanczos process along with a
recommendation.
UFM 5446 :
COMPLEX LANCZOS NEEDS X MORE WORDS.
This message could come from various places.
The user must provide more memory.
SIM 6941:
INVARIANT SUBSPACE DETECTED IN BLOCK LANCZOS.
This message indicates the need for augmenting the current block.NUMBER OF MODES FOUND X
NUMBER OF SHIFTS USED X
NUMBER OF DECOMPOSITIONS X
NUMBER OF VECTORS IN CORE X&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Complex Eigenvalue Diagnostics218
Main Index
SFM 6939.* :
UNABLE TO READ EIGENVECTORS FROM SCRATCH FILE.
USER ACTION: CLEAN UP DEVICE.
These I/O related messages should not occur normally. If they do occur, the user should clean up the disk,
verify the database allocations, etc. (* =0, 1 or 2)
Performance Diagnostics
Block CLAN Performance Analysis
*
*
&lt;em&gt;USER INFORMATION MESSAGE 5403 (CLASD&lt;/em&gt;)
BREAKDOWN OF CPU USAGE DURING COMPLEX LANCZOS ITERATIONS:
OPERATION REPETITION
STIMES (SEC):
AVERAGETOTAL
SHIFT AND FACTOR 5 .8 3.9
MATRIX-VECTOR MULTIPLY &amp;amp; FBS 908 .1 93.2
REORTHOGONALIZATION 574 .1 38.3
SOLVE BLOCK TRIDIAGONAL PROBLEM 74 .5 34.5
EIGENVECTORS AND RESIDUALS 5 13.8 69.2
*
*
*SYSTEM INFORMATION MESSAGE 6940 (CLRRDD)
SPILL OCCURRED WHEN CALCULATING LANCZOS
VECTORS.
X OUT OF A TOTAL OF Y LANCZOS VECTORS HAVE BEEN STORED OUT OF CORE.
USER ACTION: TO PREVENT SPILL, INCREASE OPEN CORE SIZE BY AT LEAST Z
WORDS
*
*
*SYSTEM INFORMATION MESSAGE 6940 (CLRVRD)
SPILL OCCURRED WHEN CALCULATING PHYSICAL EIGENVECTORS.
USER ACTION: TO PREVENT SPILL, INCREASE MAXIMUM BLOCK SIZE BY AT LEAST Z&lt;/p&gt;
&lt;p&gt;219 Chapter 7: Complex Eigenvalue Analysis
Complex Eigenvalue Diagnostics
Main Index
Orthogonality Analysis
T wo additional orthogonality  criteria (see theory in Theory of Complex Eigenvalue Analysis , 166) that can be used
for solution testing purposes are the  matrices.
The matrices created by these criteria should be diagonal. The off-diagonal terms are supposed to be
computational zeroes . The absolute magnitude of the off-diagonal terms is a good indicator of the
correctness of the solution.
The user can use the following DMAP to calculate these matrices in connection with the CEAD module:
The EIGENV matrix is a matrix containing the eigenvalues on the diagonal. This matrix can be created as a
conversion of the CLAMA table by
The user can filter out the small terms by using:
The filtered  matrices should only contain off-diagonal terms greater than . Terms greater than
point to those eigenpairs that do not satisfy the orthogonality criterion.
The DMAP statement
can be used to print out the matrices. SMPYAD LCPHX,K, CPHX,,,/PTKP/3////1 $
MPYAD LCPHX,EI GENV,/PL $
MPYAD LCPHX,EI GENV,/PTL $
SMPYAD PTL,M,PL ,,,PTKP/O1/3//-1//1 $
SMPYAD LCPHX,B, CPHX,,,/PTBP/3////1 $
SMPYAD LCPHX,M, PL,,,PTBP/SUM2/3////1 $
SMPYAD PTL,M,PH I,,,SUM2/O2/3//+1//1 $
LAMX ,,CLAMA/EIGE NV/-2 $
MATMOD O1,O2,,,,/ORTHO1F,ORTHO2F/
2////1.-6 $
MATPRN ORTHO1F,O RTHO2F// $O1O2
10x–
106–
106–&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Complex Lanczos Estimates and Requirements220
Main Index
Complex Lanczos Estimates and Requirements
The time estimates  for single vector complex Lanczos are detailed below.
Shifting time (sec) is
(7-216)
Recursion time (sec) is
(7-217)
Normalization time (sec) is
(7-218)
Packing time (sec) is
(7-219)
where:
The minimum storage requirements are as follows:&lt;/p&gt;
&lt;p&gt;where:
The rest of the available memory is used for temporary storage of accepted eigenvectors to reduce the I/O
cost of outer orthogonalization.=number of modes desired
= number of Lanczos steps
=decomposition time (see Decomposition Estimates and Requirements , 71 for details)
=solution time (see Decomposition Estimates and Requirements , 71 for details)
=average front size
Memory:
=maximum block size
=maximum size of matrix
=problem sizeTd
4NstepsNCM Ts+  
2IPREC NdesN2M
2IPREC NdesNP
Ndes
Nsteps
Td
Ts
C
2IPREC 6MBLKSZ28NMBLKSZ8MBLKSZMSZT   +  +  
MBLKSZ
MSZT Tj
N&lt;/p&gt;
&lt;p&gt;221 Chapter 7: Complex Eigenvalue Analysis
References
Main Index
References
Bai, Z., et al. ABLE: An Adaptive Block Lanczos Method for Non-Hermitian Eigenvalue Problems .
Department of Mathematics, University of Kentucky, 1996.
Cullum, J. K.; Willoughby, R. A. Lanczos Algorithms for Large Symmetric Eigenvalue Computations.&lt;br&gt;
Birkhäuser, 1985.
Cullum, J.; Willoughby, R. A. Large Scale Eigenvalue Problems. North-Holland, 1986.
Golub, G. H.; Van Loan, C. F . Matrix Computations . John Hopkins University Press, 1983.
Householder, A.S.; Bauer, F .L. On Certain Methods for Expanding the Characteristic Polynomial.
Numerische Mathematik, Volume 1, 1959, pp. 29-37.
Komzsik, L. Implicit Computational Solution of Generalized Quadratic Eigenvalue Problems.  Journal
of Finite Element Analysis and Design, 2000.
Kowalski, T . Extracting a Few Eigenpairs of Symmetric Indefinite Matrix Pairs . Ph.D. Thesis, University of
Kentucky, 2000.
Smith, B. T . et al. Matrix Eigensystem Routines - EISPACK Guide.  Springer Verlag, 1974.
Wilkinson, J. H. The Algebraic Eigenvalue Problem . Oxford University Press, 1965.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
References222
Main Index&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P12</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_012/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_012/</guid>
      <description>
        
        
        &lt;p&gt;Main Index
Glossary
Glossary&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Glossary224
Main Index
AVG Average.
BUFFER An area of memory reserved for data transfer between secondary storage and memory.
BUFFPOOL A poll of buffers used by the executive system.
BUFFSIZE Buffersize in words (in machine precision).
C Average front size.
CEAD Complex eigenvalue analysis module.
cell An element of the SYSTEM common block of MSC Nastran.
CLAN Complex Lanczos method identifier.
DECOMP Matrix decomposition functional module.
Dense A matrix or vector is dense when it has few or no zero elements.
DIAGs Diagnostic flags of MSC Nastran.
DOF(s) Degree(s)-of-freedom.
EXECUTIVE The portion of MSC Nastran controlling the execution of the program.
FACTOR T riangular matrix, which is the result of DECOMP .
FBS Forward-backward substitution functional module.
GINO General input-output system.
GIV Givens method identifier.
HESS Hessenberg method identifier.
HOU Householder method identifier.
ID Identification.
IPREC Machine precision: 1 for short-word machines.
2 for long-word machine.
Kernel Internal numerical and I/O routines used heavily by functional modules.
Keyword A word specific to a particular function or operation in MSC Nastran.
LHS Left-hand side.
M M value, unit numerical kernel time in msec.
MAXRATIO A diagnostics parameter to show ill-conditioning.
MEM Memory area reserved for memory files.
MPC Multipoint constraint.
MPYAD Matrix multiply and add functional module.
N Problem size.
NZ Nonzero words in matrix.
P P value, unit data packing time using columns.&lt;/p&gt;
&lt;p&gt;225 Glossary
Main Index
PARALLEL Keyword to specify multiple CPU execution.
Pi Pi value, unit data packing time using terms.
Ps Ps value, unit data packing time using strings.
RAM Random access memory area used by the executive system.
READ Real eigenvalue analysis module.
RHS Right-hand side.
RMS Root mean squared.
SEQP Sequencing module.
SOLVIT Iterative equation solution module.
Sparse A matrix or vector is sparse when it has many zero elements.
SPARSE Keyword to specify indexed kernel usage.
String A sequence of consecutive nonzero terms in a matrix column.
STRL Average string length.
STURM
NumberNumber of negative terms on the factor diagonal.
T railer An information record following (trailing) a matrix, which contains the main
characteristics.
Matrix density. &lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide 226
Main Index&lt;/p&gt;
&lt;p&gt;Main Index
Bibliography
Bibliography&lt;/p&gt;
&lt;p&gt;228
Main Index
Babikov, P . &amp;amp; Babikova, M. An Improved Version of Iterative Solvers for Positive Definite Symmetric Real and Non-
Hermitian Symmetric Complex Problems . ASTE, JA-A81, INTECO 1995.
Bai, Z., et al. ABLE: An Adaptive Block Lanczos Method for Non-Hermitian Eigenvalue Problems . Department of
Mathematics, University of Kentucky, 1996.
Brown, John. Price/Performance Analysis of MSC/NASTRAN . Proc. of the Sixteenth MSC Eur. Users’ Conf., Paper No.
17, September, 1989.
Bunch, J. R.; Parlett, B. N. Direct Methods for Solving Symmetric Indefinite Systems of Linear Equations.  Society for
Industrial and Applied Mathematics Journal of Numerical Analysis, Volume 8, 1971.
Caldwell, Steve P .; Wang, B.P . An Improved Approximate Method for Computing Eigenvector Derivatives in
MSC/NASTRAN . 1992 MSC World Users’ Conf. Proc., Vol. I, Paper No. 22, May, 1992.
Chatelin, F . Eigenvalues of Matrices . Wiley, 1993.
Chan, T .; Wan, W. L. Analysis of Projection Methods for Solving Linear Systems with Multiple Right Hand Sides . CAM
Report #26, UCLA, 1994.
Chiang, K. N.; Komzsik, L. The Effect of a Lagrange Multiplier Approach in MSC/NASTRAN on Large Scale Parallel
Applications . Comp. Systems in Engineering, Vol 4, #4-6, 1993.
Conca, J.M.G. Computational Assessment of Numerical Solutions . ISNA ’92, Praque, 1992
Cullum, J. K.; Willoughby, R. A. Lanczos Algorithms for Large Symmetric Eigenvalue Computations.  Birkhäuser, 1985.
Cullum, J.; Willoughby, R. A. Large Scale Eigenvalue Problems. North-Holland, 1986.
Duff, I. S.; Reid, J. K. The Multifrontal Solution of Indefinite Sparse Symmetric Linear Systems.  Harwell Report CSS122,
England, 1982.
Efrat, I.; Tismenetsky, M. Parallel iterative solvers for oil reservoir models . IBM J. Res. Dev. 30 (2), 1986.
Francis, J. G. F . The QR Transformation, A Unitary Analogue to the LR Transformation.  The Computer Journal, Volume
4, No. 3, Oct. 1961, and No. 4, Jan. 1962.
George, A.; Liu, J. W. Computer Solutions of Large Sparse Positive Definite Systems. Prentice Hall, 1981.
Givens, W. Numerical Computation of the Characteristic Values of a Real Symmetric Matrix . Oak Ridge National Lab.,
ORNL-1574, 1954.
Goehlich, D.; Komzsik, L. Decomposition of Finite Element Matrices on Parallel Computers . Proc. of the ASME Int.
Computers in Engineering Conf., 1987.
Goehlich, D.; Fulton, R.; Komzsik, L. Application of a Parallel Equation Solver to Static FEM Problems . Int. J. for
Computers and Structures, 1989.
Gockel, M. A. Handbook for Dynamic Analysis . The MacNeal-Schwendler Corp., Los Angeles, 1983.
Golub, G. H.; Van Loan, C. F . Matrix Computations . John Hopkins University Press, 1983.
Grimes, R. G.; Lewis, J. G.; Simon, H. D.; Komzsik, L.; Scott, D. S. Shifted Block Lanczos Algorithm in MSC/NASTRAN .
MSC/NASTRAN Users’ Conf. Proc. Paper No. 12, March, 1985.&lt;/p&gt;
&lt;p&gt;229
Main Index
Grimes, R. G., et al. A Shifted Block Lanczos Algorithm for Solving Sparse Symmetric Generalized Eigenproblems . SIAM,
J. Mat. Analysis Appl., 13, 1992.
Hageman &amp;amp; Young. Applied Iterative Methods . Academic Press, 1981.
Hendrickson, B., Rothberg, E. I mproving the Runtime and Quality of Nested Dissection Ordering . Silicon Graphics,
Inc., Mountain View, CA, April 11, 1996.
Householder, A.S.; Bauer, F .L. On Certain Methods for Expanding the Characteristic Polynomial. Numerische
Mathematik, Volume 1, 1959, pp. 29-37.
Karypis, G., Kumar, V. METIS©, A Software Package for Partitioning Unstructured Graphs, Partitioning Meshes, and
Computing Fill-Reducing Orderings of Sparse Matrices, Version 3.0.3 , University of Minnesota, Department of Computer
Sciences/Army HPC Research Center, Minneapolis, MN, November 5, 1997. (&lt;a href=&#34;http://www.cs.umn.edu/~karypis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.umn.edu/~karypis&lt;/a&gt;)
Komzsik, L. Implicit Computational Solution of Generalized Quadratic Eigenvalue Problems.  Journal of Finite
Element Analysis and Design, 2000.
Komzsik, L. New Features of the Lanczos Module in Version 67 of MSC/NASTRAN . Proc. of the 18th MSC Eur. Users’
Conf., Paper No. 13, June, 1991, Rome.
Komzsik, L. Optimization of Finite Element Software Systems on Supercomputers . Supercomputing in Engineering
Analysis, ed. Hojjat Adeli, Marcel-Dekker, 1991.
Komzsik, L. Parallel Processing in Finite Element Analysis . Finite Element News, England, June, 1986.
Komzsik, L. Parallel Static Solution in Finite Element Analysis . The MSC 1987 World Users Conf. Proc., Vol. I, Paper No.
17, March, 1987.
Komzsik, L; Rose, T . Substructuring in MSC/NASTRAN for Large Scale Parallel Applications . Computing Systems in
Engineering, Vol 2, #1, 1991.
Kowalski, T. Extracting a Few Eigenpairs of Symmetric Indefinite Matrix Pairs . Ph.D. Thesis, University of Kentucky,
2000.
Lanczos, C. An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators.&lt;br&gt;
Journal of the Research of the National Bureau of Standards., Volume 45, 1950, pp. 255-282.
Lewis, J. G.; Grimes, R. G. Practical Lanczos Algorithms for Solving Structural Engineering Eigenvalue Problems. Sparse
Matrices, and Their Uses, edited by I. E.Duff, Academic Press, London, 1981.
Levy, R.; Wall, S. Savings in NASTRAN Decomposition Time by Sequencing to Reduce Active Columns . NASTRAN: Users’
Exper., pp. 627-632, September, 1971, (NASA TM X-2378).
MacNeal, R. H.; Komzsik, L. Speeding Up the Lanczos Process.  RILEM, Kosice, Slovakia, 1995.
MacNeal, R. H. The NASTRAN Theoretical Manual . The MacNeal-Schwendler Corp., Los Angeles, 1972.
Manteuffel, T. A. An Incomplete Factorization T echnique for Positve Definite Linear Systems , Math. of Computation, Vol
34, #150, 1980.
McCormick, C.W. Review of NASTRAN Development Relative to Efficiency of Execution . NASTRAN: Users’ Experience,
pp. 7-28. September, 1973. (NASA TM X-2893)&lt;/p&gt;
&lt;p&gt;230
Main Index
Mera, A. MSC/NASTRAN’s Numerical Efficiency for Large Problems on CYBER Versus Cray Computer . Proc. of the
MSC/NASTRAN Eur. Users’ Conf., June, 1983.
Newmark, N. M. A Method of Computation for Structural Dynamics. Proceedings of the Americal Society of Civil
Engineers, 1959.
Ortega, J. M.; Kaiser, H. F . The LR and QR Methods for Symmetric Tridiagonal Matrices.  The Computer Journal, Volume
6, No. 1, Jan. 1963, pp. 99-101.
Parlett, B. N. The Symmetric Eigenvalue Problem . Prentice Hall, Englewood Cliffs, 1980.
Petesch, D.; Deuermeyer, D.; Clifford, G. Effects of Large Memory on MSC/NASTRAN Performance . Proc. of the 18th
MSC Eur. Users’ Conf., Paper No. 12, June, 1991.
Pissanetzsky, S. Sparse Matrix Technology.  Academic Press, 1984.
Poschmann, P .; Komzsik, L. Iterative Solution Technique for Finite Element Applications . Journal of Finite Element
Analysis and Design, 19, 1993.
Poschmann, P .; Komzsik, L., Sharapov, I. Preconditioning T echniques for Indefinite Linear Systems . Journal of Finite
Element Analysis and Design, 21, 1997.
Rothberg, E. Ordering Sparse Matrices Using Approximate Minimum Local Fill , Silicon Graphics, Inc., Mountain View,
CA, April 11, 1996.
Saad, Y. Numerical Methods for Large Eigenvalue Problems . Halsted Press, 1992.
Shamsian, S.; Komzsik, L. Sparse Matrix Methods in MSC/NASTRAN . The MSC 1990 World Users Conf. Proc., Vol. II,
Paper No. 64, March, 1990.
Sinkiewicz, J. E. Numerical Stability of Fine Mesh T orus Models . Proc. of the MSC/NASTRAN Users’ Conf., March, 1979.
Smith, B. T . et al. Matrix Eigensystem Routines - EISPACK Guide.  Springer Verlag, 1974.
Wilkinson, J. H. The Algebraic Eigenvalue Problem . Oxford University Press, 1965.
Wilkinson, J. H. The Calculation of the Eigenvectors of Codiagonal Matrices.  The Computer Journal, Volume 1, 1958,
p.90.&lt;/p&gt;
&lt;p&gt;MSC.Mvision Builder and Evaluator 2002 Installation Guide
Index
Numerical Methods User’s Guide&lt;br&gt;
Main Index
A
accuracy
required , 161
analysis
performance , 32
analytic
data, 30
B
backward
substitution , 76
buckling, 106
BUFFPOOL, 18
BUFFSIZE, 18
C
canonical
form, 106, 108
cells
system , 18
constants
timing , 28
criterion
convergence
Hessenberg, 214
termination
Lanczos, 162
D
damping
options , 211
density, 48
deselection
MPYAD method , 51
DIAG
flags, 21
DISTRIBUTED PARALLEL, 18E
EIGC Bulk Data entry
specification of , 209
eigenvalues
complex , 166
real, 106
EIGRL
entry, 152
empirical
data, 30
estimates, 55, 71, 163, 220
executive
system , 31
F
filtered
matrix , 219
forward
substitution , 76
G
Givens, 149
H
Hessenberg
method , 206
HICORE, 18
Householder
method , 149
I
identifiers
method , 48
ill-conditioning, 68
incompatible
matrices , 55
IORATE, 18
iterative, 106Index&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
232
Main Index
K
kernel
functions , 26
L
Lanczos, 106, 161, 206
left-handed, 76
loop
inner
outer, 29
outer, 30
M
matrix
condition , 68
decomposition, 58
multiplication , 36
trailers, 22
MAXRATIO
parameter , 69
memory estimates, 31
MPYAD
module , 49
N
negative
terms on factor diagonal , 69
normalization, 153, 212
O
orthogonality
test, 219
P
PARALLEL, 18
performance
analysis , 32
pivot
threshold , 67
R
REAL, 18reduction
method , 106
RITZ
vectors , 162
S
selection
MPYAD method , 52
shifting scale, 162
singularity, 68
SMPYAD
module , 49
space
saver , 154
SPARSE, 18
sparse
decomposition, 69
spill
algorithm , 212
storage
requirements , 31
STURM
number , 69
system cells, 18
T
THRESH, 67
trailer
matrix , 22
U
USPARSE, 18
V
vector
kernels , 26&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P2</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_002/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_002/</guid>
      <description>
        
        
        &lt;p&gt;23 Chapter 1: Utility Tools and Functions
Matrix Trailers
= 15 sparse unsymmetric factor
Matrix type (T)
= 1 for real, single precision
= 2 for real, double precision
= 3 for complex, single precision
= 4 for complex, double precision
Number of nonzero words in the densest column:  (NZWDS)
Density (DENS)
Calculated as:
Trailer Extension .  In addition, an extension of the trailer is available that contains the following
information:
Number of blocks needed to store the matrix (BLOCKS)
Average string length (STRL)
Number of strings in the matrix (NBRSTR)
Three unused entries (BNDL, NBRBND, ROW1)
Average bandwidth (BNDAVG)
Maximum bandwidth (BNDMAX)
Number of null columns (NULCOL)
This information is useful in making resource estimates. In parentheses the notation used in the DIAG8
printout of the .f04 file is given.
The matrices of MSC Nastran were conventionally stored as follows:
The matrix header record was followed by column records and concluded with a trailer record. The columns
contained a series of string headers, numerical terms of the string and optionally a string trailer. The strings
are consecutive nonzero terms. While this format was not storing zero terms, a must in finite element analysis,
it had the disadvantage of storing topological integer information together with numerical real
data.Currently, the following indexed matrix storage scheme is used on most matrices:
Indexed Matrix Structure .  An Indexed Matrix is made of three files, the Column, String and Numeric
files.
Each file consists of only two GINO Logical Records:
HEADER RECORD. For the Column file, it contains the Hollerith name of the data block
(NAME) plus application defined words. For the String file, it contains the combined data block
NAME and the Hollerith word STRING. For the Numeric file, it contains the combined data block
NAME and the Hollerith word NUMERIC.number of terms
COLSROWS&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;10,000&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions24
DATA RECORD. It contains the Column data (see Indexed Matrix Column data Descriptions) for
the Column file, the String data for the String file and the numeric terms following each other for
the Numeric file.&lt;/p&gt;
&lt;p&gt;25 Chapter 1: Utility Tools and Functions
Matrix Trailers
Indexed Matrix File Structure
Column File String File Numeric File
Header
Record 0 as written by application (Data block
NAME + application defined words)Data block NAME + “STRING” Data block
NAME +
“NUMERIC”
Data Record
&lt;em&gt;6\3 words per Column Entry
Word 1\first 1/2 of 1:
Column Number, negative if the column is
null
Word 2\second 1/2 of 1:
Number of Strings in Column
Word 3 and 4\2:
String Relative
Pointer to the first String of Column
Word 5 and 6\3:
Relative Pointer to the first T erm of Column
Note : If null column, then word(s) 3 to 4\2
points to the last non null column
String Pointer, word(s) 5 to 6\3 points to the
last non null
column Numeric Pointer&lt;/em&gt;2\1 word(s) per String Entry
Word 1\first 1/2 of 1:
Row number of first term in
String
Word 2\second 1/2 of 1:
Number of terms in StringAll matrix numerical
terms following each
other in one Logical
GINO Record
*n1\n2 words, where
n1 is the number of words on short word machines
n2 is the number of words on long words machines&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions26
Kernel Functions
The kernel functions are internal numerical and I/O routines commonly used by the functional modules.
Numerical Kernel Functions .  T o ensure good performance on a variety of computers, the numerical
kernels  used in MSC Nastran are coded into regular and sparse kernels as well as single-level and double-level
kernels. The regular (or vector) kernels execute basic linear algebraic operations in a dense vector  mode. The
sparse kernels deal with vectors described via indices.  Double-level kernels are block (or multicolumn)
versions of the regular or sparse kernels.
The AXPY kernel executes the following loop:
where:
The sparse version (called AXPl) of this loop is&lt;/p&gt;
&lt;h1&gt;where
In these kernels,  and  are vectors.  INDEX is an array of indices describing the sparsity pattern of the
vector.  A specific MSC Nastran kernel used in many occasions is the block AXPY (called XPY2 in MSC
Nastran).
where:
Here ,  are blocks of vectors (rectangular matrices),  is an array of scalar multipliers, and  is the
number of vectors in the block.
Similar kernels are created by executing a DOT product loop as follows:=
=a scalar
=the vector length&lt;/h1&gt;&lt;p&gt;=Yis= Xi Yi +
i 12n
s
n
YINDEXi  s= Xi  YINDEXi   +
i12n =
X Y
X
YijSj = Xij Yij +
i 12n
j 12b
XY S b
DOT:  Sum Xi
i1=n
= Yi&lt;/p&gt;
&lt;p&gt;27 Chapter 1: Utility Tools and Functions
Kernel Functions
where:
Indexed versions of the XPY2 and DOT2 kernels also exist.
To increase computational granularity and performance on hierarchic (cache) memory architectures, the
heavily used triangular matrix update kernels are organized in a triple loop fashion.
The DFMQ kernel executes the following mathematics:
where  is a triangular or trapezoidal matrix (a portion of the factor matrix) and  are vectors.
The DFMR kernel executes a high rank update of form
where now  and  are rectangular matrices.  All real, complex, symmetric, and unsymmetric variations of
these kernels exist, but their description requires details beyond the scope of this document.
Triple Look Kernels .  Additional triple loop kernels are the triple DOT (DOT3) and SAXPY (XPY3)
routines. They are essentially executing matrix-matrix operations. They are also very efficient on cache-
machines as well as very amenable to parallel execution.
I/O Kernel Functions .  Another category of kernels contains the I/O kernels. The routines in this
category are invoked when a data move is requested from the memory to the I/O buffers.
Support Kernels .  Additional support kernels frequently used in numerical modules are ZEROC, which
zeroes out a vector; MOVEW, which moves one vector into another; and SORTC, which sorts the elements
of a vector into the user-requested (ascending or descending) order.=the block size
=DOT1:  Sum XINDEXi  
i1=n
Yi =
DOT2:  Sumj Xij
i1=n
= Yij
b
j 12b
AA= uv+
A uv
AA= UV+
U V&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions28
Timing Constants
Single Loop Kernel Performance .  Timing constants  are unit (per term) execution times of numerical
and I/O kernels.  A typical numerical kernel vector performance curve shows unit time T as a function of the
loop length.  A loop is a computer science structure that executes the same operation repeatedly.  The number
of repetitions is called the loop length.&lt;/p&gt;
&lt;p&gt;Figure 1-1  Single-Level Kernel Performance Curve
The kernel performance curve can be described mathematically as
where the constant  is characteristic of the asymptotic performance of the curve since
The constant  represents the startup overhead of the loop as
These constants for all the MSC Nastran numerical kernels can be printed by using DIAG 58.Loop (vector)
Length sUnit
Time: T
1 2 . . . 1024
TA=B
s&amp;mdash;+
A
Ts= A=
B
Ts1=A= B+&lt;/p&gt;
&lt;p&gt;29 Chapter 1: Utility Tools and Functions
Timing Constants
Sometimes it is impossible to have a good fit for the datapoints given by only one curve. In these cases, two
or more segments are provided up to a certain break point in the following format:
where X is the number of segments and Y is the name of the particular kernel.
Double Loop Kernel Performance .  In the case of the double loop kernels, the unit time is a function
of both the inner loop  length and the number of columns, which is the outer loop length.  The unit time is
described by a surface as shown in Figure 1-2.
Figure 1-2  Double-Level Kernel Performance Surface
The surface on Figure 1-2 is built from curves obtained by fixing a certain outer loop length and varying the
inner loop length.  Intermediate values are found by interpolation.  Another set of curves is obtained by fixing
the inner loop length and varying the outer loop length.X Segments for Kernel Y
Segment 1 Segment 2
Break Point Break Point
A1 A2
B1 B2
Unit Time: T
Outer Loop Length
Inner Loop CurvesOuter Loop Curves
1
2
1024
Inner Loop Length..
.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions30
I/O Kernels .  There are many I/O kernels also in MSC Nastran.
The unit time for these kernels for string or term operations is
For column operations (PACK, UNPACK),
and the two values are given for real and complex  and  values.
Triple Loop Kernels .  The triple loop kernels are now included in the time estimate (GENTIM) process
of MSC Nastran.
While difficult to show diagramatically, the timing model for the triple loop kernels can be thought of as
families of double loop surfaces as shown in Figure 1-2. A family is generated for specified lengths of the
outermost loop. Values intermediate to these specified lengths are determined by interpolation.
Time Estimates
Calculating time estimates for a numerical operation in MSC Nastran is based on analytic and empirical data.
The analytic  data is an operation count that is typically the number of multiply (add) operations required to
execute the operation.  In some cases the number of data movements is counted also.
The empirical  data is the unit time for a specific kernel function, which is taken from the timetest tables
obtained by DIAG 58 and explained in Timing Constants , 28. These tables are generated on the particular
computer on which the program is installed and stored in the database.
The operation count and the execution kernel length are calculated using information contained in the
matrix trailers. Sometimes trailer information from the output matrix generated by the particular operation
is required in advance.  This information is impossible to obtain without executing the operation.  The
parameters are then estimated in such cases resulting in less reliable time estimates.
Available Time .  Time estimates in most numerical modules are also compared with the available time
(TIME entry). Operations are not started or continued if insufficient time is available.
I/O time estimates are based on the amount of data moved (an analytic data item) divided by the IORATE
and multiplied by the I/O kernel time. Since the user can overwrite the default value of the IORATE
parameter, it is possible to increase or decrease the I/O time calculated, which also results in varying the
method selection.
In most numerical modules, MSC Nastran offers more than one solution method. The method used can be
selected by the user. The method automatically selected by MSC Nastran is based on time estimates. The
estimated (CPU) execution time is calculated by multiplying the number of numerical operations by the unit
execution time of the numerical kernel executing the particular operation. In addition, an estimation is given
for the (I/O) time required to move information between the memory and secondary storage.  After the
estimates for the CPU execution time and the l/O time are added together, MSC Nastran selects the method
that uses the least time.Tsnumber of strings = A number of nonzeroes + B
TcTs= rows + columnsA  
A B&lt;/p&gt;
&lt;p&gt;31 Chapter 1: Utility Tools and Functions
Storage Requirements
Matrix Methods .  Several methods are offered because each of them is best suited to certain types of
matrices.  The difference in cost among the methods for specific cases can be an order of magnitude or more.&lt;br&gt;
As each matrix is generated, the parameters describing its size and the distribution of nonzero terms are stored
in a matrix trailer.  (The parameters that define the properties of the matrices were described in Matrix Trailers ,
22.)  For each matrix, these parameters include the number of rows and columns, the form (for example,
square or symmetric), the type (for example, real or complex), the largest number of nonzero words in any
column, and the density.  Some of the newer methods also record the number of strings in the matrix.  Other
descriptive parameters may be added in the future.
The only empirical data used in deriving the timing equations is the measurement of the time per operation
for the kernels.  These measurements are computed at the time of installation on each computer and are
stored in the delivery database for later use.  After the system is installed, the measurements may be updated
if faster hardware options are installed on the computer.  The remaining terms in the equations are derived
from careful operation counts, which account for both arithmetic and data storage operations.
Timing Equations .  Timing  equations are derived for all major numerical modules.  Conservative upper
bounds are the best estimates that can be calculated.  At present, these estimates are not used for method
selection.  Instead, the user is required to input the total amount of available CPU time to solve the total run.&lt;br&gt;
The amount of time remaining at the start of the numerical solution modules is compared with the estimate.&lt;br&gt;
The run is terminated before the numerical module starts execution if the amount of time remaining is less
than the estimate.  The goal is to minimize wasted computer resources by terminating expensive operations
before they start rather than terminating them midway through their operation before any output is available.
The many types of machine architecture which MSC Nastran supports and the great differences in operation
between scalar, vector, and parallel computing operations result in a challenge to the numerical method
developers to provide correct estimation and method selection.  There are a number of diagnostic tools which
can be used to print out the estimates and the other parameters affecting computation cost.  These tools are
generally activated by the DIAG flags described earlier.
Storage Requirements
Main storage  in MSC Nastran is composed of the space used for the code, the space used for the Executive&lt;br&gt;
System, and the actual working space used for numerical operations.&lt;br&gt;
Working Space = DIAG 13 .  The actual working space available for a numerical operation can be
obtained using DIAG 13.
Disk storage is needed during the execution of an MSC Nastran job to store temporary (scratch) files as well
as the permanent files containing the solution.
Memory Sections .  The Executive System provides the tools needed to optimize the execution using a
trade-off between memory  and disk.  The main memory is organized as follows:&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions32
RAM, MEM, BUFFPOOL .  The RAM area holds database files, while the SMEM area holds scratch files.&lt;br&gt;
The BUFFPOOL area can act as a buffer memory.  The user-selectable sizes of these areas have an effect on
the size of the working storage and provide a tool for tuning the performance of an MSC Nastran job by
finding the best ratios.
A general (module-independent) user fatal message associated with storage requirements is:
UFM 3008:
INSUFFICIENT CORE FOR MODULE XXXX
This message is self explanatory and is typically supported by messages from the module prior to message
3008.
Performance Analysis
.f04 Event Statistics .  The analysis of the performance  of an MSC Nastran run is performed using the
.f04 file.
Disk Usage .  The final segment of the .f04 file is the database usage statistics.  The part of this output most
relevant to numerical modules is the scratch space usage (the numerical modules are large consumers of
scratch space).  SCR300 is the internal scratch space used during a numerical operation and is released after
its completion.  The specific SCR300 table shows the largest consumer of internal scratch space, which is
usually one of the numerical modules.  The output HIWATER BLOCK shows the maximum secondary
storage requirement during the execution of that module. Working Storage
Executive
RAM
MEM
BUFFPOOLPrinted on DIAG 13
User-Controllable&lt;/p&gt;
&lt;p&gt;33 Chapter 1: Utility Tools and Functions
Parallel Processing
Memory Usage .  Another table in this final segment shows the largest memory usage in the run.  The
output HIWATER MEMORY shows the maximum memory requirement combining working storage and
executive system areas, described in Storage Requirements , 31.
Parallel Processing
Parallel processing in MSC Nastran numerical modules is a very specific tool.  It is very important in
enhancing performance, although its possibilities in MSC Nastran and in specific numerical modules are
theoretically limited.
The parallelization possibilities in MSC Nastran consist of three different categories:
High level
Frequency domain
Medium level
Geometry domain
Low level
Block kernels (high rank updates)
The currently available methods of parallel processing in MSC Nastran numerical modules are:
Shared memory parallel
Medium, low level
MPYAD, DCMP, FBS modules
Distributed memory parallel
High, medium level
SOLVIT, DCMP, FBS, READ modules
Details of the various parallel methods are shown in the appropriate Modules’ sections throughout.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Utility Tools and Functions34
Main Index&lt;/p&gt;
&lt;p&gt;Chapter 2: Matrix Multiply-Add Module
2 Matrix Multiply-Add Module
Multiply-Add Equation
Theory of Matrix Multiplication
MPYAD Methods
DMAP User Interface
Method Selection/Deselection
Option Selection
Diagnostics
MPYAD Estimates and Requirements&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module36
Main Index
Multiply-Add Equation
The matrix  multiply -add operation is conceptually simple.  However, the wide variety of matrix
characteristics and type combinations require a multitude of methods.
The matrix multiply -add modules (MPYAD and SMPYAD) evaluate the following matrix equations:
(MPYAD)
(2-1)
or
(SMPYAD)
(2-2)
The matrices must be compatible with respect to the rules of matrix multiplication. The  stands for
(optional) transpose. The signs of the matrices are also user parameters. In (2-2), any number (between 2 and
5) of input matrices can be present.
The detailed theory of matrix multiply -add operation is described in Theory of Matrix Multiplication , 36.
Subsequent sections provide comprehensive discussions regarding the selection and use of the various
methods.
Theory of Matrix Multiplication
The matrix multiplication module in MSC Nastran evaluates the following matrix equations:
(2-3)
or
where , , , and  are compatible matrices. The calculation of (2-3) is carried out by the following
summation:
(2-4)
where the elements , , , and  are the elements of the corresponding matrices, and  is the column
order of matrix  and the row order of matrix .  The sign of the matrices and the transpose flag are assigned
by user -defined parameters.D ATB  C =
G ATBTCTDTE  F =
T
D AB  C =
D ATB  C =
ABC D
dijaikbkj  cik
k1=n
=
abc d n
A B&lt;/p&gt;
&lt;p&gt;37 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
MSC Nastran has four major ways to execute (2-3) and performs the selection among the different methods
automatically.  The basis of the selection is the density pattern of matrices  and  and the estimated time
required for the different kernels.
Generally, these methods are able to handle any kind of input matrices (real, complex, single, or double
precision) and provide the appropriate result type.  Mixed cases are also allowed and are handled properly.
The effective execution of multiplication is accomplished by invoking the MSC Nastran kernel functions.
The four methods are summarized in the following table and explained in more detail below.
Method One (Dense x Dense)
Method one consists of several submethods. The submethod designated as method one storage 1 is also
known as basic method one.
In basic  method one, enough storage is allocated to hold as many non-null columns of matrices  and&lt;br&gt;
as memory allows.  The columns of matrix  corresponding to the non-null columns of  are initially read
into the location of matrix  (the result).  In basic method one, matrix  is processed on a string -by-string
basis.  The complete multiplication operation may take more than one pass when all the non-null columns
of matrices  and  cannot fit into memory.  The number of passes can be calculated as follows:
(2-5)
where:
The basic procedure of method one (storage 1) can be viewed as follows:Method Combination
1 Dense Dense
2 Sparse Dense
3 Sparse Sparse
4 Dense Dense
=order of problem
=number of passes
=number of non-null columns of  in memoryA B
B D
C B
D A
B D
NpN
NB&amp;mdash;&amp;mdash;- =
N
Np
NBB&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module38
Main Index
Figure 2-1  Method One
For each nonzero element of , all combinatorial terms of  currently in memory are multiplied and
accumulated in .    columns of matrix  are calculated at the end of one complete pass of matrix&lt;br&gt;
through the process . Then the completed columns of  are packed out, along with any non-null columns
of  that correspond to null columns of  skipped in this pass (as they are columns of ). This part is
saved,  non-null columns of  and the corresponding columns of  are loaded, and the process
continues.  The effective execution of  the multiplication depends on whether or not the transpose flag is set.
Nontranspose:
(2-6)
Transpose:
(2-7)
The other submethods provide for different handling of matrix  and for carrying out the multiplication
operations.  The main features of the submethods vary depending on the different ways of handling the bNBbjailB A
NBalij
b1D C=
j
i
NBailbj
or
alibj
In Memory In Memory
Note:   The underlined quantities in Figure 2-1 represent vectors.
A B
DNBD A
D
C B D
NBB C
dijail blj= dij+
dijali=  bljd+ij
A&lt;/p&gt;
&lt;p&gt;39 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
strings (series of consecutive nonzero terms in a matrix column).  A comparison of the main method and the
submethods is shown as follows:
Table 2-1  Nontranspose Cases
Table 2-2  Transpose CasesStorage A: Unpacked columns of  and
Processing  string by string
is in the inner loop
Storage B: Unpacked columns of  and
Processing  string by string
is in the outer loop
Storage C: Unpacked partial rows of&lt;br&gt;
Processing  string by string
is in the inner loop
Storage D: Partial rows of  in string format
Processing  string by string
is in the outer loop
Storage E: Unpacked columns of  and
Unpacked columns of  (band only)
is in the outer loop
Storage F: Partial rows of  in string format
Unpacked columns of
is in the outer loop
Storage 2: Unpacked non-null columns of  and
Unpacked columns of
Loops are the same as storage 1, except outermost loop
pulled inside the kernel (triple loop kernel)
Storage A: Unpacked columns of   and
Processing  string by string
is in the inner loop
Storage B: Unpacked columns of
Partial rows of
Processing  string by string
is in the outer loop
Storage C: Unpacked columns of  and
Unpacked rows of  (band only)
is in the outer loopB D
A
A
B D
A
A
B
A
A
B
A
A
B D
A
A
B
A
A
B D
A
B D
A
A
B
D
A
A
B D
A
A&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module40
Main Index
The effective execution of the multiplication operation in method one subcases (and other methods with the
exception of method three) is  accomplished by involving  the MSC Nastran kernel functions.  In method one
submethods, except storage 2, the double loop kernels of DOT2 and XPY2 are used. In storage 2, the triple
loop kernels of DOT3 are used.
Depending on whether the length of the current string of  is  or , the  string is in the inner  loop
or in the outer loop.  This explains the comments: &amp;quot;  is in the inner loop&amp;quot; or &amp;quot;  is in the outer loop&amp;quot; in
Table 2-1.  The selection between the previous two possible usages of the kernel functions depends on the
density of matrices  and .  If  is sparse, it is in  the inner loop; otherwise,  is in the outer loop.
Method Two (Sparse x Dense)
In the nontranspose case of this method, a single term of  and one full column of the partially formed&lt;br&gt;
are in the main memory.  The remaining main memory is filled with as many columns of  as possible.&lt;br&gt;
These columns of matrix  are in string  format.  This method is effective when  is large and sparse;&lt;br&gt;
otherwise, too many passes  of  must be taken.  The number of passes in  is calculated from (2-5).
The method can be graphically represented as follows:Storage D: Unpacked columns of
Partial rows of
Unpacked rows of
is in the outer loop
Storage 2: Unpacked non-null columns of  and
Unpacked columns of
Loops are the same as storage 1, except outermost loop
pulled inside the kernel (triple loop kernel)B
D
A
A
B D
A
AN M A
A A
A B A A
B D
A
A A
B B&lt;/p&gt;
&lt;p&gt;41 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
Figure 2-2  Method Two
When  is in memory, the k-th column of  is processed against it and the result is accumulated into the
k-th column of .  In the transpose case, one column of  is held in memory while  holds only a single
term at a time.  This method provides an alternative means of transposing matrix  by using the identity
matrix  and the zero matrix  when the transpose module of MSC Nastran is inefficient.
Method Three (Sparse x Sparse)
In method three, the transpose and nontranspose cases are essentially the same except for the initial transpose
of matrix  in the transpose case.  In both cases, matrix  is stored in the same way as in method two, i.e.,
in string format.  Matrix  is processed on an element -by-element basis, and the products of each  term
are calculated using the corresponding terms of  in memory.  However, in method three, the results and
storage are different from method two.  In  method three, a “storage bin” is set for the columns of matrix .&lt;br&gt;
The number of these bins is based on the anticipated density of matrix  and is calculated as follows:
(2-8)
where:
The size of the bins can be calculated as follows:=order of the problem
=density of  (estimated)B A
NBD C=
bkj aj
bkj
NAj
bkj In Memory k-th Column In Memory In Memory
bkjA
D B D
A
B C
A A
B bkj
A
D
D
Number of bins N=
N
 D&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module42
Main Index
(2-9)
This manner of storing the results takes advantage of the sparsity of matrix .  However, it requires sorting
in the bins before packing out the columns of .
Method Four (Dense x Sparse)
This method has two distinct branches depending on the transpose flag.
Nontranspose Case
First, matrix  is transposed and written into the results file.  This operation is performed with the
assumption that  is sparse.  As many columns of  as possible are unpacked into memory and the columns
of  (rows of ) are interpreted on a term -by-term basis.
Figure 2-3  Nontranspose Method Four
For each nonzero term of , the scalar product with the columns of  is formed and written into the
scratch file.  When all columns of  and rows of  are processed, the scratch  file contains one column for&lt;br&gt;
each nonzero term in .  Therefore, a final pass must be made to generate matrix .Size of binsN
Number of bins&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-1
&amp;mdash; = =
B
D
B
B A
BTB
BTA
NAbkj
NAj
bkj In Memory In Memoryak
bkj aj
In Memory In Scratch
BTA
A B
B D&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P3</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_003/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_003/</guid>
      <description>
        
        
        &lt;p&gt;43 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
Transpose Case
In this case, a set of rows of  (columns of ) are stored in an unpacked form.  These rows build a segment.&lt;br&gt;
Matrix  is processed on a string -by-string basis, providing the DOT products of the strings and the columns
of the segments.
Figure 2-4  Transpose Method Four
The results are sequentially written into the scratch  file continuously.  The structure of this file is as follows:
The number of segments is
(2-10)
Finally, the product matrix must be assembled from the scratch file, and matrix , if present, must be added.
Sparse Method
The sparse multiply method is similar to regular method three.  When  the transpose case is requested, matrix
is transposed prior to the numerical operations.  This step is typically not very expensive since  is sparse
when this method is selected.
The significance of this method is that matrix  is stored in a new sparse form.  Specifically, all nonzero terms
of a column are stored in a contiguous real memory region, and the row indices are in a separate integer array.  ATA
B
ATB
a1
arb1bna1b1 a1bn
arb1 arbn
CoI 1 CoI 2 &amp;hellip; CoI n CoI 1 &amp;hellip; CoI n  EOF
Seg 1 Seg 1 Seg 1 Seg 2 Seg n
kn
r&amp;mdash;=
C
A A
A&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module44
Main Index
The sparse kernel AXPI is used for the numerical work.  The scheme of this method is shown on the following
figure.
Figure 2-5  Sparse Method
From the figure it is clear that matrix  is processed on an element -by-element basis.  When all the nonzero
terms of  do not fit into memory at once, multiple passes are executed on matrix , and only partial results
are obtained in each pass.  In  turn, these results are summed up in a final pass.
The sparse method is advantageous when both matrices are sparse.
Triple Multiply Method
In MSC Nastran a triple multiplication operation involving only two matrices occurs in several places.  The
most common is the modal mass matrix calculation of . Note that in this multiplication operation
the matrix in the middle is symmetric.  Therefore, the result is also symmetric. No symmetry or squareness
is required for the matrices on the sides.  Historically, this operation was performed by two consecutive matrix
multiplications which did not take advantage of the symmetry or the fact that the first and third matrices are
the same.AAT
In MemoryjDB
C+ i k ij
Bkj        aij        dij       dijpartial= + 
B
A B
TM&lt;/p&gt;
&lt;p&gt;45 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
The operation in matrix form is
(2-11)
where:
Any element of matrix  can be calculated as follows:
(2-12)
where:
It can be proven by symmetry that
(2-13)
Based on the equality in (2-13), a significant amount of work can be saved by calculating only one -half of
matrix .
When designing the storage requirements, advantage is taken of the fact that matrix  is only needed once
to calculate the internal sums. Based on this observation, it is not necessary to have this matrix in the main
memory. Matrix  can be transferred through the main memory using only one string at a time.
The main memory is equally distributed among , , and three vector buffers. One of the vector
buffers must be a full column in length. Therefore, the structure of the main memory is as follows:=order of&lt;br&gt;
=  symmetric matrix
=  symmetric matrix
=  symmetric matrix
=the column index,
=the row index, C ATBAD =
A mn m columnsn rows  
B nn
C mm
D mm
C
Clkajkail bij
j1=n

    
dlk+
ik=n
=
k 1km
l 1lm
aljaim bij
i1=n

    
ajmail bij
ij=n

    
j1=n
=
j1=n

C
B
B
ATATB&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module46
Main Index
Figure 2-6  Memory Distribution for Triple Multiply
Three l/O buffers must be reserved for the three simultaneously open files of , , and the scratch file
containing partial results.  Therefore, the final main memory size is
(2-14)
From (2-14), the number of  columns fitting into memory can be calculated as follows:
(2-15)
The number of passes is calculated as follows:
(2-16)
which is equal to the number of times the triple multiply operation reads through the matrix .
The number of times the triple multiply operation reads through the matrix  can be approximated as
follows:One Column
Long Columns of k A  Rows of k ATBVector
BufferArea 1 Area 2
-Length
Buffersk
I/O Buffers
AB
nz2knn3 BUFFSIZE  2k ++ +  =
A
knz3 BUFFSIZE   – n–
2n2+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
pm
k&amp;mdash;- if m
k&amp;mdash;-integer =
int m
k&amp;mdash;-1+if m
k&amp;mdash;-integer
&lt;/h1&gt;&lt;p&gt;B
A&lt;/p&gt;
&lt;p&gt;47 Chapter 2: Matrix Multiply-Add Module
Theory of Matrix Multiplication
Main Index
(2-17)
The triple multiply method is implemented with a block spill logic where the result of the matrix  is
generated in  blocks.
Parallel Multiply Method
The parallel multiply method, which is only used when parallel processing is requested, is basically a parallel
version of method one designed to solve the CPU -intensive multiplication of dense matrices.
The storage structure of this  method is the same as that of method one.  However, the columns of matrix&lt;br&gt;
are distributed among the process es.  Consequently, although  is stored in the main (shared) memory, the
process es access different portions of it.
At the beginning of each pass, subtasks are created.  These subtasks wait for the columns of matrix  to be
brought into memory.  Once a column of  is in the main memory, all subtasks process this column with
their portion of matrix .  When all of the subtasks are finished, the main process  brings in a new column
of  and the process continues.  The parallel method is advantageous when matrix  is very dense and
multiple process es are available.
Figure 2-7  Parallel Multiply Method
Algorithms 1TC and 1NTE (see MPYAD Methods , 48) are executed in parallel when parallel MPYAD is
requested.  The parallel methods are represented in Figure 2-7.
The triple loop kernels used in Method 1 Storage 2 are also parallelized by some machine vendors providing
another way to execute parallel MPYAD.pAp
2&amp;mdash;=
C
kk
B
B
A
A
B
A B
In Memory In MemoryA CD B
123 123NTT CPU CPU&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module48
Main Index
MPYAD Methods
The methods in the MPYAD module are divided into six main categories:  four methods for different density&lt;br&gt;
combinations, one method for parallel execution, and one method for sparse operations.  These methods are
summarized in the following table:
There are no fixed density boundaries set for these methods.  Method selection is a complex topic.  Within
method one, there are also ten submethods for the special handling of the matrices.
Methods two and three are only selected in special cases.  In most cases, the sparse method replaces both
methods two and three.
The parallel multiply method is aimed at shared memory parallel computers.  It does not run on distributed
memory parallel computers.
The method 1 submethods ( A-F) are automatically deselected in some cases. As example is when the  and
matrices are the same, or another when any of the matrices are non-machine precision.
MPYAD Method Identifiers .  For selection and deselection purposes, identifiers  are assigned to certain
methods.  However, these identifiers are bit oriented, and in some cases their decimal equivalents are used:Method Combination
1
2
3
4
P
S
Method Bit Decimal
1NT 0 1
1T 1 2
2NT 2 4
2T 3 8
3NT 4 16
3T 5 32
4NT 6 64
4T 7 128
1NTA 8 256
1NTB 9 512DenseDense
SparseDense
SparseSparse
DenseSparse
DenseDense
SparseSparse
A
B&lt;/p&gt;
&lt;p&gt;49 Chapter 2: Matrix Multiply-Add Module
DMAP User Interface
Main Index
In the above table, T represents transpose, NT indicates nontranspose, and A, B, C, D, E, F , 2 are submethod
names when they appear as the last character.  For example, 1NTD is a method one, nontranspose case, D
submethod operation.
Bit 21 (with the corresponding decimal value of 2097152) is reserved for submethod diagnostics.
DMAP User Interface
The DMAP call for the MPYAD  module executing the operation in (2-1) is
where:
An alternative MPYAD call is to use the SMPYAD  module as follows1NTC 10 1024
1NTD 11 2048
1NTE 12 4096
1NTF 13 8192
1TA 14 16384
1TB 15 32768
1TC 16 65536
1TD 17 131072
SNT 18 262144
ST 19 524288
Deselect 20 1048576
DIAG 21 2097152
22 4194304
1NT2 23 8388608
1T2 24 16777216
AutoS2 25 33554432Method Bit Decimal
MPYAD A,B,C/D/T/SIGNAB/SIGNC/PREC/FORM
T =0,1: Non-transpose or transpose (see Multiply-Add Equation , 36)
PREC =0,1,2:  Machine, single, double, etc. (see Matrix Trailers , 22)
FORM =0,1,2:  Auto, square, rectangular, etc. (see Matrix Trailers , 22)&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module50
Main Index
:
where:
This module executes the operation in Equation (2-2)(2-2).
Method Selection/Deselection
MPYAD automatically selects the method with the lowest estimate of combined cpu and I/O time from a
subset of the available methods. Also, those methods that are inappropriate for the user’s specific problem are
automatically deselected. The user can override the automatic selection and deselection process by manual
selection, subject to certain limitations. The details of both automatic and user selection and deselection
criteria are described below.
Automatic Selection
By default, methods 1 (all submethods), 3, 4, and Sparse are available for automatic selection. If bit 25 of
System Cell 66 has been set (decimal value 33554432), method 2 is included in the automatic selection set
of methods. Also, if all of the default methods have been deselected, method 2 will be used, provided it was
not deselected. If all methods have been deselected, a fatal error occurs.
Automatic Deselection
If a method is determined to be inappropriate for the user’s problem, it will be automatically deselected.
Except in those cases noted below, an automatically deselected method will be unavailable for either
automatic selection or manual user selection. Note that any method that has insufficient memory to execute
the user’s problem will be deselected. The other automatic deselection criteria are described below for each
method. In this discussion &amp;ldquo;mpass I&amp;rdquo; stands for the number of passes required by method I.  and&lt;br&gt;
represent the density of the  matrix and  matrix, respectively. Also, unless the method name is qualified
by NT (non-transpose) or T (transpose), the criteria applies to both.SMPYAD A,B,C,D,E,F/G/N/SIGNG/SIGNF/PREC/TA/TB/TC
/TD/FORM
N =number of matrices given
TA,TB,TC,TD =transpose flags
FORM =as above
Method 1 – All Submethods If method S is not deselected and mpass1 is greater than
5mpass3 and  and  are less than 10%.
If MPYAD was called by the transpose module.
Method 1 – Storage A-F If the type of matrix  is real and either  or  is complex.AB
A B
AB
A B C&lt;/p&gt;
&lt;p&gt;51 Chapter 2: Matrix Multiply-Add Module
Method Selection/Deselection
Main Index
User Deselection
For method deselection , the following is required:If the type of matrix  is complex and both  and  are
real.
Method 1NT – Storage A, D,
and FUnless explicitly user selected.
Method 1T – Storage B
and C1TB unless 1TA deselected.
1TD unless 1TC is deselected.
Method 2 If method S is not deselected, unless user selected.
If the  matrix is non-null and the type of  is not machine
precision or the , , and  matrices are not all real or all
complex.
If MPYAD was called by the transpose module.
Method 3 If method S is not deselected, unless user selected.
If matrix  is real and the  and/or  matrix is complex.
Method 3NT If the requested type of the  matrix is real and any of the
input matrices are complex, or if the requested type of  is
complex and all of the input matrices are real.
Method 4 If methods 1, 2, 3, and S are not deselected and  greater
than 10%, unless method 4 has been user selected.
If matrix  is non-null and its type is not equal to machine
precision or the  and  matrices are not both real or not
both complex.
Method 4T If more than 100 &amp;ldquo;front-end&amp;rdquo; (R4) passes or more than 10
&amp;ldquo;back-end&amp;rdquo; (S4) passes.
Method Sparse If the type of matrix  is not machine precision.
If matrix  is non-null and  and  are not both real or
both complex.
If matrix  is complex and matrix  is real.
Method Parallel NT Unless 1NTE is not deselected.
If the number of columns per pass for 1NTE is less than the
number of available processes .
Method Parallel T Unless 1TC is available.
If the number of columns per pass for 1TC is less than the
number of available processes.A B C
C C
AB C
A B C
D
D
A
C
B C
B
C A C
A B&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module52
Main Index
Main Methods – Deselection
SYSTEM(66) = decimal value of method
Submethods – Deselection
SYSTEM(66) = decimal value of submethod
Sparse Method – Deselection
where SYSTEM (126) is equivalent to the SPARSE keyword.
Parallel Method – Deselection
where ncpu = number of CPUs and SYSTEM (107) is equivalent to the PARALLEL. keyword.
The philosophy of method selection is to deselect all the methods except for the one being selected.
Triple Loop Method – Deselection
SYSTEM(252) = 0 or &amp;gt; 100:  Do not use triple loops in 1T,1NT
User Selection
For method selection, the following is required:
Main Methods – Selection
SYSTEM(66) = 255 – decimal value of method identifier
Main or Submethods – Selection
SYSTEM(66) = 1048576 + bit value of method or submethod identifier
Sparse Method – Selection
SYSTEM(126) = 1: Auto selection  (This is the default.)
SYSTEM(126) = 3: Force sparse NT method
SYSTEM(126) = 5: Force sparse T method
SYSTEM(126) = 7: Force either T or NT sparseSYSTEM(126) = 0: Deselect all sparse methods
SYSTEM(126) = 2: Deselect sparse NT only
SYSTEM(126) = 4: Deselect sparse T only
SYSTEM(107) = 0:  Deselect all parallel modules
SYSTEM(107) = 4096 + ncpu:  Deselect parallel MPYAD only&lt;/p&gt;
&lt;p&gt;53 Chapter 2: Matrix Multiply-Add Module
Option Selection
Main Index
Parallel Method – Selection
Option Selection
The following table shows the type combination options that are supported (R stands for real, C for
complex).  These options are automatically selected based on matrix trailer information.  When the user
selects one particular method with an option not supported with that method, an alternate method is chosen
by MPYAD unless all of them are deselected.
Diagnostics
The MPYAD module outputs diagnostic information in two categories:  performance diagnostics and error
diagnostics.
Performance Diagnostics
DIAG 19 Output .  The following performance diagnostics is received by setting DIAG 19.SYSTEM(107) &amp;gt; 0 and
SYSTEM(66) = 1048592(T) or 1048588(NT)
Method R • R + R C • C + C R • C + R R • C + C
1T YES YES YES YES
1NT YES YES YES YES
2T YES YES YES YES
2NT YES YES NO NO
3T YES YES NO NO
3NT YES YES NO NO
4T YES YES NO YES
4NT YES YES NO NO
S YES YES NO NO
P YES YES NO NO&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module54
Main Index
Figure 2-8  Excerpt from the DIAG19 Output
In the above figure, “passes” means the number of partitions needed to create the result matrix.
T o prevent creating huge .f04 files when many MPYAD operations are executed, MSC Nastran has a
machine -dependent time limit stored in SYSTEM(20).  When a time estimate is below this value, it is not
printed.  To print all time estimates, the user should set SYSTEM(20) = 0.
Most of the diagnostics information mentioned in the above table is self explanatory.  Notice the presence of
the MPYAD keyword (SYSTEM(66)) used to verify the method selection/deselection operation.
Whenever a method is deselected, its time estimate is set to 999999.
Submethod Diagnostics
For special diagnostics on the submethods, the user must add 2097152 to the value of SYSTEM(66) (i.e.,
turn on bit 21).  The format of this diagnostic is shown in Table 2-3. The last four columns contain the
appropriate times.
where:M MATRIX A T railer(COLS ROWS FORM TYPE NZ
DENS)METHOD 1 Passes = XX CPU = XX I/O
= XX Total = XX
P MATRIX B T railer(COLS ROWS FORM TYPE NZ
DENS)METHOD 2 Passes = XX CPU = XX I/O
= XX Total = XX
Y MATRIX C T railer(COLS ROWS FORM TYPE NZ
DENS)METHOD 3 Passes = XX CPU = XX I/O
= XX Total = XX
A Working Memory = XX SYSTEM (66) = XX METHOD 4 Passes = XX CPU = XX I/O
= XX Total = XX
D T ranspose Flag = XX SYSTEM (126) = XX METHOD S Passes = XX CPU = XX I/O
= XX Total = XX
Table 2-3  Method One Submethods
NEW1 = B DESELECT NCPP PASSES KERNEL CPU I/O TOTAL
A YES x x x x x x
B NO x x x x x x
C NO x x x x x x
D YES x x x x x x
E YES x x x x x x
F YES x x x x x x
1 YES x x x x x x&lt;/p&gt;
&lt;p&gt;55 Chapter 2: Matrix Multiply-Add Module
MPYAD Estimates and Requirements
Main Index
Error Diagnostics
Error messages are abbreviated as follows:
The following error -related messages may be received from MPYAD:
UFM 3055 :
AN ATTEMPT TO MULTIPLY NONCONFORMABLE MATRICES.
The message is given if the number of columns of A is not equal to the number of rows in B, the number of
rows of C is not equal to the number of rows of A, or the number of columns of C is not equal to the number
of columns of B.  This message is also given when MPYAD is called from another module.
SFM 5423 :
ATTEMPT TO MULTIPLY INCOMPATIBLE  MATRICES.
The cause for this message is the same as for UFM 3055.  However, this message is more elaborate and prints
the trailers for all matrices involved.  This message comes from the MPYAD module.
UFM 6199 :
INSUFFICIENT CORE AVAILABLE FOR MATRIX MULTIPLY.
This message results while using the sparse multiply method when the storage estimate based on the trailer
information is exceeded during the actual execution of the operation.
MPYAD Estimates and Requirements
The CPU time estimate  for the sparse multiply -add method is based on the following input matrix
characteristics:
(2-18)
Computation time (sec):
(2-19)
Data move time (sec):NCPP =number of columns per pass
NEW1 =B indicates that submethod B is chosen
UFM User Fatal Message
SFM System Fatal Message
UWM User Warning Messages
SWM System Warning Messages
UIM User Information Messages
SIM System Information Messages
A : mn  B  : np  C : mpA  
mnpAM &lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Matrix Multiply-Add Module56
Main Index
(2-20)
where:
The minimum storage requirements are as follows:=
=workspace available in words
=one of either , , or  depending on the particular methods used
=machine precision (1 for short-word machines, 2 for long-word machines)
=density of  matrixnppPnpassmnAP&lt;em&gt; 2mpCDP&lt;/em&gt;   + + 
npass mnA
W IPREC1+  &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-
W
P&lt;em&gt;PsP Pi
IPREC
&lt;/em&gt; *
Note:    are defined in the Glossary . MP and P*
Disk:
Memory:mnA npB 2mpD ++
2nm+IPREC &lt;/p&gt;
&lt;p&gt;Main Index
Chapter 3: Matrix Decomposition
3 Matrix Decomposition
Decomposition Process
Theory of Decomposition
User Interface
Method Selection
Option Selection
Diagnostics
Decomposition Estimates and Requirements
References&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Decomposition Process58
Main Index
Decomposition Process
The decomposition operation is the first step in solving large linear systems of equations.
For symmetric matrices:&lt;/p&gt;
&lt;p&gt;where:
or
where:
For unsymmetric matrices:&lt;/p&gt;
&lt;p&gt;where:=system matrix
=lower triangular factor
=diagonal matrix
=system matrix
=Cholesky factor
=system matrix
=lower triangular factor
=an upper triangular factorA LDLT=
A
L
D
A CCT=
A
C
A LU =
A
L
U&lt;/p&gt;
&lt;p&gt;59 Chapter 3: Matrix Decomposition
Theory of Decomposition
Main Index
Theory of Decomposition
Symmetric Decomposition Method
The symmetric decomposition algorithm of MSC Nastran is a sparse algorithm.  This algorithm relies on fill-
in reducing sequencing and sparse numerical kernels.  The specific implementation also allows for the
indefiniteness of the input matrix.  This method is based on Duff, et al., 1982.
The factor has a specific storage scheme that can be interpreted only by the sparse FBS method.
Mathematical Algorithm
Permute and partition  as follows:
where the assumption is that the inverse of the  by  submatrix  exists.  lf  is indefinite, appropriate
pivoting is required to ensure the existence of the inverse.  This requirement is fulfilled by the presence of the
permutation matrices in the previous equation.  The order of E is either 1 or 2.  Then the elimination of
is shown as
Take , permute and partition again to obtain the following:
and continue the process until
The final factored form of
is then given by building the following:A
PAP ECT
CB=
s s E A
P
E
PAPIs0
CE1–In1–E 0
0 BCE1–CT–=IsE1–CT
0In1–
A2BCE1–CT– =
PA2 PE2C2T
C2B2=
orderBk1 or 2 =
PAP LDLT=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Decomposition60
Main Index
and
where  is built of 1 by 1 and 2 by 2 diagonal blocks.  The  identity submatrices are also of order 1 or 2
and the  submatrices are rectangular with one or two columns.  The rows of the  matrices extend to
the bottom of the L matrix.
The most important step is the proper selection of the  partition.  This issue is addressed later in this guide.
The module consists of two distinct phases:  the symbolic phase and the numeric phase.
Symbolic Phase
This phase first reads the input matrix  and creates the following information: one vector of length NZ
(where NZ is the number of nonzero terms of the upper half of the input matrix ) contains the column
indices, and another vector of the same length contains the row indices of the nonzero terms of the upper
triangular half of the matrix .  Both of these vectors contain integers.  Another responsibility of this phase
is to eliminate the zero rows and columns of the input matrix.
The selection of the  partition (i.e., the general elimination process) can be executed in different sequences.&lt;br&gt;
The performance of the elimination using the different sequences is obviously different.  T o find an effective
elimination sequence, a symbolic decomposition is also executed in the preface.  An important criterion is to
minimize the fill-in created in each step of the elimination process, thereby reducing the numerical work and
the l/O requirements.
The reordering methods may be prefaced by a compression process based on grid-dof connection or the so-
called supermodal amalgamation principle (both available with any reordering). Both contribute by making
the reordering more efficient in time and profile.LI10 0 .
C1E11–I20 .
C2 E21–I3.
C3E31–.
Ik=
DE100 .
0E20 .
00E3.
000BkCk Ek1–CkT–=
D Ik
Ci Ei1–
E
A
A
A
E&lt;/p&gt;
&lt;p&gt;61 Chapter 3: Matrix Decomposition
Theory of Decomposition
Main Index
Several different reordering algorithms are available in the symbolic phase:  multiple minimum degree
algorithm (MMD), Metis, EXTREME, MLV (multilevel vertex partitioning), etc.  These are described in the
references.
Each of the methods can be selected by setting system cell 206 to the desired option; see User Interface , 63 for
details.
In each of the above methods, the elimination of a chosen variable is performed based on severing all the edges
connected to it and connecting those nodes which were connected through the eliminated variable.  Severing
these edges leaves a reduced graph where the same process can continue until the reduced graph is a single
node only.  Then using this term as the root, create a so-called assembly tree of the matrix.  The final
elimination sequence is obtained by traversing the assembly tree.  Note that this sequence may be changed
due to numerical reasons.
Finally, the elimination sequence is stored into an array of length  (where  is the order of the&lt;br&gt;
matrix).  Note that at this phase, the actual terms of the matrix are not needed.
Numeric Phase
The mathematical decomposition process was described previously except for the details of the pivot selection
for numerical stability.  The strategy applied is a variant of the Bunch-Parlett method (1971) and is
implemented as follows.
Let us assume that the next potential pivot row is the j-th.  The diagonal entry of that row is tested against
all the other terms  as follows:
where t is based on an input parameter.  If the inequality is true, then the decomposition process uses s = 1
(1 by 1 pivot) and  as the pivot term (  matrix).  If the inequality is not true and the  term is the
largest in the pivot row, then the following pivotal matrix is tested for stability:
If
is satisfied, then  is the pivot.  Here  is the largest term in row  and  are the terms of .3N N A
k = j1n + 
ajjtajk
ajjE ajl
E2ajjajl
aljall=
maxieijalm
t&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;
j1=2

E2alml eijE21–&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Decomposition62
Main Index
If both of the above pivots fail, then a search is performed in the remaining possible pivot rows  for
pivots.  When one is found, that particular row is permuted into the j-th row position (by putting ones into
the proper locations of the P matrix), and the numerical elimination proceeds.
The numerical work is executed primarily by vector kernels.  The triangular kernels DFMR and DFMQ are
also used in addition to the conventional AXPY kernel.
Numerical Reliability of Symmetric Decomposition
The numerical reliability of the matrix decomposition process is monitored via the matrix/factor diagonal
ratio as follows:
where  is the original diagonal term of the matrix and  is the corresponding diagonal term of the factor.&lt;br&gt;
The maximum value of these ratios is used to indicate how well-conditioned the original matrix was.  The
higher this ratio, the closer the matrix is to singularity.  As shown by the algorithm of the decomposition,
small  values are the cause of numerical instability.  Hence in the case of unusually high matrix/factor
diagonal ratios, the user should practice extreme care in evaluating the results.
The sources of high ratios are the mechanisms in MSC Nastran.  A mechanism is a group of degrees of
freedom that may move independently from the rest of the structure as a rigid body.
For example, when unconstrained directions allow the entire model to move (a mechanism) a high ratio
occurs at the last grid point in the internal sequence.  Another possible cause of high ratios is connecting
flexible elements to stiff elements.  Finally, missing elements can also cause high ratios.
In general, ratios below  are usually acceptable; however, the safety limit is approximately .
Unsymmetric Decomposition
The sparse unsymmetric decomposition algorithm is another variation of the Gaussian elimination as follows:
For  (loop on all columns)
For  (loop on all rows)
If , then (elements of lower triangular)
If , thenkj
aii
di&amp;mdash;&amp;mdash;
aiidi
di
107103
j1= &amp;hellip;, n,
i1= &amp;hellip;, n,
ij
lijaijlik ukj
k1=mini1–j1– 
– =
ij&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P4</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_004/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_004/</guid>
      <description>
        
        
        &lt;p&gt;63 Chapter 3: Matrix Decomposition
User Interface
Main Index
For numerical reliability, the above elimination order is modified when necessary.  The pivoting step is based
on the following convention:
The meaning of  is that the  term is accepted as a possible pivot term if it is larger than the maximum
taken in the k-th column multiplied by a ratio of t (which is based on a user-specified threshold parameter;
see ).
From the computer science aspect, the sparse unsymmetric decomposition is similar to the symmetric
decomposition, using indexed vector operations and frontal logic which are not discussed here in detail.  The
sparse unsymmetric decomposition also has a distinct symbolic and numeric phase similar to symmetric
sparse decomposition.
Partial Decomposition
The sparse symmetric decomposition method may be used to decompose only a certain partition specified
by a partitioning vector (PARTVEC input data block for DCMP). In this case the following decomposition
is obtained:
where
The results of  are stored in the LD output data block and the  is in the LSCM data
block, see User Interface , 63.
User Interfaceuijaijlik ukj
k1=mini1–j1– 
–
lii&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
akkmax
iaikt 
akk
AAooAoa
AaoAaaLoo
LaoIDoo
AaaLooTLaoT
I= =
Aaa AaaLao Doo LaoT– =
LooDooLao Aaa
DCMP Matrix decomposition with extended diagnostics&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
User Interface64
Main Index
Decompose a square matrix  into upper and lower triangular factors  and  and diagonal matrix
. DCMP also provides extended diagnostics.
Format:
Input Data Blocks:
Output Data Blocks:
Parameters:DCMP USET,SIL ,EQEXIN,A,PARTVEC,EQMAP/
LD,U,LSC M/
S,N,KSYM /CHOLSKY/BAILOUT/MAXRATIO/SETNAME/F1/D
ECOMP/
DEBUG/TH RESH/S,N,MINDIAG/S,N,DET/S,N,POWER/S,N
,SING/
S,N,NBRC HG/S,N,ERR $
USET Degree-of-freedom set membership table.
SIL Scalar index list.
EQEXIN Equivalence between external and internal numbers.
A A square matrix (real or complex, symmetric or unsymmetric).
PARTVEC Partitioning vector specified when A is a partition of SETNAME. Its rowsize is indicated
by SETNAME. A is the zero-th partition from PARTVEC. In the partial decomposition
case it defines .
EQMAP Table of degree-of-freedom global-to-local maps for domain decomposition.
LD Nonstandard lower triangular factor  and diagonal matrix  or Cholesky Factor.
also contains [ ] for partial decomposition.
U Nonstandard upper triangular factor of  or Cholesky Factor.
LSCM Schur complement matrix based on internal partitioning of A.
KSYM Input/output-integer-default=1. See Method Selection , 65
CHOLSKY Input-integer-default=0. See Method Selection , 65A U L
D
A LU   for unsymmetric  A =
A LDLT   for symmetric  A =
Aoo
L D
LD Lao
A&lt;/p&gt;
&lt;p&gt;65 Chapter 3: Matrix Decomposition
Method Selection
Main Index
Method Selection
To select decomposition methods, the following parameters are used:BAILOUT Input-integer-default=0. If BAILOUT &amp;gt;0, then the module exits with error message if
factor to diagonal ratio exceeds MAXRATIO. If BAILOUT &amp;lt;-1, then the module
continues with warning message if factor to diagonal ratio exceeds MAXRATIO.
MAXRATIO Input-real-default=1.E5. See the BAILOUT and ERR parameter.
SETNAME Input-character-default=‘H’. One or two letters indicating the set membership of .
F1 Input-real-default = 0.0. Tolerance for suppressing numbers of small magnitude. Matrix
elements with magnitudes less than F1 will be set to zero.
DECOMP Input-integer-default=-1. See Option Selection , 66.
DEBUG Input-integer-default=-1. See Option Selection , 66.
THRESH Input-integer-default=-6. See Option Selection , 66.
MINDIAG Output-real double precision-default=0.0D0.
DET Output-complex-default=(0.0,0.0).
POWER Output-integer-default=0.
SIGN Output-integer-default=0. See Option Selection , 66.
NBRCHG Output-integer-default=0. See READ module.
ERR Output-integer-default=-1. If BAILOUT=-1, this parameter always remains at zero. If
BAILOUT=0 and the factor to diagonal ratio is negative or greater than MAXRATIO,
ERR is reset to -1.A
KSYM 1 Use symmetric decomposition (default).
0 Use unsymmetric decomposition.
-1 Use decomposition consistent with form of . KSYM will be reset to 0 or 1
consistent with actual decomposition type.
3 Use symmetric partial decomposition.
CHOLSKY If KSYM=1 or KSYM=-1 and  is symmetric then:
1 Use Cholesky decomposition.
0 Use standard decomposition (default).
If KSYM=3, then CHOLSKY is set to the number of degrees of freedom in the o-set.A
A&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection66
Main Index
Option Selection
Minimum Front Option
T o increase performance of the sparse symmetric decomposition, the user may set SYSTEM(198), the
MINFRONT keyword to a value greater than 1.  The appropriate value is problem and machine dependent.&lt;br&gt;
Its meaning is to restrict the sparse strategy to above a certain minimum front size (characteristically 8 - 16).
Reordering Options
The various reordering options are selected via SYSTEM(206) as follows:
Compression Options
The supernodal compression scheme currently is available only with EXTREME and Metis.
The grid-based compression scheme is automatically executed when the datablocks defining the grid-DOF
connections (USET,SIL) are available to the module.
Perturbation Option
If DEBUG (= SYSTEM(60)) is set then an  is put on zero diagonal terms. If not set,
is used.SYSTEM(206)
DCMPSEQ Method
0 EXTREME for 3D, Metis or MMD for 2D
1 MMD - definite matrices
2 MMD - indefinite matrices
3 No sequencing
4 Default, EXTREME
8 Metis
9 Better of Metis and MMD
32 MLV
64 T urn on Supernodal Compression Scheme
 10DEBUG –=
 1010–=&lt;/p&gt;
&lt;p&gt;67 Chapter 3: Matrix Decomposition
Option Selection
Main Index
High Rank Options
An additional performance improvement is possible, with the high rank update in the sparse decomposition
methods.  The rank of update in the various sparse decomposition methods is set as:
The defaults of these cells are set automatically, since they are machine dependent.
Diagnostic Options
These options of the sparse symmetric decomposition are requested as follows:
The THRESH  parameter is used to control the pivoting for the unsymmetric and the sparse (which also
pivots) decompositions.  The pivot threshold is
In the unsymmetric case, pivoting  occurs if a factor diagonal value is less than .  In the symmetric case of
the sparse decomposition, pivoting occurs when the ratio for the factor diagonal value to the largest term in
the pivot row is less than .
The default value of THRESH is 6 for the sparse decomposition and 10 for the unsymmetric
decomposition. The latter may also be defined by SYSTEM(91) for the unsymmetric case.
In the case of a READ module the THRESH parameter may be set by SYSTEM(89).
The shared memory parallel execution of the sparse symmetric decomposition can be selected by
SYSTEM(126) = 8 and SYSTEM(107) &amp;gt; 1
and deselected by turning off either one of these system cells.SYSTEM Sparse Decomposition
(205) Symmetric, real
(219) Symmetric, complex
(220) Unsymmetric, real
(221) Unsymmetric, complex
SYSTEM Action
(69) = 64 Stop after diagnostic phase
(166) = 2 Provides internal diagnostics
4 Overwrites MAXRATIO by 1.0
8 Provides MAXRATIO vector in U
t10THRESH=
t
t&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Diagnostics68
Main Index
Diagnostics
The diagnostics given by the decomposition module are organized into numerical, performance, statistical,
and error diagnostics.
Numerical Diagnostics
These diagnostics are related to the accuracy of the solution.
Singularity
Causes of Singularity .  A matrix is singular if its inverse cannot be calculated.  The SING parameter is
set to -1 if the matrix is singular.  Singularity  is a relative issue with respect to stiffness ratios.  However, some
independent general reasons for singularity include:
Degree of freedom without stiffness
2-D problem, normal rotation unconstrained
3-D problem, rotational DOFs at solids unconstrained
Planar joint in space structure
Singularity Test
T o avoid singularity, the user can enter the AUTOSPC keyword.  If AUTOSPC is set to YES, the singular
degrees of freedom are automatically constrained out.  Any degree of freedom is considered to be singular if
where  is the term in the i-th row and the j-th column of  matrix, and  is the largest term in
.
The default for  is  and can be changed by the keyword EPZERO.  The SPC entries constraining the
singular DOFs are generated by setting the SPCGEN keyword to YES.
Parameter EPPRT (Bulk Data) (default = ) is used to set a threshold below which all potential
singularities are listed.  If EPPRT is greater than EPZERO, then the printing of singularities with a ratio of
exactly zero is suppressed.
Ill-Conditioning
Causes of Ill-Conditioning .  The ill-conditioning of a matrix  can be caused by any of the following
reasons:
Low stiffness in rotation
Large massAij
Amax&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;
AijA Amax
Aelem
108–
108–&lt;/p&gt;
&lt;p&gt;69 Chapter 3: Matrix Decomposition
Diagnostics
Main Index
Very stiff beam
Mechanisms
MAXRATIO Parameter .  Ill-conditioning is diagnosed by the MAXRATIO diagnostics parameter which
is defined as
where  is the i-th diagonal term in the  matrix.
The maximum MAXRATIO of the decomposition is printed under User Information Message 4158 or 4698
when its value is greater than .  This limit can be changed by setting the keyword MAXRATIO.
In the case of  pivoting, this ratio is not calculated.
Negative  Terms on Factor Diagonal
STURM Number .  The NBRCHG parameter (DMAP call) gives the number of negative terms on the
factor diagonal (it is also called the STURM  number).  This diagnostics information message is important
when decomposition is used in an eigenvalue module.  In this case the number of negative terms provides the
number of negative eigenvalues of the matrix.  Since the matrix decomposed in this case is usually a shifted
matrix, the NBRCHG gives the number of eigenvalues to the left of the shift.  User Information Messages
4158 and 5010 in the eigenvalue modules print the value of NBRCHG.
Performance Diagnostics
For symmetric decomposition, the following message (UIM 4157) appears:
The integer words in factor are the row and column index information, and the real words are the actual terms
of the factor matrix . In the sparse methods, the integer and real words are stored in two separate records.
The 4157 message is followed by UIM 6439 for the sparse symmetric decomposition as follows:MATRIX SIZE NUMBER OF NONZEROES
NUMBER OF ZERO COLUMNS NUMBER OF ZERO DIAGONALS
CPU TIME ESTIMATE I/O TIME ESTIMATE
EST . MEMORY REQUIRED MEMORY AVAILABLE
EST . INTEGER WORDS IN FACTOR EST . NONZERO TERMS IN FACTOR
EST . MAX FRONT SIZE RANK OF UPDATE
UIM 6439 (DFMSA) ACTUAL MEMORY AND DISK SPACE REQUIREMENTS FOR
SPARSE SYMMETRIC DECOMPOSITIONMAXRATIOAii
Dii&amp;mdash;&amp;mdash;- =
DiiD
107
22
L&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Diagnostics70
Main Index
For unsymmetric sparse decomposition, UIM 4216 provides the following information.
This message is also followed by a UIM 6439 which gives actual values for the estimates in UIM 4216.
Statistical Diagnostics
The following messages are self-explanatory.
UIM 4158:
STATISTICS FOR SYMMETRIC (PARALLEL AND/OR SPARSE) DECOMPOSITION OF DATA
BLOCK XX
NUMBER OF NEGATIVE TERMS ON FACTOR DIAGONAL.
MAXIMUM RATIO OF MATRIX DIAGONAL TO FACTOR DIAGONAL.
UIM 4367:
STATISTICS FOR UNSYMMETRIC DECOMPOSITION OF DATA BLOCK XX FOLLOW.
NUMBER OF PIVOT OPERATIONS = XX.
UWM 5221 :
STATISTICS FOR DECOMPOSITION OF MATRIX XX.
THE FOLLOWING DEGREES OF FREEDOM HAVE NULL COLUMNS.
UWM 4698 :
STATISTICS FOR DECOMPOSITION OF MATRIX XX.
THE FOLLOWING DEGREES OF FREEDOM HAVE FACTOR DIAGONAL RATIOS GREATER
THAN MAXRATIO OR HAVE NEGATIVE TERMS ON THE FACTOR DIAGONAL.
Error Diagnostics
The following are messages from sparse decomposition and are described as follows:
SFM 4370 :
DECOMPOSITION REQUIRES THAT PRECISION OF DATA BLOCK XX EQUAL SYSTEM
PRECISION.
This is a general limitation of the module.SPARSE DECOMP MEMORY REQUIRED MAXIMUM FRONT SIZE
INTEGER WORDS IN FACTOR NONZERO TERMS IN FACTOR
MATRIX SIZE NUMBER OF NONZEROES
NUMBER OF ZERO COLUMNS NUMBER OF ZERO DIAGONAL TERMS
CPU TIME ESTIMATE I/O TIME ESTIMATE
ESTIMATED MEMORY REQUIREMENT MEMORY AVAILABLE
EST. INTEGER WORDS IN FACTOR EST . NONZERO TERMS
ESTIMATED MAXIMUM FRONT SIZE RANK OF UPDATE&lt;/p&gt;
&lt;p&gt;71 Chapter 3: Matrix Decomposition
Decomposition Estimates and Requirements
Main Index
UFM 3057 :
MATRIX XX IS NOT POSITIVE DEFINITE.
A Cholesky option was requested by the user on a non-positive definite matrix.
SFM 4218 :
UNSYMMETRIC DECOMPOSITION IS ABORTED DUE TO INSUFFICIENT MEMORY.
The preface of the unsymmetric decomposition module needs more memory to execute.
SFM 4255 :
UNSYMMETRIC DECOMPOSITION OF DATA BLOCK XX FAILS AT ROW XX.  UNABLE TO
PIVOT .
A singular matrix was given to the module.
UW(F)M 6136 (DFMSA) :
INSUFFICIENT CORE FOR SYMBOLIC (NUMERIC) PHASE OF SPARSE DECOMPOSITION.
USER ACTION:  INCREASE CORE BY XX WORDS.
UFM 6133 (DFMSD) :
SINGULAR MATRIX IN SPARSE DECOMPOSITION.
USER ACTION:  CHECK MODEL
UFM 6137 (DFMSD) :
INPUT MATRIX IS RANK DEFICIENT , RANK = XX.
USER ACTION:  CHECK MODEL
Decomposition Estimates and Requirements
The CPU time estimate  for the sparse symmetric decomposition method includes:
Computation time (sec):
Data move time (sec):
where:
Storage Requirements .  The minimum storage requirements are as follows:=average number of connected DOFs
=approximate number of nonzeroes in the upper triangle of the original matrix
=density of the original matrix1
2&amp;mdash;NNfront2M 
1
2&amp;mdash;NNfrontPs2 Ps Nz + 
Nfront
NzN22 N+
&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
References72
Main Index
where:
The CPU time estimate for the sparse unsymmetric decomposition method is
Computation time (sec):
Data move time (sec):
References
Bunch, J. R.; Parlett, B. N. Direct Methods for Solving Symmetric Indefinite Systems of Linear Equations.&lt;br&gt;
Society for Industrial and Applied Mathematics Journal of Numerical Analysis, Volume 8, 1971.
Duff, I. S.; Reid, J. K. The Multifrontal Solution of Indefinite Sparse Symmetric Linear Systems.  Harwell
Report CSS122, England, 1982.
George, A.; Liu, J. W. Computer Solutions of Large Sparse Positive Definite Systems. Prentice Hall, 1981.
Goehlich, D.; Komzsik, L. Decomposition of Finite Element Matrices on Parallel Computers . Proc. of the
ASME Int. Computers in Engineering Conf., 1987.
Golub, G. H.; Van Loan, C. F . Matrix Computations . John Hopkins University Press, 1983.
Hendrickson, B., Rothberg, E. I mproving the Runtime and Quality of Nested Dissection Ordering . Silicon
Graphics, Inc., Mountain View, CA, April 11, 1996.
Karypis, G., Kumar, V. METIS©, A Software Package for Partitioning Unstructured Graphs, Partitioning
Meshes, and Computing Fill-Reducing Orderings of Sparse Matrices, Version 3.0.3 , University of Minnesota,
Department of Computer Sciences/Army HPC Research Center, Minneapolis, MN, November 5, 1997.
(&lt;a href=&#34;http://www.cs.umn.edu/~karypis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://www.cs.umn.edu/~karypis&lt;/a&gt;)
Pissanetzsky, S. Sparse Matrix Technology.  Academic Press, 1984.Disk:
Memory:
NWPT =number of words per term
1 for 64 bit word real arithmetics
2 for 64 bit word complex arithmetics
2 for 32 bit word real arithmetics
4 for 32 bit word complex arithmetics1NWPT + NfrontNN+
62NWPT+ N
NNfront2 M
NNfront Ps4 PsNz+ &lt;/p&gt;
&lt;p&gt;73 Chapter 3: Matrix Decomposition
References
Main Index
Shamsian, S.; Komzsik, L. Sparse Matrix Methods in MSC/NASTRAN . The MSC 1990 World Users Conf.
Proc., Vol. II, Paper No. 64, March, 1990.
Rothberg, E. Ordering Sparse Matrices Using Approximate Minimum Local Fil l, Silicon Graphics, Inc.,
Mountain View, CA, April 11, 1996.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
References74
Main Index&lt;/p&gt;
&lt;p&gt;Main Index
Chapter 4: Direct Solution of Linear Systems
4 Direct Solution of Linear Systems
Solution Process
Theory of Forward-Backward Substitution
User Interface
Method Selection
Option Selection
Diagnostics
FBS Estimates and Requirements&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Solution Process76
Main Index
Solution Process
Solution of linear systems is an important and time-consuming component of MSC Nastran runs.&lt;br&gt;
Mathematically, the solution process represents a right-handed
or a left-handed
direct solution.&lt;br&gt;
The iterative solution directly solves  and is described in Iterative Solution of Systems of Linear Systems, 83 .
The direct solution method contains two distinct parts:  a forward and a backward substitution, hence its
name forward-backward substitution.
The forward-backward substitution is a follow-up operation to the decomposition process (see Matrix
Decomposition , 57).
The right-handed direct solution step is executed using the triangular factors calculated in the decomposition
as a forward step of
The backward  step of the operation uses the intermediate result  as follows:
In the case of unsymmetric decomposition, the backward step is
The left-handed forward step to solve  is
and
In  and , the decomposition is assumed to be a Cholesky method. In the  case,  is modified as
The theory covering the solution of systems of linear equations is described in the following section.AX B =
XTA BT=
LY B =
Y
LTX D1–Y =
UX Y =
WTLTBT=
ZTLWT      =
LDLT
ZTL D1–WT=&lt;/p&gt;
&lt;p&gt;77 Chapter 4: Direct Solution of Linear Systems
Theory of Forward-Backward Substitution
Main Index
Theory of Forward-Backward Substitution
Right-Handed Method
The elements of matrices  and  of  and  are given by
and
MSC Nastran uses a sparse implementation of the above algorithm, as well as special modifications for the
parallel, and left-handed cases.
Left-Handed Method
The elements of  and  are:
and
In the above equations, the  matrix is assumed to have been decomposed with the Cholesky method.
Sparse Method
The sparse option of FBS executes the forward-backward substitution from the factor of the sparse
(multifrontal) decomposition.  Thereby, one must consider the permutations executed for pivoting given by
for the symmetric case, or  and  for the unsymmetric case, and then solve the following matrix
equations.  For the symmetric case,Y X
yijbijlik ykj
k1=i1–
– =
xijyij
di&amp;mdash;&amp;ndash; lki xkj
ki1+=n
– =
wikbikwjk lij
j1=i1–
–
lii&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; =
zikwik
ji1+=n
 wjk lij–
lii&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
A
P P Q
LDLTPXPB=
Main Index&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
User Interface78
Main Index
For the unsymmetric case,
The usual forward pass solves the following equation for the symmetric case
For the unsymmetric case,
for .  The backward pass gives  from the following equation for the symmetric case
For the unsymmetric case,
The storage requirements are real areas for the right-hand side , the result , and an integer vector of
length  holding the permutation information plus an area for the factor.
Parallel Method
A shared memory, parallel execution of the sparse method is also available.  The parallelization is either on
the factor side of the equations, based on special shared memory parallel kernels, or on the right-hand side
when multiple loads exists.
User Interface
T o solve the matrix equation  (right-handed solution) or  (left-
handed solution) using the triangular factors computed by DCMP .
Input Data Blocks:
Output Data Block:PLUQX PB=
LY PB=
PLY PB=
Y X
DLTPXY=
UQX Y=
B X
N
FBS LD,U,B/X /KSYM/SIGN/FBTYP $
LD Lower triangular factor/diagonal, or Cholesky factor.
U Upper triangular factor. Purged unless  is unsymmetric.
B Rectangular matrix.
X Rectangular matrix having the same dimensions as .AX B= XTA BT=
A
B&lt;/p&gt;
&lt;p&gt;79 Chapter 4: Direct Solution of Linear Systems
Method Selection
Main Index
Parameters:
Method Selection
FBS Method Selection
The FBS method selection is executed via the following parameters:
Option Selection
Right-handed FBS Options
The user can control the flow of the forward-backward substitution via the FBTYP parameter (DMAP call),
which is also equivalent to SYSTEM(74).  The default value is 0, which indicates a full execution of both
passes of the solution.  Set FBTYP to +1 for a forward pass only; set FBTYP to -1 for a backwards only partial
execution.  These options are useful in eigenvalue modules.
SYSTEM(73) = +1 indicates the presence of Cholesky factor .  The default is 0, which indicates
the regular  factor. To summarize this:KSYM Input-integer-default = -1. See Method Selection , 79.
SIGN Input-integer-default = 1. See Method Selection , 79.
FBTYP Input-integer-default = 0. See Option Selection , 79
KSYM Symmetry flag.
-1 choose symmetric if  is purged, otherwise unsymmetric (default).
0 matrix  is unsymmetric.
1 matrix  is symmetric.
2 perform left-handed solution. See Option Selection , 79.
SIGN Sign of .
1 solve  (default).
-1 solve .U
A
A
B
AX B =
AX B–=
LLT
LDLT&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Diagnostics80
Main Index
Left-handed FBS Option
T o indicate a left-handed solution, SYSTEM(70) = 2 is needed. Note that SYSTEM(70) is equivalent to
FBSOPT . The left-handed FBS obtains the solution by rows, as opposed to columns. Since the solution is
packed out via the usual GINO facilities, the solution rows are stored as columns of the solution (X) matrix.
The user may input a transposed or untransposed right-hand side (B) matrix. T o summarize this:
The FBS options controlled by cells 73 and 74 apply to the left-handed solution appropriately.
Parallel FBS Solution
The user needs FBSOPT = -2 and PARALLEL &amp;gt; 1 for the right-hand side shared memory parallel execution.
For the factor side parallel only PARALLEL &amp;gt; 1 is needed.
The parallel method can be deselected by setting PARALLEL = 1024 + ncpu where ncpu = number of CPUs.&lt;br&gt;
This setting leaves the other parallel methods enabled.
Diagnostics
For the direct solution of systems of linear equations, diagnostics can also be classified as follows:  numerical
diagnostics, performance messages, and error diagnostics.
Numerical Diagnostics
UIM 5293:SYSTEM Value Action
(73) +1Cholesky factor:
elseRegular factor:  (default)
(74) +1 Solve
1Solve
0 Full FBS (default)
SYSTEM Value Action
(72) =1LLTX B =
LDLTX B =
LY B or LDY B = =
LTX Y =
BTXTA =
1 B XTA =
FOR DATA BLOCK . . . . LOADSEQ EPSILON EXTERNAL WORK
. . . . . . . . . . . .
. . . . . . . . . . . .&lt;/p&gt;
&lt;p&gt;81 Chapter 4: Direct Solution of Linear Systems
Diagnostics
Main Index
Performance Messages
UIM 4234:
UFBS TIME ESTIMATE  TO FORM XX TYPE = X CPU = X I/0 = X TOTAL = X PASSES = X
UIM 4153:
FBS METHOD X TIME ESTIMATE TO FORM XX CPU = X I/0 = X TOTAL = X PASSES = X
These messages are printed only when the CPU time estimate is greater than the value of SYSTEM(20)
(default = machine-dependent).  To force the printing of these messages, the user must set SYSTEM(20) = 0.
Error Diagnostics
FBS 1(2, 3 OR 4) FATAL ERROR 20 :     USER FATAL MESSAGE
This error should not occur under normal circumstances.  The cause for the error is that information from
the GINO control block is incompatible with information from the buffer.  This occurs in methods 1 or 1A
only.
SFM FBSUB LOGIC ERROR 10 :     USER FATAL MESSAGE
This error is similar to the previous one from method 2.
SFM 6069 (LFBSS) :
SYMMETRIC LEFT HANDED FBS IS CALLED TO SOLVE A COMPLEX SYSTEM WITH
CHOLESKY FACTOR.
This option is not supported.
SFM 6070 (LFBSS) :
ERROR IN READING THE FACTOR IN SYMMETRIC LEFT HANDED FBS
These messages are from the left-handed method.  They are either from symmetric, unsymmetric, or from
the timing phase of either one.  The cause for this error is similar to the cause for Error 20.
SFM 6072 (LFBSU) :
INCORRECT PIVOTING INSTRUCTIONS IN UNSYMMETRIC FACTOR DURING A LEFT
HANDED FBS.
This message is similar to Error 20 since it indicates inconsistency in a data block.
SFM 6073 (LFBSU) :
ERROR IN READING THE FACTOR IN UNSYMMETRIC LEFT HANDED FBS
Similar causes as Error 20.
SFM 6201 (FBSQCK) :
SPARSE FBS CANNOT BE EXECUTED WHEN THE FACTOR IS REAL AND THE RIGHT HAND
SIDE IS COMPLEX.
Recommendation:  Do not select the sparse decomposition and FBS methods under these circumstances.
UFM 6138 (DFMSB) :
INSUFFICIENT CORE FOR SPARSE FBS.
USER ACTION:  INCREASE CORE BY XX WORDS.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
FBS Estimates and Requirements82
Main Index
FBS Estimates and Requirements
Sparse FBS Estimates
The CPU time for sparse FBS is:
Data move time (sec):
where:
The minimum storage requirements of FBS are:
where:
References
Komzsik, L. Parallel Processing in Finite Element Analysis . Finite Element News, England, June, 1986.
Komzsik, L. Parallel Static Solution in Finite Element Analysis . The MSC 1987 World Users Conf. Proc.,
Vol. I, Paper No. 17, March, 1987.=number of right-hand sides
=number of nonzeroes in the factor
=number of passes
Disk:
Memory:
=average front size
=machine precision (1 for short-word machines, 2 for long-word machines)2 NRHSNZFACM 
2 NNRHSP2 NZFACPsNpass + 
NRHS
NZFAC
Npass
NZFAC2 NNRHSIPREC  +
12Nfront2IPREC2 NIPREC + 
Nfront
IPREC&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P5</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_005/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_005/</guid>
      <description>
        
        
        &lt;p&gt;Main Index
Chapter 5: Iterative Solution of Systems of Linear Systems
5Iterative Solution of Systems of
Linear Systems
Iterative Solver Solutions
Theory of the Conjugate Gradient Method
Preconditioning Methods
User Interface
Iterative Method Selection
Option Selection
Iterative Solution Diagnostics
Iterative Solver Estimates and Requirements
References&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Iterative Solver Solutions84
Main Index
Iterative Solver Solutions
The previous chapter discussed the solution of linear systems using the direct method, which consists of
matrix decomposition followed by a forward-backward substitution.  In this chapter an alternative method,
the iterative solver, is described.
Solution Sequences .  The iterative solver can be used in the following solution sequences:
Linear statics (SOLs 1, 101)
Nonlinear statics (SOL 106)
Direct frequency response (SOLs 8, 108)
Modal frequency response (SOLs 11, 111)
Parallel Execution .  In addition to the above sequential versions, the iterative solver is implemented for
parallel execution on distributed memory machines in SOL 1 and SOL 101 (linear statics).
Methods
For symmetric positive definite systems, several different versions of the preconditioned conjugate gradient
method are available in MSC Nastran.  For unsymmetric or indefinite systems, the preconditioned bi-
conjugate gradient or the preconditioned conjugate residual method is used.
Theory of the Conjugate Gradient Method
The conjugate gradient method minimizes the error function
The first derivative (gradient) of this function is
which is the negative of the residual.  An iterative procedure is obtained by calculating consecutive versions
of the approximate solution as follows:
and
where the direction   and the distance  are computed to minimize the above error function.
Computationally efficient forms to calculate these quantities areFx1
2&amp;mdash; xTAxxTb– =
dF
dx&amp;mdash;&amp;mdash;- Axb r–=– =
xi1+xii pi+ =
ri1+rii Api– =
p &lt;/p&gt;
&lt;h1&gt;85 Chapter 5: Iterative Solution of Systems of Linear Systems
Theory of the Conjugate Gradient Method
Main Index
where
See Hageman and Young, 1981 [1] for more details.  The algorithmic formulation is fairly straightforward.
Convergence Control
Convergence is achieved inside the iteration loop when the following inequality holds:
where  denotes the residual vector after  iterations,  denotes the initial right-hand side vector, and&lt;br&gt;
is a user-defined parameter (default: 1.E-06).  If this ratio does not become smaller than  within the
maximum number of iterations,  is compared to an energy norm  after the iteration loop is finished.  The
solution is accepted if the following inequality holds:
where  is the solution vector,  is the final residual vector, and  is the initial right-hand side vector as
before.  Experience has shown that the convergence criterion inside the iteration loop  is far more conservative
than the convergence criterion outside the iteration loop .
Based on user selection, an alternate convergence criterion is available for Jacobi, Cholesky and RIC
preconditioning:
where:&lt;/h1&gt;&lt;p&gt;=iriTri
piTApi&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; =
pirii pi1–+ =
iriTri
ri1–T ri1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
rn
b&amp;mdash;&amp;mdash;&amp;mdash;
rnn b 


xTr
bTx&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;
x r b
xne1fn–
xnxn1+xn–
fnxn
xn1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of the Conjugate Gradient Method86
Main Index
See Conca, 1992 [2] for details.
Block Conjugate Gradient Method (BIC)
The BIC method is the most recent and most recommended method for an iterative solution of the linear
system in MSC Nastran.  It was introduced several years ago and has gone through major improvements with
regard to memory management and spill logic since then.
BIC is a block version of the conjugate gradient method described above where ‘block’ refers to two
characteristics of the method:
1.The efficient solution of the linear system with a block of right-hand sides where the following
inequality is expected to hold for  ( = the number of right-hand sides)
where  refers to the time needed to solve the linear system with just the i-th right-hand side and&lt;br&gt;
refers to the time needed to solve the linear system with the complete block of  right-hand sides.
The method will still work for ; however, the inequality may not hold.
2.Both the matrix and the preconditioner are stored in block-structured form to improve the
performance.
The block of direction vectors is updated in every iteration as
and the blocks of residual and solution vectors are updated as
where all matrices are  size except for , which is obtained by the concatenation of the two matrices
as
and it is of size .  Convergence is achieved when the following inequality holdsm10 m
titm
i1=m

titm
m
m10
pkTkTATk1–TkT Rk=
Rk1+RkATk Pk– =
Xk1+XkTk Pk– =
nm Tk
TkXkXk1–Rk–  =
n2m
maxmax
iXn1+Xn–
Xn1+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-  ,  max
iRn1+
F&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;
   &lt;/p&gt;
&lt;p&gt;87 Chapter 5: Iterative Solution of Systems of Linear Systems
Theory of the Conjugate Gradient Method
Main Index
where  is the column index, ,  is the rectangular matrix of the  residual vectors,  is the set of
right-hand sides, and  is the set of solution vectors.  See Babikov, 1995 [3] for more details.
The algorithmic formulation of this method employed in the iterative solution module is fairly
straightforward, except for the very difficult issue of calculating the inverse matrix in .  However, the
details of that calculation are beyond the scope of this guide.
Real and Complex BIC
The BIC methods for real and complex linear systems of equations are different and, therefore, are described
separately in the following paragraphs.  The parameters are described in User Interface , 92.
The real BIC method follows the basic algorithm shown below:i im R r F
X
Pk&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of the Conjugate Gradient Method88
Main Index
In the above algorithm,  is the preconditioner,  is the block of right-hand side vectors,  is the block of
updated solution vectors, and  is the block of updated residual vectors.
The memory management and the spill logic are rather complex and only the basic steps are listed below:
1.Execute a bandwidth-reducing permutation
2.Symbolic phase
Find the best memory usage given a certain amount of memory and an IPAD value (please see User
Interface , 92 for details on IPAD).  The following logical steps are traversed in this order:X0X1–0 = =
R0R1–F– = =
Loop on k 12 ITSMAX ,=
WkB1–Rk=
VkAWk=
TkXkXk1–Wk–  =
PkRkRk1–Vk–  =
GkPkTTk1–=
HkGkTkTRk=
XkXk1–Tk Hk– =
IF errorITSEP S  then converged
End loop on kRkRk1–Pk Hk– =
B F X
R&lt;/p&gt;
&lt;h1&gt;89 Chapter 5: Iterative Solution of Systems of Linear Systems
Theory of the Conjugate Gradient Method
Main Index
a.Check if matrix and factor fit in core; if yes, go to step “Set up memory accordingly. ” on page 89.
b.Check if factor fits in core with matrix being out-of-core; if yes, go to step “Set up memory
accordingly. ” on page 89.
c.Check if memory is sufficient when factor and matrix are both out-of-core; if yes, go to step “Set
up memory accordingly. ” on page 89.
d.Decrease padding level:  IPAD = IPAD - 1; go back to step “Check if matrix and factor fit in core;
if yes, go to step “Set up memory accordingly.” on page 89.” on page 89.
e.Set up memory accordingly.
3.The numeric phase of the preconditioner calculation makes use of the following techniques:
a.Calculation of preconditioner in double precision; storage of final preconditioner in single
precision.
b.Restarts with global shift regularization if incomplete factorization fails.
c.Post truncation for well-conditioned problems.
4.Using the techniques described above, more memory may be available than was predicted by the
symbolic phase.  Unless both matrix and preconditioner were in core, as much as possible is read from
the scratch files and saved in memory.
The memory management and spill logic for the complex BIC methods are different.
There are two different BIC methods for the complex case that are selected via the IPAD value.  For IPAD &amp;lt;
5, the complex BIC algorithm is similar to the real BIC method.  However, for IPAD 5, a very different
strategy is used, which is described below.
The solution of the system
is based on its equivalent representation
where matrices  and  (= , since  is symmetric) have fully zero imaginary parts and  is
truly complex.  The solution is found using the Schur complement.
The spill logic for the first complex method (IPAD &amp;lt; 5) is very simple:  if the matrix and preconditioner do
not both fit in core, then a fully out-of-core approach is used.
The spill logic for the second complex BIC method is more sophisticated.  It determines whether only ,
only , or  and  can be kept in core.  is always kept in core since it usually has very few
nonzero terms.AX F=
A11A12
A12A22     X1
X2F1
F2&lt;/h1&gt;&lt;p&gt;A11A12A21TA A22
A11
A22A11A22A12&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Preconditioning Methods90
Main Index
The second complex BIC method is recommended and is also the default for complex symmetric linear
systems if the iterative solver was chosen.
Preconditioning Methods
The use of a preconditioner is recommended to reduce the number of iterations.  The disadvantage of this
process is that the preconditioning calculation increases the amount of work in each iteration.  With the use
of a preconditioner matrix, the original problem is now written as
where  is the preconditioning matrix.  In this form, the preconditioner is applied in every iteration step.&lt;br&gt;
This is called stepwise preconditioning.  The Jacobi and Cholesky preconditioners are stepwise.  If&lt;br&gt;
is chosen, then the problem is trivial:
Of course, the cost of this preconditioning is equivalent to the direct solution of the system.
The following four major stepwise preconditioning strategies are supported in MSC Nastran.
Jacobi (J) .  For the Jacobi method, the  matrix is a diagonal matrix containing the diagonal terms of the
matrix.  The preconditioning step in every iteration is a simple division of the current residual vector by
these terms.
Cholesky (C) .  In the Cholesky method, the selection of the preconditioner matrix is
where  is an incomplete Cholesky factor of .  Despite wide acceptance, the use of standard Cholesky
preconditioning in the conjugate gradient method is only proven to be very good for finite difference
discretization of partial differential equations.  For example, the conjugate gradient method has convergence
problems when used in finite element problems with high Poisson ratios.
Reduced Incomplete Cholesky (RIC) .  In the RIC method the preconditioner matrix is calculated
based on elemental information as
where  is the elemental matrices used to assemble .
This method significantly increases the performance of the iterative solver for shell problems.  See Efrat, 1986
[4] for details.
Block Incomplete Cholesky (BIC) .  Another disadvantage of the Cholesky method is the calculation
cost for the incomplete or reduced factor and the memory requirement for parts of the Cholesky factor.  A P1–AxP1–b =
P
P A=
P1–AxIx= A1–b =
P
A
P CCTA =
C A
P CCTBfAAelem  = =
AelemA&lt;/p&gt;
&lt;p&gt;91 Chapter 5: Iterative Solution of Systems of Linear Systems
Preconditioning Methods
Main Index
specific block sparse implementation has reduced these disadvantages. This proprietary method uses a global
shift regularization strategy, a post-truncation technique for well-conditioned problems, and allow a certain
level of fill-in based on delicate numerical considerations.  The implementation of the block conjugate
gradient method accelerates convergence for problems involving multiple loads.  Moreover, a band reordering
method is used for the matrix in the symbolic phase of the iterative solver.  See Babikov, 1995 for details.
Scaling
Another approach for preconditioning is to use the preconditioner as a transformation.  Then
is transformed into
In this case the solution of the transformed system has to be converted back to the original solution as follows:
An example of this transformation approach is the diagonal scaling.  Diagonal scaling is a useful tool for
matrices whose terms differ significantly in magnitude.  In this method, the following transformation is
performed:
where  = an intermediate result, such as
The diagonal terms of the scaled matrix are unity as a result of the diagonal scaling.  This scaling makes the
Jacobi preconditioning step trivial.  The other (Cholesky type) preconditioning methods may be combined
with scaling.
Numerical Reliability of Equation Solutions
The accuracy of the solution of the linear equation systems in MSC Nastran is evaluated with the following
residual vector:
This residual vector is calculated for each solution vector in statics analysis.  Then a scalar value is calculated
as follows:
The magnitude of this error ratio indicates the numerical accuracy of the solution vectors.P1–APP1–x P1–b =
A x b=
x Px=
D1–AD1–x˜D1–b = 
x˜
D aii and xD1–x˜ = =
r bAx–=
xTr
xTb&amp;mdash;&amp;mdash;&amp;ndash; =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
User Interface92
Main Index
User Interface
Format for non-p-version analysis:
Format for p-version analysis:
Input Data Blocks:SOLVIT A,B,XS,P C,USET,KGG,GM,SIL,EQEXIN,EDT,CASECC,EQ
MAP/
X,R,PC1, EPSSE/
SIGN/ITS OPT/ITSEPS/ITSMAX/IPAD/IEXT/ADPTINDX/
NSKIP/MS GLVL/PREFONLY/S,N,ITSERR/SEID $
SOLVIT A,B,XS,P S,USET,USET0,SIL0,SIL,EQEXIN,EDT,CASEC
C,
EQMAP/
X,R,PG,E PSSE/
SIGN/ITS OPT/ITSEPS/ITSMAX/IPAD/IEXT/ADPTINDX/
NSKIP/MS GLVL/PREFONLY/S,N,ITSERR/SEID $
A Square matrix (real or complex, symmetric or unsymmetric).
B Rectangular matrix (real or complex), the right-hand side.
XS Optional starting vector, same type as B (may be purged).
PC Optional stepwise preconditioner, same type as A (may be purged).
USET Degree-of-freedom set membership table. See Remark 3.
KGG Stiffness matrix - g-set. See Remark 3.
GM Multipoint constraint transformation matrix. See Remark 3.
USET0 USET table from previous adaptivity index in p-version analysis.
SIL Scalar index list.
SIL0 SIL table from previous adaptivity index in p-version analysis.
EQEXIN Equivalence table between external and internal grid/scalar identification numbers.
Required for p-version preconditioning only.
EDT Table of Bulk Data entry images related to element deformation, aerodynamics, p-
element analysis, divergence analysis, and the iterative solver.
CASECC Table of Case Control command images. Required if SMETHOD Case Control
command is used and NSKIP=-1.
EQMAP Table of degree-of-freedom global-to-local maps for domain decomposition.&lt;/p&gt;
&lt;p&gt;93 Chapter 5: Iterative Solution of Systems of Linear Systems
User Interface
Main Index
Output Data Blocks:
Parameters:X Solution matrix. Rectangular matrix having the same dimensions and type as [B].
R Residual matrix. Rectangular matrix having the same dimensions and type as [B], the
residual [R] = [B] - [A][X].
PC1 Updated stepwise preconditioner matrix. See Remark 6.
EPSSE Table of epsilon and external work.
SIGN Input-integer-default = 0. Sign flag for [B].
0 : + [B]
1 : -  [B]
ITSOPT Input-integer-default = 0. Preconditioner flag. See Option Selection , 95.
ITSEPS Input-real-default = 1.0E-6. Convergence parameter epsilon.
ITSMAX Input-integer-default = 0. Maximum number of iterations. The default value implies
N/4 (N = dimension of [A]).
IPAD Input-integer-default = 0 (see table below). Padding level for reduced or block
incomplete Cholesky factorization (0, 1, 2, &amp;hellip;). See Option Selection , 95.
IEXT Input-integer-default = 0. Extraction level in reduced or block incomplete Cholesky
factorization. See Option Selection , 95.
ADPTINDX Input-integer-default=0. P-version analysis adaptivity index. See Remark 7.
NSKIP Input-integer-default=1. Record number of current subcase in CASECC and used
only if the SMETHOD command selects the ITER Bulk Data entry which specifies
values for the desired iteration parameters. If NSKIP=-1 then CASECC is not
required and the values are taken from the module specification of the values.
MSGLVL Input-integer-default=0. Message level output. See Option Selection , 95.
PREFONLY Input-integer-default=0. Preface execution only. If set to -1 then SOLVIT is
terminated after the preface information is computed and printed.
ITSERR Output-integer-default=0. Iterative solver return code.
1 no convergence
2 insufficent memory
SEID Input-integer-default=0. Superelement identification number.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Iterative Method Selection94
Main Index
Remarks:
1.If ITSOPT = 3, the IPAD level is recommended to be 0, 1, or 2 (IEXT = 0) and should be increased
when IEXT is increased.
2.The amount of memory needed for ITSOPT = 3, 10, and 11 increases with the increase of the
parameters IPAD and IEXT.
3.For ITSOPT = 1 or 2, the input data blocks USET, KGG, and GM may be purged. For ITSOPT =3,
USET must be specified. KGG and GM are necessary only if IEXT =2.
4.If the message “ *** USER FATAL MESSAGE 6288 (SITDRV): UNABLE TO CONVERGE
WITH ITERATIVE METHOD” is issued, then results will still be printed but may be inaccurate.
5.If data block PC1 is specified, the CPU time will increase slightly.
6.If SOLVIT is to be used for p-element analysis and ADPTINDX&amp;gt;1, then XS and PC must be the
solution matrix and pre-conditioner from the previous adaptivity p-level. Also, the USET and SIL
from the previous p-level are specified for U and KGG and the USET and SIL from the current p-
level are specified for GM and SIL.
7.For frequency response analysis with ITSOPT=10 or 11 (block incomplete Cholesky), IEXT=0 is not
available and IEXT=1 is used automatically.
Iterative Method Selection
The NASTRAN keyword ITER (equivalent to SYSTEM(216)) is used to select the iterative solution by
ITER = YES.  The default is ITER = NO.
The defaults for the iterative solver can be changed via the Bulk Data entry ITER, which needs to be selected
in the Case Control Section as
SMETHOD = SID
The Bulk Data entry ITER uses a free parameter format as follows:
where CHAR = character type and INT = integer.
Note that the order and existence of the parameters is not mandatory.  For example, the following Bulk Data
entry selected by SMETHOD=10 in the Case Control Section is valid:
This entry chooses a Jacobi preconditioner with 1.0E-04 accuracy.ITER SID
PRECOND= CHAR CONV=CHAR
MSGFLG=C HAR ITSEPS=REAL ITSMAX=INT
IPAD=INT  IEXT=INT PREFONLY=INT
ITER 10
ITSEPS=1 .0E-04 PRECOND=J&lt;/p&gt;
&lt;p&gt;95 Chapter 5: Iterative Solution of Systems of Linear Systems
Option Selection
Main Index
Continuation lines must start in column 9, aligning with SID.  Embedded spaces and commas are allowed
and the conventional continuation entry markers (+xx, +xx) are obsolete.
All integer and real parameters correspond to the appropriate SOLVIT parameters shown below.  The
character parameters used for method and option selections are also described in Option Selection , 95.
Option Selection
Preconditioner Options
The iterative solver preconditioner options are controlled via the IOPT and the PRECOND parameters as
shown below:
The scaling option is chosen by adding an ‘S’ to PRECOND or negative sign of IOPT.  For example, CS or
-2 means incomplete Cholesky with scaling.
The option PRECOND=USER can be used in the following ways:
For direct frequency response (SOLs 8, 108) it will result in using the direct method for the first
frequency.  The factor from this decomposition will then be used for all subsequent frequencies as
the preconditioner with the iterative solver.
In cases where several linear systems of the same size need to be solved, and where the system
matrices differ only slightly, this option can be used with a DMAP ALTER.  A possible scenario is to
use the direct method to solve the first linear system, and then use the SOLVIT module for the
solution of the subsequent systems.  Specify the output data block from the decomposition
containing the factor as the 4th input data block (= user given preconditioner) to the SOLVIT
module.  The following lines of DMAP show a simple example of the solution of two linear systems:
A X = F
and
B Y = G
where A and B do not differ too muchPRECOND IOPT Preconditioner Type
J(S) 1(-1) Jacobi real, complex, symmetric,
unsymmetric
C(S) 2(-2) Incomplete Cholesky real, complex, symmetric,
unsymmetric
RIC(S) 3(-3) Reduced Incomplete Cholesky real, symmetric
USER 4 User given real, complex, symmetric
DECOMP A/L, /$&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection96
Main Index
The hierarchic (for p-version elements) preconditioning strategies are chosen as:
The block incomplete Cholesky (BIC) technique options are:
For the distributed parallel SOLVIT only Jacobi preconditioning is available.
Convergence Criterion Options
The convergence criterion options of SOLVIT are selected by CONV as follows:
For the BIC method the convergence criterion is automatically set to .FBS L, , F/X /$
SOLVIT B,G,,L,, ,,,,,/Y,,//4//////-1 $
PRECOND IOPT Strategy Type
PBCJ 5 Incomplete geometric, Jacobi hierarchic real, symmetric
PBDJ 6 Complete geometric, Jacobi hierarchic (default
for p-version problems)real, symmetric
PBDC 7 Complete geometric, Incomplete hierarchic real, symmetric
PRECOND IOPT Technique Type
BIC 11 Well conditioned problems (default for real
problems)real, symmetric
BICCMPLX 10 Complex problems (default for complex
problems)complex symmetric
CONV Criterion
AR&lt;br&gt;
GE&lt;br&gt;
AREX   and
GEEX   and&lt;/p&gt;
&lt;p&gt;97 Chapter 5: Iterative Solution of Systems of Linear Systems
Option Selection
Main Index
Diagnostic Output Option
The diagnostic output from the iterative solver is controlled by MSGFLG or MSGLVL as follows:
The parameter PREFONLY (default=0) can be set to -1, which will terminate the iterative solver after the
preface, giving some helpful information in UIM 4157 (.f04 file) such as matrix size and optimal memory
requirements for best performance.
Incomplete Cholesky Density Option
The density of the incomplete factor is controlled by the IPAD parameter as follows:MSGFLG MSGLVL Action
no (default) 0 Only minimal output (i.e., UIM 6447 [see Option Selection , 95],
information on whether convergence was achieved).  In case of multiple
loads, only if one or more loads did not converge, a final output is given
for each right hand side.
yes 1 For 1 RHS:  Output above + convergence ratio and norm of residual for
each iteration are given.
For &amp;gt; 1 RHS:  Minimal output + final output is given for each RHS.
IPAD Default Method ITSOPTModel
Type Type of [A]
0 reduced incomplete Cholesky 3 all real
2 block incomplete Cholesky 10,11 3-D real
3 block incomplete Cholesky 10,11 2-D or
mixedreal
5 block incomplete Cholesky 10,11 all complex&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection98
Main Index
Extraction Level Option for Incomplete Cholesky
Recommendations
The recommended preconditioner options are as follows:
In the case where it is known in advance that a particular model needs to be run many times (because the
model needs to be modified in between, the load vector changes, or for some other reason), it is highly
recommended to determine the best parameter setting for the iterative solver.  T o do this, one can simply vary
the IPAD value from 1, &amp;hellip; , 4, while monitoring the EST OPTIMAL MEMORY (see Performance
Diagnostics), the size of the SCRATCH FILE (see Performance Diagnostics), and the CPU time spent in
SOLVIT .  The defaults have been selected to give the best performance for most problems, but since the
iterative solver is very sensitive to the conditioning of the matrix, they may not be best for all problems.
T o find the EST OPTIMAL MEMORY without having to go through a complete run, PREFONLY=-1 can
be set on the ITER Bulk Data entry.  This option will give the desired information and cause MSC Nastran
to exit after the symbolic phase of the iterative solver.
The distributed memory parallel execution of the SOLVIT module takes advantage of the fact that the matrix
multiply operation (the most time-consuming part of certain iterative strategies) is also easily executed while
the system matrix resides in parts on the local memories.  This is also the method of the iterative option of
the STATICS supermodule.IEXT
Default Reduced Block
0 solid bodies, no rigid elements. Requires USET and SIL
1 shells only Heuristic block structure (default)
2 mixed including rigid elements n/a
Real symmetric positive definite systems
Sequential : PRECOND = BIC
Parallel : PRECOND = J
Complex symmetric positive definite systems
Sequential : PRECOND = BICCMPLX
p-version analysis
Sequential : PRECOND = PBDJ
Direct frequency response (SOLs 8, 108)
Sequential : PRECOND = USER
Unsymmetric or indefinite systems
Sequential : PRECOND = J
Note:   Except for SOL 108, all of the above recommended options are also the defaults.&lt;/p&gt;
&lt;p&gt;99 Chapter 5: Iterative Solution of Systems of Linear Systems
Iterative Solution Diagnostics
Main Index
Examples:
1.Solve [A][X]=[B] with Jacobi pre-conditioning with convergence established at 1.E-4 and maximum
allowed iterations of 55 specified for the module parameters.
SOLVIT   A,B,,,,,,,,,/X,,//1/1.E-4/55///-1 $
2.Same as 1 except parameters are obtained from the SMETHOD command and ITER entry.
SOLVIT   A,B,,,,,,,,EDT,CASECC/X,, $
Iterative Solution Diagnostics
There are several different types of diagnostics given by the iterative solver, including:
Accuracy diagnostics (.f06 file)
Performance and memory diagnostics (.f04 file)
Accuracy Diagnostics
In the .f06 output file of an MSC Nastran run, the following accuracy diagnostics and convergence messages
may be found:
UIM 6447:
ITERATIVE SOLVER DIAGNOSTIC OUTPUT
The following example shows diagnostics for MSCFLG=yes and MSGFLG=no (default).
are as given in  and , EPSILON is given in , and EXTERNAL WORK  is just the denominator of EPSILON.R
B&amp;mdash;&amp;mdash;&amp;ndash; and XI1+XI –
XI&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Iterative Solution Diagnostics100
Main Index
MSGFLG=yes
MSGFLG=no (Default)
This last portion of the diagnostic table prints either the last iteration number only or all the iteration
numbers based on the user selection of MSGFLG (see SOLVIT OPTIONS).
UIM 6448:
SOLUTION CONVERGED WITH ITERATIVE METHOD.*** USER INFORMATION MESSAGE 6447 (SITDR3 )
ITERATIVE SOLVER DIAGNOSTIC OUTPUT
IPS :  0.9999999975E-06
BIC PRECONDITIONING
ITERATION NUMBER       ||R|| / ||B||        ||X(I+1)-X(I)|| / ||X(I)||
1             .1209539266E+00             .10000000E+01
2             .2251168868E-02             .14229420E-01
3             .8846335248E-04             .613 78225E-03
4             .1581571489E-05             .13831341E-04
5             .5083508633E-07             .47836956E-06
*** USER INFORMATION MESSAGE 5293 (SBUT5 )
FOR DATA BLOCK KLL
LOAD SEQ. NO.              EPSILON               EXTERNAL WORK
1          -4.3350458E-16           1.2827542E+05
*** USER INFORMATION MESSAGE 6448 (SITDR3 )
SOLUTION CONVERGED WITH ITERATIVE MET HOD.
*** USER INFORMATION MESSAGE 6447 (SITDR3 )
ITERATIVE SOLVER DIAGNOSTIC OUTPUT
EPS :  0.9999999975E-06
BIC PRECONDITIONING
ITERATION NUMBER       ||R|| / ||B||     ||X(I+1)-X(I)|| / ||X(I)||
5             .5083508633E-07          .47836956E-06
*** USER INFORMATION MESSAGE 5293 (SBUT5 )
FOR DATA BLOCK KLL
LOAD SEQ. NO.             EPSILON            EXTERNAL WORK
1         -4.3350458E-16        1.2827542E+05
*** USER INFORMATION MESSAGE 6448 (SITDR3 )
SOLUTION CONVERGED WITH ITERATIVE METHOD.
MSGFLG=YES will print the information in every iteration.
MSGFLG=NO must be set by the user to suppress the information (default).&lt;/p&gt;
&lt;p&gt;101 Chapter 5: Iterative Solution of Systems of Linear Systems
Iterative Solution Diagnostics
Main Index
UIM 6320:
SOLUTION WAS CONTINUED BECAUSE EXTERNAL CONVERGENCE CRITERION WAS
PASSED.
This message was printed if convergence is not achieved within the maximum number of iterations, even
though the solution is accepted due to the energy norm check (see Iterative Solver Solutions ).
UFM 6288:
UNABLE TO CONVERGE WITH ITERATIVE METHOD.
UIM 5293:
FOR DATA BLOCK XX
LOADSEQ NO  EPSILON  EXTERNAL WORK
Performance Diagnostics
Performance diagnostics as well as information about the system matrix and the memory requirements are
given in the .f04 MSC Nastran output file with SIM 4157, which is also used in the direct solution.  An
example is shown below with interpretation of the meaning of the different variables:
MATRIX SIZE, DENSITY, STRING LENGTH, NUMBER OF STRINGS, NONZERO TERMS, and
FULL BAND WIDTH are all obvious characteristics of the system matrix.*** SYSTEM INFORMATION MESSAGE 4157 (SITDR3)
PARAMETERS FOR THE ITERATIVE SOLUTION WITH DATA BLOCK KLL (TYPE = RDP ) FOLLOW
MATRIX SIZE =  134333 ROWS                  DENSITY =  .00056
STRING LENGTH =    5.19 AVG         NUMBER OF STRINGS =    1910 K
NONZERO TERMS =   10107 K             FULL BAND WIDTH =    6243 AVG
MEMORY AVAILABLE =   37359 K WORDS                  IPAD =       2
NUMBER OF RHS =       1
BLOCK SIZE =       5             EST OPTIMAL MEMORY=   27347 K WORDS
EST MINIMUM MEMORY =    4217 K WORDS
MEMORY USED =   36639 K WORDS      PREFACE CPU TIME =   52.49 SECONDS
SCRATCH FILE =       0 K WORDS         AVG. CPU/ITER =   .7693 SECONDS
MEMORY AVAILABLE = K words of memory available to the iterative solver.
IPAD = padding level used (see User Interface , 92).
NUMBER OF RHS = number of load vectors.
BLOCK SIZE = block size used to block-structure the matrix.
ESTIMATED OPTIMAL
MEMORY= memory needed by iterative solver to run in core; ensures
optimal performance.
ESTIMATED MINIMUM
MEMORY= absolute minimum memory needed for iterative solver to
run; will not give best performance.
MEMORY USED = memory that was actually used by iterative solver.&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Iterative Solver Estimates and Requirements102
Main Index
The parameters IPAD, BLOCK SIZE, SCRATCH FILE, and MEMORY USED appear only for BIC
preconditioning.
For p-version analysis the additional information is printed in SIM 4157.
Iterative Solver Estimates and Requirements
Time estimates for a complete iterative solution are not given, because the number of operations per iteration
is different for each preconditioner. Moreover, it is never known in advance how many iterations are required
to achieve convergence.
However, the following paragraph gives the computation time for one iteration using Jacobi and BIC
preconditioning. The calculation of the computation time t is based on the operations necessary in each
iteration using a particular preconditioner.
Jacobi:
BIC:
where:PREFACE CPU TIME = time required for memory estimation, re-ordering, and
preconditioner calculation.
SCRATCH FILE =&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;0  in core run
0  amount of spill
GEOMETRIC DOFs =number of rows in stiffness matrix corresponding to the geometric
degrees of freedom.
HIERARCHIC DOFs =number of rows in stiffness matrix corresponding to the p degrees of
freedom.
-1matrix/vector multiplication
-1preconditioner application
-3dot products
ﬁ
-1matrix/vector multiplication
-1preconditioner application
-8dot products
-2saxpy
ﬁriaii
tM= NRHSN2N6N ++  
z Pr1–=
tM= NRHSN22NZp 10N + +   &lt;/p&gt;
&lt;/blockquote&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P6</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_006/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_006/</guid>
      <description>
        
        
        &lt;p&gt;103 Chapter 5: Iterative Solution of Systems of Linear Systems
Iterative Solver Estimates and Requirements
Main Index
The minimum and optimal memory estimates are equally difficult to determine, since they also depend on
the preconditioner used. Since BIC preconditioning is the most frequently used option and since the
estimates for Jacobi preconditioning are rather straightforward, some memory estimates for those two options
are given below:
Minimum memory in words (MINMEM)
•Jacobi:
•BIC:
Optimal memory in words (OPTMEM)
•Jacobi:
•BIC:
where:=number of right-hand sides
N =number of rows in matrix
P =preconditioner
=number of nonzero terms in P
M =average time of one multiply-add operation
=number of right-hand sides
NWPT =number of words per term
1 for long word machines
2 for short word machines
NZA =number of nonzero terms in system matrix
N =number of rows in system matrixNRHS
NZp
5 5NRHS2+ NNWPT      symmetric +
5 8NRHS4+ NNWPT      unsymmetric +
0.5NZA
OPTMEM MINMEM NZA2N+ NWPT + =
IPAD 23 OPTMEM 3NZA  =
IPAD 4 OPTMEM 4NZA =
NRHS&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
References104
Main Index
Remark
Memory requirements for BIC are only very rough estimates because of the:
Automatic reduction of IPAD value.
Automatic decision of post-truncation of the preconditioner for well-conditioned problems.
Recommendation
T o find reliable estimates, use:
PREFONLY = -1 on ITER Bulk Data entry.
References
Babikov, P . &amp;amp; Babikova, M. An Improved Version of Iterative Solvers for Positive Definite Symmetric Real
and Non-Hermitian Symmetric Complex Problems . ASTE, JA-A81, INTECO 1995.
Conca, J.M.G. Computational Assessment of Numerical Solutions . ISNA ’92, Praque, 1992.
Efrat, I.; Tismenetsky, M. Parallel iterative solvers for oil reservoir models . IBM J. Res. Dev. 30 (2), 1986.
Hageman &amp;amp; Young. Applied Iterative Methods . Academic Press, 1981.
Manteuffel, T . A. An Incomplete Factorization T echnique for Positve Definite Linear Systems , Math. of
Computation, Vol 34, #150, 1980.
McCormick, Caleb W., Review of NASTRAN Development Relative to Efficiency of Execution . NASTRAN:
Users’ Exper., pp. 7-28, September, 1973, (NASA TM X-2893).
Poschmann, P .; Komzsik, L. Iterative Solution Technique for Finite Element Applications . Journal of Finite
Element Analysis and Design, 19, 1993.
Poschmann, P .; Komzsik, L., Sharapov, I. Preconditioning T echniques for Indefinite Linear Systems . Journal
of Finite Element Analysis and Design, 21, 1997.&lt;/p&gt;
&lt;p&gt;Main Index
Chapter 6: Real Symmetric Eigenvalue Analysis
6Real Symmetric Eigenvalue
Analysis
Real Eigenvalue Problems
Theory of Real Eigenvalue Analysis
Solution Method Characteristics
DMAP User Interface
Method Selection
Option Selection
Real Symmetric Eigenvalue Diagnostics
Real Lanczos Estimates and Requirements
References&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Real Eigenvalue Problems106
Main Index
Real Eigenvalue Problems
The solution of the real eigenvalue  problem is very important for many analysis solution sequences. The
problem is numerically difficult and time consuming; therefore, MSC Nastran offers methods in two
different categories: the reduction (tridiagonal) method, and the iterative (Lanczos) method.
The problem of normal modes analysis is of form:
The problem of the buckling analysis is stated as
where:
These problems may be solved with a reduction  type method by transforming to a canonical  form and
reducing the whole matrix to tridiagonal form. An iterative  method usually does not modify the matrices&lt;br&gt;
and ; it may use their linear combination of  where  is a shift value. The Lanczos&lt;br&gt;
method, as implemented in MSC Nastran, is a method using this technique. The detailed theory of real
eigenvalue analysis is discussed in Theory of Real Eigenvalue Analysis , 106.
Although the methods are mathematically interchangeable, the Lanczos method is recommended for the
solution of large buckling and normal modes problems, for example, those arising in the analysis of complete
vehicles. The reduction methods are useful for small normal modes problems in analysis of structural
components.
Theory of Real Eigenvalue Analysis
T wo main methods of real eigenvalue extraction are provided in MSC Nastran in order to solve the wide
variety of problems arising in finite element analysis applications:
1.Reduction (Tridiagonal) Method
2.Iterative (Lanczos) Method
In a reduction method, the matrix of coefficients is first transformed while preserving its eigenvalues into a
special form (diagonal, tridiagonal, or upper Hessenberg) from which the eigenvalues may easily be extracted.
In an iterative method, a certain number of roots are extracted at a  time by iterative procedures applied to the =the stiffness
=differential stiffness
=mass matrices
=eigenvalue
=eigenvectorKxMx =
KxKdx =
K
Kd
M

x
K
M KsM + s&lt;/p&gt;
&lt;p&gt;107 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
original dynamic matrix. One of the methods used in MSC Nastran is a transformation method (tridiagonal
method), and one (shifted block Lanczos method) is an iterative method.
The preliminary transformation procedure of the transformation methods requires that the major share of
the total effort be expended prior to the extraction of the first eigenvalue. Thus, the total effort is not strongly
dependent on the number of extracted eigenvalues. In marked contrast, the total effort in the iterative
methods is linearly proportional to the number of extracted eigenvalues. Therefore, it follows that the
iterative methods are more efficient when only a few eigenvalues are required and less efficient when a high
proportion of eigenvalues are required.
The general characteristics of the real methods used in MSC Nastran are compared in Table 6-1. The
tridiagonal method is available only for the evaluation of the vibration modes of conservative systems and not
for buckling analysis due to restrictions on the matrix form. The Lanczos method is available for all vibration
modes and buckling problems currently solved by MSC Nastran.
It may be noted from Table 6-1 that a narrow bandwidth and a small proportion of extracted roots tend to
favor the Lanczos method. An example of such a problem is the evaluation of the lowest few modes of a
structure. When the bandwidth is relatively large, and/or when a high proportion of the eigenvalues are
required, the tridiagonal method is probably more efficient, assuming the problem size is not too large.
The main advantage of including two methods is to provide a backup method if one method should fail (as
sometimes happens with all methods of eigenvalue extraction).
Table 6-1  Comparison of Methods of Real Eigenvalue Extraction
Characteristic/MethodTridiagonal
MethodLanczos
Method
Matrix pencil
oror
Restrictions on matrix
characterA real, symmetrical,
constant
or positive
semidefinite
or
positive
semidefiniteAI
AK
M&amp;mdash;&amp;ndash;=
AM
KM+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =MKM–1–MM  
KKKd– 1–KK  
M Singular 
KM Singular  +M
K&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis108
Main Index
where:
Reduction (Tridiagonal) Method
The reduction method of MSC Nastran offers Givens or Householder tridiagonalization options.
Transformation to Canonical Form
In the tridiagonal method when the eigenvalue problem is solved in a canonical mathematical form, a
Cholesky decomposition is performed as follows:
where  is a lower triangular matrix. The procedure used to obtain the factors is described in Decomposition
Process , 58. The symmetric matrix  is then obtained by the following transformation:
Let
where  is the transformed vector. Then
After the eigenvalue of  is found, the eigenvectors can be calculated by using .Obtains eigenvalues in order All at once Several&amp;ndash;nearest to
the shift point
Takes advantage of bandwidth
or sparsityNo Yes
Number of calculations
Recommended All modes Few modes
=number of equations
=semi-bandwidth or similar decomposition parameter (such as average front size)
=number of extracted eigenvaluesTable 6-1  Comparison of Methods of Real Eigenvalue Extraction
Characteristic/MethodTridiagonal
MethodLanczos
Method
0n3 0nb2E
n
b
E
M CCT=
C
A
C1–KuC1–CCTu0= –
u C1T–x =
x
A C1–KC1T–=&lt;/p&gt;
&lt;p&gt;109 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Tridiagonal Method
T ridiagonal methods are particularly effective for obtaining vibration modes when all or a substantial fraction
of the eigenvalues and eigenvectors of a  real symmetric matrix are desired. The general restrictions on the use
of the method within MSC Nastran are described in Table 6-1. The basic steps employed in the method are
as follows. First, the canonical matrix is transformed into a tridiagonal matrix
Next  is transformed to diagonal form:
Finally, the eigenvectors are computed over a given frequency range or for a given number of eigenvalues and
are converted to physical form.
Givens Tridiagonalization Method
The most recognized and stable tridiagonalization methods are the Givens and Householder methods. In the
tridiagonal method of MSC Nastran, both the Givens and Householder solutions are used. This section
describes the Givens solutions, and the next section describes the Householder solutions.
The Givens method depends on orthogonal transformations  of a symmetric matrix . An
orthogonal transformation is one whose matrix  satisfies the following:
The eigenvalues of a matrix are preserved under an orthogonal transformation since
Consequently, if det  vanishes, then det  also vanishes.
The effect of a series of orthogonal transformations on the eigenvectors of a matrix is a succession of
multiplications by orthogonal matrices. If
and if  are orthogonal matrices, then
Through the substitution
we obtainA At
At
Atdiag 
TATTA
T
TTTTTT I = =
TAI – TTTATTI – =
AI –  TATTI –  
Axx =
T1T2 Tr 
TrTr1–T2T1Ax= TrTr1–T2T1x
x T1TT2TTr1–TTrTy =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis110
Main Index
where  is applied repeatedly to obtain the final form. Here  is an eigenvector of the transformed matrix:
and that  may be obtained from  by:
The Givens method uses orthogonal matrices , which are identical with the unit matrix  except for
the four elements:
The orthogonal transformation  leaves all the elements of  unchanged except those in the
st and j-th rows and columns, the so-called plane of rotation. The four pivotal elements of the
transformed matrix are:&lt;/p&gt;
&lt;p&gt;where , etc., are elements of the untransformed matrix. The other elements of the st and j-th
rows and columns of the transformed matrix are:
In the Givens method,  is chosen such that  vanishes, which happens whenTrTr1–T2T1AT1TT2TTr1–TTrTy
= TrTr1–T2T1T1TT2TTr1–TTrTy
= y
y
TrTr1–T2T1AT1TT2TTr1–TTrT
x y
x T1TT2TTr1–TTrTy =
T I
ti1+i1+tjji1+jcos = =
ti1+jtji1+– i1+jsin = = 
TATTA
i1+
ai1+i1+ai1+i1+cos2i1+jajjsin2i1+jai1+j2i1+jsin + + =
ajjai1+i1+2i1+jsin ajjcos2i1+jai1+j2i1+jsin – + =
ai1+iaji1+ai1+j2i1+jcos =1
2&amp;mdash;ai1+i1+ajj–  2i1+jsin – =
ajji1+
ai1+sasi1+ai1+si1+jcos = ajsi1+jsin + =
ajsasjai1+s– i1+jsin ajsi1+jcos + = = 
i1+jaij
i1+jtanaij
aii1+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =&lt;/p&gt;
&lt;p&gt;111 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
The calculation of  followed by the orthogonal transformation
is carried out for a sequence of iterations with . The values of  used in , , , and  are
. For each , a set of  transformations is performed with  assuming the
values of  before the next value of  is used. As a result, the elements in the matrix
positions   are successively reduced
to zero together with their transposes, the  elements. Thus, the set of
transformations reduces the matrix to tridiagonal form.
MSC Nastran employs a procedure introduced by Wilkinson (1965) in which the Givens method is modified
by grouping the  transformations together, which produces zeroes in the i-th row and column.
This procedure should not be confused with the Householder method which eliminates a row and column
at a time. The Wilkinson process is particularly advantageous when the matrix  is so large that all the
elements cannot be held in memory at one time. This process only requires  transfers of the matrix
to and from auxiliary storage instead of the  transfers required by the unmodified Givens
method. This method requires 4 n memory locations for working space that are divided into four groups of
n storage locations each. The first  rows and columns play no part in the i-th major step. This step
has five substeps as follows:
1.The i-th rows of  are transferred to the set of memory locations in group 1.
2.The values of , , ,  are computed
successively from the following:
where the superscripted term is computed by the following:
and the starting value for  is as follows:i1+j
Am TmAm1–TmT=
A0 A = i
123 n1–  i ni– 1– j
i2i3n ++ i
1314,  1n2425 2n n2n– 
3141 nn1– 
ni– 1–
A
n2–
n1–n2–2
i1–
A
i1+i2+cos i1+i2+ sin i1+ncos i1+nsin
i1+jcosaij1+j1–
aii1+j1–2aij1–2+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; =
i1+jsinaii
aii1+j1–2aij2+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
aii1+j1–aii1+j2–2aij1–2+ =
ji2+=
aii1+i1+aii1+=&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis112
Main Index
The  may be overwritten on those elements of the untransformed matrix  which are
no longer required, and the  is stored in the group 2 storage locations.
3.The st row of  is transferred to the group 3 storage locations. Only those elements on
and above the diagonal are used in this and succeeding rows. For , in turn,
the operations in substeps 4 and 5 are carried out.
4.The k-th row  is transferred to the group 4 storage locations. The elements ,
, and  are subjected to the row and column operations involving  and
. For  in turn, the part of the row transformation involving
and  is performed on  and . At this point, all  the transformations
involving the i-th major step were performed on all the elements of row  and on the elements ,
,  of row .
5.The completed k-th row is transferred to auxiliary storage.
When substeps 4 and 5 are complete for all appropriate values of , the work on row  is also
complete. The values of  and  for , ,  are transferred to the
auxiliary storage, and row  is transferred to the group 1 core storage locations. Since the  row
plays the same part in the next major step as in the i-th in the step just described, then the process is ready
for substep 2 in the next major step. In fact, substep 1 is only required in the first major step because the
appropriate row is already in the storage locations of group 1 while performing subsequent steps.
Householder Tridiagonalization Method
The Householder tridiagonalization method uses the following transformation:
where:
The elements of  are chosen so that  has zeroes in the r-th row except the three diagonal positions. The
configuration and partitioning of the  can be shown as:=&lt;/h1&gt;&lt;p&gt;=1ijcos aij
i1+jsin
i1+ A
k i2i3n ++=
A ai1+i1+
ai1+kakkikcos
i1+ksin ik1k2n ++=
i1+kcos i1+ksin ai1+ak
k i2+
i3+ n i1+
k i1+
i1+kcos i1+ksin k i2+= i3+n
i1+ i1+
ArPrAr1–Pr=
A0 A
Pr
I2wrwrT–
wrTwr
wrAr
Ar1–&lt;/p&gt;
&lt;p&gt;113 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
where:
The  transformation matrix can be partitioned as
where  = a vector of order
By executing the multiplication given in the right-hand side of , the following is obtained:
where .
If we have chosen  so that  is null except for its first component, then the first  rows and
columns of  are tridiagonal.=a tridiagonal matrix of order  (partial result)
=a square matrix of order  (part of original matrix)
=a vector having  componentsAr1–&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  = =r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nr–xx &lt;br&gt;
xxx&lt;br&gt;
xxxxx
xxxx
xxxx
xxxxCr1–0
br1–T
0br1–Br1–
Cr1– r
Br1– nr–
br1– nr–
Pr
Prr &lt;br&gt;
nr–    I0
0Qr I 0
0I2vrvrT–= =
vrnr–
Ar1– =&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nr–Cr1–0
crT
0 crQrBr1–QrT
crQr Br1–=
vrcrr1+
Ar&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis114
Main Index
An algorithm formulation can be developed by writing  in the following form:
where  is a vector of order  with zeroes as the first  elements. Adding further changes as follows:
where:
and  is the  element of . Substituting  into , the following equation results:
with a new notation as follows:
Now the form becomes the following:
By introducing one more intermediate element=&lt;/h1&gt;&lt;h1&gt;=&lt;/h1&gt;&lt;h1&gt;=Pr
PrI2wrwrT–=
wrn r
PrIururT
2KrT&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –=
ur 0 i12r =
ur1+r arr1+  Sr
uir ari ir2n +=
Sr2
ari2
ir1+=n

2Kr2
Sr2  arr1+ Sr
aijij A
ArIururT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –

Ar1–IururT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –
&lt;/h1&gt;&lt;p&gt;prAr1–ur
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
ArAr1–urprTprurT–ururTprurT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- + – =&lt;/p&gt;
&lt;p&gt;115 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
the final algorithmic form is obtained
which takes advantage of the symmetry.
Modified Tridiagonal Methods
The modified tridiagonal methods (i.e., modified Givens or Householder) are used if  is singular. These
methods have a different transformation scheme to obtain the canonical form. First, perform a Cholesky
decomposition on a shifted matrix as follows:
where  is a positive shift value obtained from the diagonal terms of  and  matrices. The new
form of  is now
where:
The  shift value is calculated as:
where  and  are the diagonal elements of  and  respectively. If  or  or
, the term is omitted from the summation. If all terms are omitted,  is set to 0.001. If the
value calculated does not result in a stable Cholesky decomposition, the value is slightly perturbed, and
the decomposition is repeated up to 2 times. After three unsuccessful shifts, a fatal error is given.=
=qrpr1
2&amp;mdash;ururTpr
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;

– =
ArAr1–urqrTqrurT– – =
M
KsM +  CCT=
sK M
AIx0= –
 1
s+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;
AC1–MC1T–
s
s1
n12 Mii
Kii&amp;mdash;&amp;mdash;&amp;ndash;
i1=n
&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
MiiKiiM K Mii0= Kii0=
MiiKii 108 s
s&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis116
Main Index
The details of the above equation can be seen by rearranging  as follows:
Then by premultiplying and substituting  the following is obtained:
Finally, premultiplying by  and substituting the term
gives the form of .
QR Method of Eigenvalue Extraction
In the tridiagonal methods of Givens or Householder, MSC Nastran employs the  transformation of
Francis (1962) which is an orthogonal transformation  so that  may be
factored into the product  where  is an upper triangular matrix. Thus,
and
Now  by virtue of the orthogonality property; therefore,
It follows that  is determined from  by performing in succession the decomposition given by&lt;br&gt;
and multiplication. Francis has shown that if a matrix  is nonsingular, then  approaches
an upper triangular matrix as . Since eigenvalues are preserved under orthogonal transformation, it
follows that the diagonal elements of the limiting matrix are the eigenvalues of the original matrix .
Although the method can be applied to any matrix, it is particularly suitable for tridiagonal matrices because
the bandwidth of the matrix can be preserved as will be shown. In the case where  is symmetric, the
matrix  tends to take a diagonal form as .KsMs+M – + u0=
1–s+
MCCT
s+&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; –
   u 0=
C1–
x CTu =
QR
Ar1+ QrTArQr = Ar
QrRr Rr
Ar QrRr =
Ar1+ QrTArQr =
QrTQrRrQr =
QrTQr I=
Ar1+ RrQr =
Ar1+ Ar
A A1 = Ar
r
A
A
Ar r&lt;/p&gt;
&lt;p&gt;117 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Even though the upper triangular matrix  and the orthogonal matrix  are unique, several methods
are available for performing the decomposition. In the method of calculation devised by Francis,  is
expressed as a product of  elementary rotation matrices where  is the order of  as follows:
The nonzero elements of the j-th elementary rotation matrix are the following:
The manner in which the  and  coefficients are obtained from the elements of  are shown later.
From the orthogonality property
we can define the nonzero elements of ,  and  as follows:Rr Qr
Qr
n1– n Ar
Qr T1T2Tn1– =
tjjjtj1+j1+jcj= =
tj1+jjtjjs+j–=
tkkj1= kj or j1+ 
cjsjAr
Rr Qr1–Ar QrTAr = =
Tn1–TTn2–TT2TT1Ar =
ArAr1+ Rr
Ara1b1 0
b2a2b3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bn1–an1–bn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0   bnan=&amp;hellip;
Ar1+a1b1  0
b2 a2b3&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bn1–an1–bn
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0   bn an=&amp;hellip;&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis118
Main Index
and
The coefficients of the elementary rotation matrices are selected to reduce the subdiagonal terms of  to
zero. Specifically,
and
Substitution yields the elements of  as follows:Rrr1  q1        t1      0&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;r2  q2  t2    
              
              
              
    rn1–     qn1–  tn1–
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0                      rn    =&amp;hellip;
Rr
sjbj1+
pj2bj1+2+ 12&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
cjpj
pj2bj1+2+ 12&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =

j12n1– =
p1a1=
p2c1a2s1b2– =
pjcj1–ajsj1–cj2–bj– = j34n1– =
Rr&lt;/p&gt;
&lt;p&gt;119 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
From , the elements of  are the following:
MSC Nastran uses a variation of Francis’s original method that avoids the calculation of square roots. This
variation uses the following equations in place of  as follows:
andrjcjpjsjbj1++ =  j12n1– =
rnpn=&lt;br&gt;
q1c1b2s1a2+ =&lt;br&gt;
qjsjaj1+cjcj1–bj1++ = j23n1– =
tjsjbj2+=              j12n2– = 
Ar1+
a1c1r1s1q1+ =&lt;br&gt;
ajcj1–cjrjsjqj+ =   j23n1– =
ancn1–rn=&lt;br&gt;
bj1+ sjrj1–=            j12n2– = 
aj1sj2+gjsj2aj1++ =  j12n1– =

angn=
bj1+2sj2pj1+2bj2+2+  = j12n2– =
bn2sn1–2pn2=&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis120
Main Index
The reason that the use of , , and  in place of  avoids the calculation of square roots can best be seen by
considering the terms input to and produced by these equations. For , the input terms consist of the elements
of  (which are ) and  (which are the elements of ). This
completes one iteration step but involves the calculation of square roots. However, for , , and , the input terms
consist of  and . The data produced is  and
, which serve as the input to the next iteration step. No square roots require computation
here.
Convergence of the tridiagonal matrix to a diagonal form is speeded up by origin shifting. MSC Nastran
employs a procedure suggested by Wilkinson’s quadratic shift which has shown that when the eigenvalues are
real, the best shift strategy is to subtract  from each diagonal element of  and thereby reduce each
eigenvalue by .
Another device useful in speeding up the determination of eigenvalues of tridiagonal matrices takes advantage
of zeroes that may occur in the off-diagonal rows. Let  matrix be represented in partitioned form as
follows:g1a1=
gjcj1–pjajsj1–2ajgj1–+  – = = j23n =
g12a12=
g12gj2
cj1–2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;        if cj1–0                 j23n =
cj2–2bj2    if cj1–0=               j23n =&lt;/h1&gt;&lt;p&gt;
Ar a1a2an b2b3bn  Ar1+
a1a2an b12b22bn2 a1a2an
b2b3bn 
anA
an
Ar&lt;/p&gt;
&lt;p&gt;121 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
In this matrix the diagonal terms in the lower right-hand partition are eigenvalues that were determined in
previous iterations. The j-th is the next lowest row in which the off-diagonal term  is zero thereby
uncoupling the equations in the first  rows from the rest. As a result, the eigenvalues of the matrix in
the central block may be obtained separately. Other uncoupled blocks may be found in the upper left
partition.
The iteration described by  and  is continued until  decreases to a satisfactory level of accuracy so
that  may be accepted as an eigenvalue of the shifted matrix.  must be negligible compared to
, that is,  must approximately equal . Then  is transferred to the
lower partition, and the process is continued until all the eigenvalues of the partitioned matrix are extracted.a1b1                                       &lt;br&gt;
b1a2b2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     aj2–   bj2–                                 
     bj2–   aj1–                                 
           ajbj                                   
                 bjaj1+bj1+                          
                                            
                                                            
                               bm2–am1–bm1–           
            bm1–am1–            
                    am1+          
                            
                            
                          an...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip;&amp;hellip;
bj1–
j1–
bm1–2
ambm1–2
am2bm1–2am2+ am2am&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis122
Main Index
Computation of Eigenvectors
The eigenvector corresponding to any eigenvalue  of the tridiagonal matrix may be determined by solving
of the following equations:
If the first  equations are used, the solution is obtained by taking  to be unity and substituting in
the equations to obtain values of . Wilkinson (1958) shows that this method is unstable and
suggests the following approach, which is used in MSC Nastran.
The tridiagonal matrix  is factored into the product of a lower unit triangle  and an upper
triangle . Partial pivoting is used in this decomposition; i.e., the pivotal row at each stage is selected to
be the equation with the largest coefficient of the variable being eliminated. At each stage, there are only be
two equations containing that variable. The eigenvector  is then obtained from the solution of the
equation as follows:
where  is randomly selected. The solution is easily obtained by backward substitution because&lt;br&gt;
takes the form as follows:
An improved solution is obtained by repeated application of  using the current estimate of  on the right-
hand side. Thus,i
n1–
a1i–x1b2 x20= +
b2 x1a2i–x2b3 x30= + +
bn1– xn1–an1––i xn1–bn xn0= + +
bn xn1–ani–xn0= +&amp;hellip;
n1– x1
x2x3xn
AiI– Li
Ui
i
Uii C =
C Ui
Uip1q1r1&lt;br&gt;
p2q2r2&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    pn2–qn2–rn2–
pn1–qn1–
 pn=...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;i&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P7</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_007/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_007/</guid>
      <description>
        
        
        &lt;p&gt;123 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
where
Wilkinson showed that, if the computed eigenvalue  is a close approximation to the true eigenvalue,
convergence is so rapid that not more than two iterations are required. The applied test is for the maximum
component of the eigenvector to remain unchanged (to single precision accuracy) in one iteration. The initial
vector  is chosen so that each element is unity.
In the case of a double eigenvalue, the above method gives one eigenvector . If you start with any initial
vector  orthogonal to  and apply the previous algorithm, convergence to the other eigenvector
results. The following procedure is used in MSC Nastran to compute eigenvectors associated with
multiple eigenvalues. If eigenvectors  with elements
are obtained, an initial vector  orthogonal to each of these
eigenvectors is obtained by taking the components  as unity and solving the
simultaneous equations as follows:
Accumulated round-off errors result in the computed multiple eigenvectors since they are not exactly
orthogonal to one another. The following Gram-Schmidt algorithm is used to produce an orthogonal set of
eigenvectors  from the almost orthogonal set . For , select as follows:Uiin
in1–
=
10
C =
i
C
1
b 1
2
12m 
d a1sa2sans  T= b
bm1+bm2+bn
b1a11b2a21 bmam1as1
sm1+=n
–= + + +
b1a12b2a22 bmam2as2
sm1+=n
–= + + +
b1a1mb2ma2m bmammasm
sm1+=n
–= + + +

&amp;hellip;
k ys xs s 1=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis124
Main Index
Then for , calculate as follows:
where  denotes the Euclidean norm&lt;/p&gt;
&lt;p&gt;of vector  and  is a scalar product of the vectors  and .
When all the eigenvectors of the tridiagonal matrix are extracted, the back transformations are carried out to
obtain the eigenvectors of the original matrix . Finally, for all of the tridiagonal methods, the eigenvectors
are given one last stage of orthogonalization as follows:
Find , such that
by Cholesky factorization. Find the reorthogonalized eigenvectors  by a forward solution pass of
. It can be shown that this operation is equivalent to orthogonalizing the second vector with
respect to the first, orthogonalizing the third with respect to the purified second and first vector, , and
orthogonalizing the last vector with respect to all of the other purified vectors. If  is poorly
conditioned with respect to the Cholesky decomposition, a fatal error message is produced.
Shared Memory Parallel Householder Method
The most time-consuming part of the reduction type methods is the reduction to tridiagonal form. Besides
being more easily vectorizable, the Householder method also lends itself easily to parallelization. The
computer science aspects of the parallel Householder method (i.e., creating subtasks, waiting, etc.) are similar y1x1
x1&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
1sk
zs xs xsTyt  yt
t1=s1–
– =
yszs
zs&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
zs
zs12zs22 zsn2+ + +
zs1zs2zsn  TxsTyt xs yt
A
MmodalTM =
L
LTL Mmodal=

LTT=

Mmodal&lt;/p&gt;
&lt;p&gt;125 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
to those of the parallel methods mentioned previously and are not detailed here. The necessary mathematical
details follow.
A logical suggestion is to “aggregate” the Householder transformations, i.e., work in column blocks instead
of columns. For example, by observing the first k steps, the following can be written:
where:
Completing the process in k-size blocks, produces the following:
where
Before the j-th block step, the structure of  is as follows:=
and =  matrices (  representation)AkPkPk1– P1  APkPk1– P1  T=
QkAKkT=
QkIWkYkT+
W Y nk WY
AjQjAQjT= j1= 2nk
QjPjkPjk1– Pjkk1+–   =
A&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis126
Main Index
where:
The process is then to generate  so that  assumes tridiagonal form and afterwards update
the remainder of the matrix as shown:
This block update can be parallelized and forms the parallel portion of the MSC Nastran Householder code.
It is intended for shared memory parallel computers. It does not run on distributed memory computers. A
similar approach can also be used in the reduction to Hessenberg’s form, which is one of MSC Nastran’s
complex eigenvalue methods. However, this approach is not implemented as yet.=tridiagonal matrix of order
=general matrix of order
=general matrix of order Aj Cj1– = Bj1– A1j1–
j1–k k n jk –
Cj1–nj1–k
Bj1–nk
Aj1–nnjk–
IWj YjT+ Bj1–
AjIWjYjT+ Aj1–IWjYjT+ T=&lt;/p&gt;
&lt;h1&gt;127 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Real Symmetric Lanczos Method
The basic Lanczos recurrence (Lanczos, 1950) is a transformation process to tridiagonal form. However, the
Lanczos algorithm truncates the tridiagonalization process and provides approximations to the eigenpairs
(eigenvalues and eigenvectors) of the original matrix. The block representation increases performance in
general and reliability on problems with multiple roots. The matrices used in the Lanczos method are
specifically selected to allow the best possible formulation of the Lanczos iteration and are described in
Table 6-1 of Theory of Real Eigenvalue Analysis .
Basic Lanczos Recurrence
The basic Lanczos algorithm solves the problem as follows:
and can be formulated as follows:
1.Initialization
a.A starting vector  is chosen with .
b.  and  are set.
2.lteration
For , iterate until convergence as follows:
where:
If this algorithm is carried out without round-off errors, the vectors , are orthonormal. These
vectors can be considered to be the column of an orthogonal matrix . The scalars
and  can be combined in a tridiagonal matrix  as follows:=&lt;/h1&gt;&lt;p&gt;=Axx=
q1q11=
10= q00=
j123 =
rj1+Aqjjqjjqj1–– – =
jqjTAqj
j1+rj1+
qj1+rj1+j1+
q1q2q3
Qjq1q2qj   =
jjTj&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis128
Main Index
With this notation, the first  steps of the Lanczos algorithm can be combined in matrix form as follows:
where the vector  is the unit vector with zero entries except for the j-th row which holds the value of one.
This mathematical presentation is not sufficient for a practical implementation of the Lanczos algorithm that
is capable of coping with finite precision arithmetic. The computation of the Lanczos loop can be carried out
in many mathematically equivalent ways, one of which is the best for finite arithmetic, while several others
do not work in finite arithmetic (Grimes, R.G., et al.). Even with the recurrence written in the proper form,
round-off errors can cause the algorithm to behave quite differently from the ideal description given above.
More details on the effects of round-off errors are given in the section on preserving orthogonality.
The Lanczos algorithm is valuable for solving sparse eigenvalue problems because it needs only a partial
tridiagonalization and does not actually modify the matrix . The matrix  enters the algorithm only in
the formation of the matrix vector product . In practice, this means that the sparsity of  can be
exploited in a natural way since all that is needed is an efficient subroutine to compute  from .
The eigenvalues and corresponding eigenvectors of  are computed from those of . Let  and  be an
eigenpair of the tridiagonal matrix ; that is, let
Then  and  with  is an approximate eigenpair for the original problem. Note that in a typical
application, the order of matrix  may be in the tens of thousands, whereas the order of  is about 20 to
30. Hence, it is much easier to solve the eigenvalue problem of  than of . But the question is: How good is Tj12   &lt;br&gt;
223      &lt;br&gt;
334&lt;/p&gt;
&lt;p&gt;iii1+&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   j
  jj=...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip;
j
AQjQjTjj1+qj1+ejT= –
ej
A A
Aq A
Av v
A T s
Tj
Tjs s=
 y y Qj=
A Tj&lt;/p&gt;
&lt;p&gt;129 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
the approximate eigenpair , or more directly, when should the Lanczos recurrence be stopped so that the
eigenpairs are satisfactorily accurate?
The correct answer requires the norm of the residual for the eigenpair , which requires the full
computation of a tentative eigenpair. In fact, this residual norm can be obtained from  by observing that
where  is the j-th or bottom element of the eigenvector . Hence, the norm of the residual is given by
and the quantity can be computed without computing the eigenvector  explicitly. A small extra
effort allows the convergence of the eigenpairs to be monitored and terminates the Lanczos loop whenever
the required number of sufficiently accurate eigenpairs is found. The eigenvector  is computed only once
at the end.
Why should it be expected that some of the eigenvalues of the tridiagonal matrix  converge quickly to  the
eigenvalues of ? This question is answered by some intricate mathematical theorems which guarantee that,
under reasonable assumptions on the properties of the starting vector, approximations to some of the extreme
eigenvalues appear in the tridiagonal matrix very quickly. T wo factors can impede this rapid convergence. If
the starting vector has no components in the direction of one of the eigenvectors, the convergence of this
eigenvector is delayed. A cluster of poorly separated eigenvalues also causes slow convergence.
Shifted Algorithm
The basic Lanczos algorithm as discussed in the previous section must be modified in two respects to solve
the practical vibration problem. The vibration problem is a symmetric generalized eigenvalue problem using
the following form:
where  is a symmetric matrix,  is a symmetric positive semidefinite matrix, and  is the eigenvalue. If
is positive definite, the Cholesky factorization of  can be computed and  can be reduced to an ordinary
eigenvalue equation. However, there are considerably more difficulties when  is only semidefinite
(nonnegative definite). The implementation of the Lanczos algorithm in MSC Nastran avoids the Cholesky
factorization of  to use the more general form of the problem.
The typical eigenvalue problem is to compute the smallest (closest to zero) eigenvalues and eigenvectors of .
These eigenvalues are usually clustered closely together when compared to the largest eigenvalues. The
Lanczos algorithm applied directly to  yields precise approximations to these large, well-separated, but
uninteresting eigenvalues, and poor approximations to the small, clustered, and interesting eigenvalues. In
order to overcome this convergence difficulty, the Lanczos algorithm is applied to the shifted and inverted
eigenvalue problem in the following equation:y
Ayy–
Ayyj1+qj1+sj= –
sjs
j1+sjy
y
Tj
A
KxMx =
K M 
M M
M
M
MKM–1–Mx1
–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; = Mx&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis130
Main Index
In  the shift s is chosen close to the eigenvalues of interest. This formulation of the shifted and inverted
eigenvalue problem is only one of several possible ones.  is the preferred choice for vibration problems because
of its improved rate of convergence to the desired eigenvalues and the fact that the eigenvectors of  are also
eigenvectors of .
The relationship between the Lanczos algorithm of  and  is comparable to the relationship between the power
method and the inverse power method (inverse iteration). The choice of shift in the inverse power method
directs the convergence toward the eigenpair closest to the shift. For well-chosen shifts, the convergence can
be very rapid. The same is true for the shifted and inverted Lanczos algorithm. A properly chosen shift
guarantees rapid convergence not only to the closest eigenpair but also to a whole group of eigenpairs in the
vicinity of the shift. The cost of this accelerated convergence towards the desirable eigenvalues is the
factorization of  which, for large matrices, can be the dominating cost in either algorithm. The
computational superiority of the Lanczos algorithm derives from its more thorough use of this expensive
factorization. Even a relatively short run of the shifted and inverted Lanczos algorithm can extract many
eigenvalues using only one factorization.
The Lanczos algorithm applied to the vibration problem of  can be formulated as follows:
1.Initialization
a.A starting vector  is chosen.
b. is computed.
c. is normalized.
d.  and .
2.lteration
For  iteration is performed until convergence occurs.
where:
There are two remarkable facts about this formulation of the Lanczos algorithm for the eigenvalue problem.
First, no factorization of the possibly singular mass matrix  is required. Second, the Lanczos vectors
are orthogonal with respect to the inner product defined by . When  is semidefinite, it does
not define a true inner product. In that case, the initialization step guarantees that all Lanczos vectors are =&lt;/h1&gt;&lt;p&gt;=kM–
r1
r1KM–1–Mr1=
q1r1rTMr112 =
10= q00=
j123 =
rj1+KM–1–Mqjj– qjjqj1–– =
jqjTMKM–1–Mqj
j1+rj1+TMrj1+ 12
qj1+ rj1+j1+
M
q1q2 M M&lt;/p&gt;
&lt;p&gt;131 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
orthogonal to the null space of . Thus, the Lanczos algorithm works implicitly only with the components
of  that are orthogonal to its null space. This restriction of  to a positive definite matrix and the use of
the  inner product causes no problem. An extra benefit is that the Lanczos algorithm in the version above
generates no approximations to the infinite eigenvalues of  that arise when  is semidefinite.
The tridiagonal matrix  defined in  only contains approximations to the finite eigenvalues of  via the so-
called spectral transformation. For example, if  is an eigenvalue of , then
is an approximate eigenvalue of . The approximate eigenvector  can be computed in the same way as in the
previous section. Since the Lanczos vectors are -orthogonal, the eigenvectors corresponding to different
eigenvalues are also -orthogonal.
To solve the buckling problem of the form
a different shifting and inverting strategy is required. In , the differential stiffness matrix  is merely
symmetric and has no definite properties, whereas the stiffness matrix  is positive semidefinite. The
semidefinite property of  in the vibration analysis is critical to the Lanczos algorithm. The shifting strategy
applied in the vibration case cannot be applied to the buckling case by substituting  for .
Since the eigenvalues closest to zero are usually wanted, a simple approach is to interchange the roles of&lt;br&gt;
and  and then compute the largest eigenvalues of the problem. Therefore, by applying the Lanczos
algorithm without shift,
where . This approach has two drawbacks. First, it requires the factorization of the possibly
semidefinite matrix , thereby preventing its use in problems with rigid body modes. Second, it does not
allow any shifting for other eigenvalues.
A general shifting and inverting strategy is possible for the buckling problem. As shown previously, the
operator  is factored for an arbitrary shift, but the Lanczos recurrence is carried out using -
orthogonality among the Lanczos vectors. Each multiplication by the mass matrix  in the vibration case is
replaced by a multiplication by the stiffness  matrix in the buckling case. The rest of the Lanczos recurrence
remains the same. Hence, in  the buckling case the Lanczos algorithm works with the operator
and -orthogonality.M
M M
M
M
Tj
 Tj
1
&amp;mdash;+ =
y
M
M
KxKdx =
Kd
K
M
KdM
K
Kd
KdKx =
 1=
K
KKd– K
M
KKd– 1–K K&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis132
Main Index
This shifted and inverted operator allows for arbitrary shifts with the exception of a shift at zero that reduces
the problem to an identity matrix. For all other shifts, an eigenvalue  of  can be transformed as
to yield an approximate eigenvalue of . Eigenvectors are computed as before without any additional back
transformation resulting in a -orthogonal set. Since the stiffness matrix  is used in the initialization step
in the same way as  is used in the vibration problem, the sequence of Lanczos vectors and hence the
eigenvectors are orthogonal to the null space of . Therefore,  does not yield any approximation to the
exact zero eigenvalues of the buckling problem. The rigid body modes of the structure are  not computed
during the buckling analysis.
Block Method
In exact arithmetic, the simple Lanczos algorithm can only compute one eigenvector in a degenerate set.
Because of round-off errors introduced into the Lanczos recurrence, additional eigenvectors of multiple
eigenvalues eventually appear in the tridiagonal matrix . A second eigenvector of a multiple eigenvalue
only converges a number of steps after the first copy converged. (Effectively, this is the case where the starting
vector is orthogonal to the desired eigenvector.) Thus, the simple Lanczos algorithm has difficulties with
eigenvalues of high multiplicity.
Each step of the shifted Lanczos recurrence requires the solution of a sparse linear system of equations of the
form  and one multiplication by the matrix . In MSC Nastran these operations require
accessing matrices stored on disk files and thus entail significant I/O costs.
Block Lanczos algorithms have been developed in which the basic Lanczos recurrence is carried out for&lt;br&gt;
vectors simultaneously. If the idea of a block code is combined with the shifted and inverted Lanczos
algorithm, the following recurrence is obtained for the vibration problem:
1.Initialization
a.A starting block of  column vectors  is chosen.
b.  is computed.
c.An upper triangular  matrix  and an -orthonormal  matrix  are computed
so that .
d.The upper triangular  matrix  is set to  as well as .
2.lteration
a.For , the process iterates until convergence as follows: Tj

1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =
K K
M
K Tj
Tj
KM–x M
p
p R1
R1KM–1–MR1=
pp B0M np Q1R1Q1B0=
pp B10 Q00=
j123 =
Rj1+KM–1–MQjQjAjQj1–BjT– – =&lt;/p&gt;
&lt;p&gt;133 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
where
b.The following factorization is computed
where  is an  matrix with -orthonormal columns and  is a  upper
triangular matrix.
Using the block version of the  Lanczos algorithm provides the benefit that multiple eigenvalues of the original
problem can be computed more easily. All degenerate eigenvectors can be found together when the
multiplicity of an eigenvalue is less than or  equal to the block size . Further, the amount of I/O per column
is reduced by a factor of 1/  since it is possible to solve and perform the matrix multiplication for  vectors
simultaneously.
The description of the block Lanczos code is incomplete without a description of the factorization&lt;br&gt;
and consideration of the fact that  is now block tridiagonal. The factorization  is required to
obtain -orthonormal vectors in the recurrence. In this factorization,  and  are  matrices and&lt;br&gt;
is an upper triangular  matrix, which is chosen so that the columns of  are -orthonormal. This
procedure is implemented using a generalization of the modified Gram-Schmidt procedure that avoids
repeated multiplications by matrix . Both vectors in  and a second set, initially set to , are updated
during the Gram-Schmidt orthogonalization. At the completion of the procedure, the first set  of vectors are
transformed to  and the second set to . A multiplication by  is required at the beginning of the
procedure. A second multiplication is made at the end to ensure that  is accurate, but  no additional
multiplications by  are required during the procedure.
Another consequence of using a block Lanczos algorithm is that matrix , from which the approximations
to the eigenvalues are computed, is now a block tridiagonal matrix represented as follows:AjQjTMKM–1–MQj=
Rj1+Qj1+Bj1+=
Qj1+np M Bj1+pp
p
p p
R QB=
TjR QB=
M R Q np B
pp Q M
M R MR
Q MQ M
MQ
M
Tj&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis134
Main Index
Since the values for  are upper triangular,  is actually a band matrix with bandwidth .
The eigenvalues and eigenvectors of  are computed by the following procedure. First, reduce  to a
tridiagonal form . An orthogonal matrix  is found so that
Then perform an eigenextraction for . An orthogonal matrix  is found so that
where  is the diagonal  matrix of the eigenvalues of . Then, by combining  and , the following is obtained:
where the orthogonal matrix is the eigenvector matrix for .
The orthogonal matrices  and  are products of simple orthogonal matrices (Givens rotations)
or TjA1B2T   &lt;br&gt;
B2A2B3T      &lt;br&gt;
B3A3B4T&lt;/p&gt;
&lt;p&gt;BiAiBi1+T&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   BjT
  BjAj=...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip;
BjTjp1+
TjTj
T QH
QHT Tj QHT=
T QT
QTT T QT=
 T
QTT QHTTjQH QT=
QH QT H
QHQT
QH1QH2QHS
QT1QT2QTR&lt;/p&gt;
&lt;p&gt;135 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
These product matrices are accumulated by beginning with the identity matrix I and successively multiplying
on the right by
where:
.
The algorithms used in  and  are standard and numerically stable.
The actual implementation of the band  reduction algorithm of  uses modified (square root free) Givens
rotations to reduce the overall cost by eliminating the square root computations associated with each rotation.
The extraction of the eigenvalues and eigenvectors of  (see ) is essentially the same procedure used by the
algorithm (see QR Method of Eigenvalue Extraction , 116). The procedure used here is an explicitly shifted&lt;br&gt;
algorithm using ordinary Givens rotations. The square root
required for the calculation of the Givens rotations is computed by a special recursion based on the
Pythagorean Theorem. This recursion avoids overflow by not forming  or  and avoids destructive
underflow occurring with the implicitly shifted  algorithm for the matrices produced by the Lanczos
shifted block. The spectral transformation is applied as before to find approximate eigenvalues for the original
problem of . If  is an eigenvector of , then  with  is an
approximate eigenvector of . The residual norms of the eigenpairs are now given by  where the
values for  are the last  components of the eigenvector .
The expense of l/O operations suggests  that block size  should be as large as possible. The available memory
precludes choosing very large block sizes. However, large block sizes also entail other costs because the -
orthogonal factorization requires additional computation and the overall convergence depends highly on the
number of Lanczos steps and less on  the dimension of . Therefore, the actual block size is taken as a
compromise among the reduction of l/O costs, the possible increase in arithmetic operations, and the largest
multiplicity of the expected eigenvalues.QHi
i1r =
Note:   The eigenvector matrix is formed by products that take linear combinations of the columns. In
the intermediate steps when only the last  rows of the eigenvector matrix are desired, the
leading rows are ignored and the rotations, are applied only to the last  rows of the identity
matrix.p
p
T
QR QL
a2b2+
a2b2
QL
s Tjy Qjs = QjQ1Q2Qj   =
Bj1+sj
sjp s
p
M
Tj&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis136
Main Index
Orthogonalization
The Lanczos algorithm produces an orthonormal (or -orthonormal) set of Lanczos vectors in exact
arithmetic. (For simplicity, the orthogonality referred to always implies orthogonality with respect to the
inner product defined by ). The numerical algorithm is affected by round-off errors that cause the Lanczos
vectors to lose their  orthogonality. Maintenance of orthogonality is essential for preserving the convergence
properties of the Lanczos algorithm. Early implementations of the Lanczos algorithm employed a “full
reorthogonalization” scheme in which each new block of Lanczos vectors was explicitly reorthogonalized
against all previous Lanczos vectors. This process required  vector operations at step  as well as access
to all previous vectors (usually stored out-of-core). Instead of this expensive scheme, the Lanczos algorithm
in MSC Nastran employs a combination of several efficient reorthogonalization mechanisms that together
accomplish the computation of a sufficiently orthonormal set of Lanczos vectors.
Loss of orthogonalarity can occur in four different areas:
1.Within a given block of Lanczos vectors (internal).
2.With respect to the previous two blocks of Lanczos vectors (local).
3.With respect to the whole set of previously computed Lanczos vectors (global).
4.With respect to eigenvectors from different shifts (external).
Problems with orthogonality within a block of Lanczos vectors can arise if the vectors of  are almost
linearly dependent. For example, this problem occurs  when the shift 0 is extremely close  to an eigenvalue. In
this case, one step of the generalized modified Gram-Schmidt procedure is not sufficient to produce vectors
that are orthogonal to working precision. Gram-Schmidt is considered an iterative procedure that possibly
can be repeated several times until the Lanczos vectors are orthogonal. The Gram-Schmidt procedure is
referred to as “internal reorthogonalization,” which also requires updating the elements of .
The local loss  of orthogonality involves the previous two blocks of Lanczos vectors. The recurrence can be
considered an orthogonalization of the block  against the blocks  and . The block ,
computed from the Lanczos recurrence, may not be orthogonal to full working precision to  and .
Investigations indicate that the orthogonality between  and  is crucial for the correct continuation
of the Lanczos process. Therefore, one step of local reorthogonalization is carried out; i.e., the block&lt;br&gt;
is reorthogonalized against the block . This procedure may be repeated until the two blocks are orthogonal
to working precision. This local reorthogonalization also requires updating the elements of .
The global loss of  orthogonality between the block  and previous Lanczos blocks is of a different
nature. The Lanczos recurrence involves only three blocks of Lanczos vectors. The beauty and efficiency of
this recurrence lies in the fact that the three-term recurrence in exact arithmetic is enough to assure global&lt;br&gt;
orthogonality among all of the Lanczos vectors. Unfortunately, this is no longer true under the influence of
round-off errors. Once some tiny error is introduced  into the recurrence,  it becomes amplified in the course
of the next Lanczos steps and soon the global orthogonality property is lost. The mechanisms of this loss of M
M
pj2j
Rj1+
Bj1+
Rj1+QjQj1–Rj1+
QjQj1–
Rj1+Qj
Rj1+
Qj
Aj
Qj1+&lt;/p&gt;
&lt;p&gt;137 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
orthogonality have been investigated in the last decade by several researchers and are now well understood.
There are two important insights from the theoretical works that provide for an efficient implementation of
the global reorthogonalization. First, it is possible to monitor the loss of orthogonalization inexpensively by
updating certain estimates for loss of orthogonality at every Lanczos step. Second, it is sufficient to maintain
orthogonality at the level of the square root of the machine’s precision (semi-orthogonality) and still obtain
fully accurate eigenvalues. These two observations give rise to the scheme of partial reorthogonalization that
is generalized to the block Lanczos algorithm in the MSC Nastran implementation.
The fourth type of loss of orthogonality can only occur in the context of the shifted and inverted algorithm
and has nothing to do with the Lanczos recurrence directly. The MSC Nastran Lanczos algorithm begins by
computing  some eigenvalues and eigenvectors with an initial shift . If not all of the desired eigenvalues
are found, then a second Lanczos run with a new shift  is made. For reasons of efficiency and simplicity
in bookkeeping, the previously computed eigenvalues are prevented from being recomputed. This benefit is
achieved by the external selective orthogonalization implemented in MSC Nastran. For each new shift,  a set
of critical converged eigenvalues is determined. The Lanczos vectors are then kept orthogonal against the
corresponding eigenvectors of this selective group of computed eigenvalues using a  modification of the
technique of  selective orthogonalization. Estimates for the loss of  orthogonality with respect to the computed
eigenvectors are updated at each Lanczos step, and reorthogonalizations are performed only when semi-
orthogonality is about to be lost.
Shift Strategy
The eigenanalysis problem in MSC Nastran is to compute either the lowest  eigenvalues in magnitude or
all eigenvalues in an interval  where  and  can be any real numbers, such as . The shift
strategy incorporated in the block shifted Lanczos code allows the shifts selected in the interval  to
rapidly find  the desired eigenvalues without wasted factorizations. The main objective for the shift strategy is
to encompass the region containing the eigenvalues of interest with a trust region in which all the eigenvalues
have been computed and the number of eigenvalues has been verified with a Sturm sequence check.
The first shift is chosen at the primary point of interest (usually 0 in vibration or -1 in buckling) or the left
endpoint if it is finite. Additional shifts  are chosen to form or extend a trust region until the requested
eigenvalues are computed. These shifts are selected based on the information computed during previous&lt;br&gt;
Lanczos runs. A trust region can be extended in either direction.
The shift strategy is designed to  place the final shift at the correct place to simultaneously compute the last&lt;br&gt;
required eigenvalues and to make a Sturm sequence check for the entire interval. However, a final shift may
be required for a separate final Sturm sequence check. This check is designed conservatively so that shifts are&lt;br&gt;
not taken so far as to skip past part of the spectrum. However, if this does happen, the Sturm sequence count
indicates that not all  the eigenvalues are computed between two shift points. The shift strategy immediately&lt;br&gt;
attempts to find the missing eigenvalues rather than to extend the trust region further.
T wo values, called the left and right sentinels, are associated with all shifts. A right sentinel is defined as the
rightmost accepted eigenvalue that has no unaccepted eigenvalue approximations between the shift and itself.
The left sentinel is defined similarly for the accepted and unaccepted approximations to the left of the shift.&lt;br&gt;
When eigenvalues are missing  between two shifts, it is usually because the last shift was too  far from the
previous shift. In this case, the missing eigenvalues should be between the right sentinel of the leftmost of the 1
2
m
ab a b ab
ab&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis138
Main Index
two shifts  and the left  sentinel of the rightmost of the two shifts. A new shift is selected to bisect  the interval
between the above two sentinels. In the rare case when the sentinels overlap each other, the new shift bisects
the full interval between the two previous shifts. (The usual  reason that such an overlap occurs is that the
multiplicity of eigenvalues is greater than the Lanczos block size.)
There are several ways in which the shifting deviates from the description above. Four major special cases are
listed in this section. The first special case is the setting of a number . Initially,  is an approximation to
the first nonrigid body mode of the structure. In the case of an initial shift at  0 for a  structure with rigid body
modes, only the rigid body modes in the initial Lanczos run are computed. In such cases the shift strategy
does not update the value of , but instead uses the initial  to move away from 0 towards the first  nonzero
eigenvalue. There are  other special cases related to the rigid body modes where the Lanczos  algorithm may
terminate with no new information available for the next shift  selection. In these cases  is not updated, and
the previous value is maintained.
The second special case is when no new information is computed at two consecutive shifts. This case may
occur if there is a very large gap between modes of a structure or all remaining modes are infinite. The shift
strategy expands its step size to cross  the suspected hole in the spectrum. If the user-specified interval&lt;br&gt;
has a finite endpoint that was  not used previously, the shift strategy selects the new shift at that endpoint. If
the interval has only infinite endpoints remaining, then the new shift  is chosen at 10  past the last shift. If
no new information is computed at this shift, the next shift is chosen at 100  past the last shift. If there is
still no new information, then the Lanczos procedure terminates with the assumption that the remaining
eigenvalues are infinite. An appropriate warning is returned with those eigenvalues that were computed.
The third special case is the buckling problem. The operator used in the buckling problem for the Lanczos
iteration is ill-posed for shifts at or  near 0. The shift  strategy process of the buckling problem is  similar to the
vibration problem with the exception that the shifts near 0 are perturbed from 0.
The fourth major special case is the processing of factorization errors. In rare cases when the selected shift&lt;br&gt;
is exactly an eigenvalue, the operator  is singular, and the factorization can fail. If this  occurs, the
shift is perturbed, and an additional factorization is performed. If three factorization errors occur
consecutively, then the Lanczos procedure terminates with an appropriate error message and returns any
computed eigenvalues and eigenvectors. Presumably, the cause of such a failure is  an improperly posed
eigenvalue problem for which the mass and stiffness matrices have a common null space (massless
mechanisms).
Summary of Procedure
The general Lanczos procedure implemented in MSC Nastran can be summarized in the following figures.
There are two major levels in the procedure: the outer level in the shift strategy and administration, and the
inner level in the actual Lanczos iteration. Figure 6-1 describes the outer level and Figure 6-2 describes the inner
level. 
 

ab



KM–&lt;/p&gt;
&lt;p&gt;139 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Figure 6-1  Outer Level of the Lanczos ProcedureAdministration
Accepted EigenpairsUse the
Same
Shift?
Prepare OutputNo
Yes
Yes NoInitialization
Select a Shift
Value
Decompose Shifted
Matrix
Run Inner Level
Flowchart 2
Done&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis140
Main Index
Figure 6-2  Inner Level of the Lanczos ProcedureAnalyze :
Compute the Spectrum of
and analyze Residual Norms
for ConvergenceTjTjLanczos Step:
Compute&lt;br&gt;
from
the Lanczos recurrenceajqj1+j1+Initialization of Inner Level
Lanczos Loop:
for , maxstp do j12 =
Did Sufficiently
Many Eigenvalues
Converge?
Postprocessing of Eigenvectors
Exit Lanczos Loop
Compute Eigenvectors of A
End of Lanczos Run
(Return to Outer Level)No
Yes&lt;/p&gt;
&lt;p&gt;141 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Segmented Lanczos Method
The segmented version of Lanczos is intended to alleviate the orthogonality problems encountered on very
wide frequency range runs. The orthogonality may be lost when very long Lanczos runs are executed while
trying to span a wide frequency range of interest. The segment version forces intermittent shifts at semi-
regular intervals resulting in more frequent restarts of the Lanczos process. While it has significantly improved
the solution quality, i.e., we avoid aborts due to loss of orthogonality, the number of shifts is usually more
than the non-segmented run.
Frequency Domain Decomposition-Based Distributed Parallel Lanczos Method
The frequency domain decomposition-based normal modes analysis is built on the frequency segment
approach of the Lanczos method. The frequency range of interest given by the user is subdivided
automatically. The user also has the choice of introducing intermediate frequencies directly on the EIGRL
continuation card. This may be especially advantageous in case of repeated runs (the most practical situation),
where the user may be able to enforce better load balancing than the automatic intermediate frequency
generation method. This process is executed in a parent/child p aradigm. One (the parent)  of the processes&lt;br&gt;
will execute the coordination and collection of the separately calculated eigenvectors into an appropriately
ordered set. This guarantees the continuation and proper exit of the parent p rocess. The child p rocesses will
contain only the eigenvalues and eigenvectors they found upon exiting the READ module.
Geometric Domain Decomposition-Based Distributed Parallel Lanczos
The principle of geometric domain decomposition is not confined completely to the READ module as the
frequency domain decomposition. The geometric domain decomposition principle transcends many DMAP
modules prior to the READ module and the eigenproblem presented to READ is only a subset of the original
problem.
It is important to emphasize that this version still solves the global (see ) eigenvalue problem as &amp;ldquo;exactly&amp;rdquo; as
the single process r un. The significant difference is that the geometry and therefore the global matrices are
partitioned and the local processes  see only the local portions. In the current implementation, we restrict
ourselves to as many subdomains as we have processes . One might call this a &amp;ldquo;distributed global solution
technique.&amp;rdquo;
In general, there are two alternatives to execute this process. One is to keep only short, local  size
vectors and another is to also add a global  size vector to the local Lanczos processes. The first method,
while minimal in storage requirements, requires interprocess communication even when executing vector
operations such as the Lanczos step. The latter technique requires communications only when matrix
operations are executed, while it has  word redundancy on each process,  so this seems more attractive, and
is used in our implementation.
In the geometry domain parallel Lanczos method, the stiffness and mass matrices are partitioned into
submatrices based on the geometry subdomains as follows:nojnaj
na
na&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis142
Main Index
where the superscript j refers to the j-th subdomain, subscript a to the common boundary of the subdomains,
and s is the number of the subdomains, so . The size of these global matrices as well as the
eigenvectors is N. For the presentation of the algorithm, let us partition the Lanczos vectors accordingly:
Furthermore, the boundary portion may be partitioned as:KKoo1    Koa1
Koo2   Koa2
.  .
Kooj Koaj
..
Kao1Kao2.Kaoj.Kaa=
MMoo1    Moa1
Moo2   Moa2
.  .
Mooj Moaj
..
Mao1Mao2.Maoj.Maa=
j12s =
xxo1
xo2
.
xoj
.
xa=&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P8</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_008/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_008/</guid>
      <description>
        
        
        &lt;p&gt;143 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
i.e., the a set is the global boundary set; s is the number of subdomains. Finally, the j-th process  will receive
the j-th subdomain information of:
where the submatrices are partitioned into local o and a-sets and the appropriate sizes are  and .
The main computational elements of the Lanczos method are executed in the distributed form as follows.
Simultaneous Shifted Matrix Generation .  Since the inputs to the READ module are the  and
matrices, the task of creating the local system matrix will be simultaneously executed on each of the
processes  serially:
The  matrix is the distributed submatrix on the local memory of the j-th process  (node). Naturally, the
local mass matrix component will also be saved locally and, since it is needed in each Lanczos step, it will be
stored in local memory if possible. The shifted stiffness matrix will be stored on the local scratch disk of the
node.
Distributed Factorization .  The parallel, distributed implementation will execute the following steps:xaxa1
xa2
.
xaj
.
xas=
KoojKoaj
KaojKaajMoojMoaj
MaojMaajxoj
xaj 
nojnaj
Kj
Mj
Aj AoojAoaj
AaojAaajKoojKoaj
KaojKaaj0MoojMoaj
MaojMaaj–
     
= =
Aj
Important: The  shift is calculated from local information such as matrix norms and runtime
measures; therefore, it is process  dependent. Hence, some communication between
the nodes is required to assure that the shift is uniform across the processes .0&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis144
Main Index
1.The partial decomposition will formally decompose this j-th subdomain as follows:
where the identity matrices are not computed; they are only presented to make the matrix equation
algebraically correct and the  submatrix is the boundary submatrix of the j-th partition updated
by the factorization contributions from the interior as:
2.The boundary submatrices are summed up as:
3.The boundary is decomposed as:
The distributed decomposition step will be executed outside the Lanczos recurrence logic and the
resulting:
partial factor and the global boundary  factor will be utilized inside the recurrence.
Since one important function of the decomposition operation in the Lanczos framework is to
establish the Sturm count ( ), another interprocess  communication is needed:
where the Sturm counts in the equation above are the global, boundary, and local interior Sturm
counts in that order.Aj AoojAoaj
AaojAaajLooj0
LaojIDooj0
0AaajLooTjLoaTj
0 I= =
Aaaj
AaajAaajLaoj Dooj LooTj– =
Aaa Aaaj
j1=s
=
Aaa Laa Daa LaaT=
LLaoj Looj
Laoj=
Laa
S
SgSaSoj
j1=s
+ =&lt;/p&gt;
&lt;p&gt;145 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
Distributed Matrix Multiply .  This is the first operation where the fact of having  +  +  long
Lanczos vectors in all local processes, but executing a local  +  size matrix operation requires extra care.
The basic local matrix-vector multiply operation is:
where the local vector partitions are defined as:
and
Note that the  boundary partitions are the local subsets of the . The operation may be executed
in the following steps:
1.Execute
2.Scatter  to
3.Send  to parent p rocess
4.Parent s ums up
5.Receive  from parent p rocess
6.Gather  from
Operation 1. will be executed by the existing architecture, which consists of two phases. In subroutine phase
1 we save the M matrix in a sparse form, storing the indices and numerical terms separately. This in turn is
read in and interpreted by the 2nd phase that executes the actual numerical operation by calling the XPI2R* nojnajna
nojnaj
yj yoj
yajMoojMoaj
MaojMaajxoj
xajMjxj= = =
xxj
xaxoj
xaj
xa= =
yyj
yayoj
yaj
ya= =
xajyaj xaya
yjMjxj=
yajya
ya
ya
ya
yajya&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis146
Main Index
indexed, double SAXPY kernel. This phase does not need to understand the local versus boundary structure
of the  matrix.
In addition, the existing option of keeping a portion or all of the mass matrix in memory is also operational
in the distributed environment. The operations ( 3. through 5.) execute the communication and summing up
of the boundary segments between the processes  sharing the boundary. Finally, the scattering and gathering
of the local boundary segments ( 2., 6.) is straightforward.
Upon completing this step, all processes  have a shared and complete  vector, identical in content. This is
necessary to proceed with the local Lanczos process.
Distributed F-B Substitution .  This phase will contain the following elements:
1.The  factorization of the interior of the j-th subdomain is independent
of the boundary allowing a forward substitution on the interior as:
It is important to note that in this step the overlapping boundary regions are zeroed out on all
subprocesses, except for one, which will do the update.
2.The global boundary solution is a complete forward-backward substitution of:
3.Finally, the interior solution is a backward only substitution:
The local  vector is again partitioned as:
therefore, a scattering and gathering operation described in steps 2. and 6. of Distributed Matrix Multiply ,
145
Simultaneous Lanczos Step .  Since at this stage all processes  have their local  and
vectors (  refers to the Lanczos step number) as well as the last two Lanczos vectors ,
the Lanczos step will be executed simultaneously as follows:M
ya
LLaoTjLLooTj LLaoTj  =
LooTj zojLooj xaj+ Looj Dooj 1–  yojzoj= =
Laa Daa LaaT zayajLaoj Dooj zoj– 
j1=s
=
zojLooT–j zojLaoTj zaj–  =
z
zzj
zazoj
zaj
za= =
ykjMjxkj=
zkjAjxkj= k xkjxk1–j&lt;/p&gt;
&lt;p&gt;147 Chapter 6: Real Symmetric Eigenvalue Analysis
Theory of Real Eigenvalue Analysis
Main Index
1.Execute local inner product:
2.Create global inner product via communication:
3.Execute local saxpy:
In order to normalize the next Lanczos vector, another matrix vector multiplication is needed inside
of the Lanczos step (using the same method described in Distributed Matrix Multiply , 145.
The calculation of the normalization parameter follows steps 1. through 3. above.
This leads to the next normalized Lanczos vector
and the step is completed.
Distributed Orthogonalization Scheme .  While this operation does not explicitly involve matrix
operations, its performance impact is such that we have a distributed implementation. The main
orthogonalization tasks are: against the last two Lanczos blocks; against earlier found eigenvectors at the
initial phase and during the iteration, respectively; and, finally, within the current block.
The efficiency of the distributed orthogonalization scheme is coming from the fact that we can distribute
(vector) operations but need to communicate only  (scalar) data.
The distributed orthogonalization is executed in the following steps:
1.Calculate local inner products.
2.Exchange and sum up local inner products.kjykTjzkj=
kkj
j1=s
=
xk1+jzkjk xkj– k1– xk1–j– =
yk1+jMjxk1+j=
kjxk1+Tj yk1+j 12=
and
kkj12
j1=s
=
xk1+jxk1+jk =
Okn Ok&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Real Eigenvalue Analysis148
Main Index
3.Execute local saxpy step.
In step 1., each process  is to calculate:
where  is the set of selected Lanczos vectors. In step 2., the following global sum is needed on each process :
Finally, step 3. is executed in two segments, , as
and
followed by a gathering into the  array from
After this step, the Lanczos process may be continued again from the Distributed Matrix Multiply , 145. The process
continues on all the nodes until a certain number of steps are executed or convergence achieved, which
decision again needs communication..
Simultaneous Tridiagonal Solution Generation .  Since all process es have the Lanczos blocks, and
hence the same block tridiagonal matrix, the solution of the tridiagonal problem of  will be executed on all
nodes simultaneously. Once the tridiagonal solution has been completed, selected  will be accepted as
eigenvalues of the original problem and the corresponding  will become the basis to compute eigenvectors.
Note that the  vectors are the same  size on all the nodes. Since all the nodes have&lt;br&gt;
length Lanczos vectors , the eigenvector computation of the local segments will not
require additional communication.
The decision to stop the Lanczos run is based on analyzing the  matrix and cost analysis. Since the terms
of this matrix are identical on all nodes, the stop must be simultaneous, but is synchronized for safety. If more
eigenvectors are needed, another Lanczos run may be executed or ultimately the complete process starting
from Simultaneous Shifted Matrix Generation , 143 may be repeated with a different  shift value. If all required
eigenvectors are found, the final orthogonality test will be executed on the local matrices and vectors and the
READ module will complete by setting ND to the number of accepted eigenvectors. Note that upon exiting
the READ modules, all the processes  have a complete LAMA table; however, the PHIA matrix contains only
the local rows of the eigenvectors.ijxk1+Tj Mj xij=
i
iij
j1=s
=
xaxoj
xok1+jxok1+ji xoij– =
xak1+xak1+i xai– =
xak1+jxak1+

uk
ukk nojnajna+ +
xii1= 2k 
Tk
0&lt;/p&gt;
&lt;p&gt;149 Chapter 6: Real Symmetric Eigenvalue Analysis
Solution Method Characteristics
Main Index
The data recovery for each domain is done independently on each process  on the local segments of the
eigenvectors provided by READ. As this does not require any communication, it helps in the overall speedup
and disk-space savings during that part of the run. Following data recovery, the output may or may not be
merged based on the mergeofp keyword.
Solution Method Characteristics
The real eigenvalue solution methods are categorized as shown in Table 6-2:
DMAP User Interface
Input Data Blocks:Table 6-2  Real Eigenvalue Methods
Method Type Identifier Application Restriction
Givens Reduction GIV All Roots M Positive Definite
Householder Reduction HOU All Roots M Positive Definite
Modified Reduction Reduction All Roots
Not Singular
Lanczos Iteration LANC Small Number of
Roots Not Singular
M Positive
Semidefinite
SymmetricMHOUGIVAHOUGIV KsM +
KsM +
K
READ KAA,MAA,MR,DAR,DYNAMIC,USET,CASECC,
PARTVEC,SIL,VACOMP,INVEC,LLL,EQEXIN,GAPAR/
LAMA,PHIA,MI,OEIGS,LAMMAT,OUTVEC/
FORMAT/S,N,NEIGV/NSKIP/FLUID/SETNAME/SID/METH/
F1/F2/NE/ND/MSGLVL/MAXSET/SHFSCL/NORM/PRTSUM/MAXRATIO $
KAA Stiffness matrix.
MAA Mass matrix.
MR Rigid body mass matrix
DAR Rigid body transformation matrix.
DYNAMIC Eigenvalue Extraction Data (output by IFP module).
USET Degree-of-freedom set membership table.
CASECC Case Control Data Table (selects EIGR, EIGRL, or EIGB entries, output by IFP
module).&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
DMAP User Interface150
Main Index
Output Data Blocks:
Parameters:PARTVEC Partitioning vector with values of 1.0 at the rows corresponding to degrees of freedom
which were eliminated in the partition to obtain KAA and MAA. Required for
maximum efficiency. See SETNAME parameter description below.
SIL Scalar index list. Required for maximum efficiency.
VACOMP Partitioning vector of size of a-set with a value of 1.0 at the rows corresponding to r-set
degrees-of-freedom. The USET table may be specified here as well. If VACOMP is
purged and DAR does not have the same number of rows as KAA, then the partitioning
vector will be determined from the size of MR.
INVEC Starting vector(s) for Lanczos method only or EQMAP data blocks for geometry
domain parallel.
LLL Lower triangular factor from decomposition of KAA. Use to enhance shift logic for
buckling eigenvalue extraction or VF01: interior boundary partitioning vector for
geometry domain parallel Lanczos method.
EQEXIN Equivalence between external and internal grid identification numbers. Required for
maximum efficiency.
GAPAR Local-global boundary partitioning vector for geometry domain parallel Lanczos
method.
LAMA Normal modes eigenvalue summary table.
PHIA Normal modes eigenvector matrix in the a-set.
OEIGS Real eigenvalue extraction summary.
MI Modal mass matrix.
LAMMAT Diagonal matrix containing eigenvalues on the diagonal (Lanczos and QLHOU
only).
OUTVEC Last vector block (Lanczos only).
FORMAT Input-Character-no default. If FORMAT  ’MODES’, READ will solve a buckling
problem of .
NEIGV Output-integer-no default. NEIGV is the number of eigenvectors found. If none
were found, NEIGV = 0. If m modes were found (but error encountered), NEIGV
= -m. If m modes were found, NEIGV = m.
NSKIP Input-integer-default=1. The method used by READ is taken from the NSKIP
record of CASECC.
FLUID Input-logical-default=FALSE. If FLUID = TRUE, then the EIGRL or EIGR entry
is selected from METHOD(FLUID) Case Control command.KKd + &lt;/p&gt;
&lt;p&gt;151 Chapter 6: Real Symmetric Eigenvalue Analysis
DMAP User Interface
Main Index
SETNAME Input-character-default=&amp;lsquo;A&amp;rsquo;. For maximum efficiency, the rows and columns KAA
and MAA must correspond to or be a partition of the displacement set specified by
SETNAME. If KAA and MAA are a partition then PARTVEC must also be
specified.
SID Input-integer-default=0. Alternate set identification number.
If SID=0, the set identification number is obtained from the METHOD command
in CASECC and used to select the EIGR or EIGRL entries in DYNAMIC.&lt;br&gt;
If SID&amp;gt;0, then METHOD command is ignored and the EIGR or EIGRL is selected
by this parameter value. All subsequent parameter values (METH, F1, etc.) are
ignored.
If SID&amp;lt;0, then both the METHOD command and all EIGR or EIGRL entries are
ignored and the subsequent parameter values (METH, F1, etc.) will be used to
control the eigenvalue extraction.
METH Input-character-default=&amp;lsquo;LAN&amp;rsquo;. If SID&amp;lt;0, then METH specifies the method of
eigenvalue extraction.
LAN Lanczos
GIV Givens
MGIV Modified Givens
HOU Householder
MHOU Modified Householder
AGIV Automatic selection of GIV or MGIV
AHOU Automatic selection of HOU or MHOU
F1 Input-real-default=0.0. The lower frequency bound.
F2 Input-real-default=0.0. The upper frequency bound. The default value of 0.0
implies machine infinity.
NE Input-integer-default=0. The number of estimated eigenvalues for non-Lanczos
methods only. For the Lanczos method, NE is the problem size (default=20) below
which the QL Householder option is used if it is enabled.
ND  Input-integer-default=0. The number of desired eigenvalues.
MSGLVL Input-integer-default=0. The level of diagnostic output for the Lanczos method
only.
0 no output
1 warning and fatal messages
2 summary output
3 detailed output on cost and convergence
4 detailed output on orthogonalization&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Method Selection152
Main Index
Method Selection
EIGR Entry .  The method selection of any method may be performed with the EIGR Bulk Data entry
using the following format:
The SID is the set ID number corresponding to a METHOD command in the Case Control Section.
METHOD should be equal to any of the identifiers given in Solution Method Characteristics , 149. F1, F2 are
frequency minimum and maximum values specifying the boundaries of the user’s frequency range of interest.
NE and ND are the number of roots estimated and desired to be found, respectively. On the continuation
entry, the user can choose some normalization options, which are detailed in Option Selection , 153.
EIGRL Entry .  T o select the Lanczos method in details, the user should use the EIGRL  Bulk Data entry
with the following format:
The MSGLVL entry (0 through 3, default = 0) controls the amount of diagnostics output. MAXSET specifies
the maximum number of vectors in a block of the Lanczos iteration. It is also equivalent to or may be
overridden by the value of SYSTEM cell 263. The value of SHFSCL is an estimate for the location of the
first nonzero eigenvalue of the problem.
The following parameters are only used if the F1 and F2 frequency range is to be broken up into segments.
ALPH is the constant defining the modal distributions function ( Frequency Segment Option , 155). Its default
value is 1.0, which results in an even distribution of segments. NUMS is the number of segments in the MAXSET Input-integer-default=0. Vector block size for Lanczos method only.
SHFSCL  Input-real-default=0.0. Estimate of the first flexible natural frequency. SHFSCL
must be greater than 0.0. For Lanczos method only.
NORM Input-character-default=&amp;lsquo;MASS&amp;rsquo;. Method for normalizing eigenvectors. See Option
Selection , 153 for details.
PRTSUM Input-logical-default=TRUE. Lanczos eigenvalue summary print flag. See
Performance Diagnostics , 101 for details.
MAXRATIO Input-real-default= . May be overwritten in the DMAP by: param, maxratio,
value.105
EIGR SID METHOD F1 F2 NE ND
NORM G C
EIGRL SID F1 F2 ND MSGLVL MAXSET SHFSCL NORM
ALPH NUMS f1 f2 f3 f4 f5 f6
f7 f8 f9 f10 f11 f12 f13 f14
f15&lt;/p&gt;
&lt;p&gt;153 Chapter 6: Real Symmetric Eigenvalue Analysis
Option Selection
Main Index
frequency range (default = 1). f1 to f15 are segment boundaries such that F1&amp;lt; f1 &amp;lt; f2 &amp;hellip; &amp;lt; f15 &amp;lt; F2.  f1 to f15
if not specified will be computed based on a distribution given by ALPH.
Different combinations of F1, F2, and ND specify different options in the Lanczos module (see Frequency and
Mode Options , 153).
The main purpose of the SHFSCL is to aid the automatic shift logic in finding the eigenvalues especially in
crossing the (possibly big) gap between the computationally zero (rigid body) modes and the finite (flexible)
modes. Another use of SHFSCL is to create a cutoff frequency for the so-called supported modes. If a mode
is above SHFSCL, it is not supported even if it is flagged by a SUPORT entry.
The NORMalization parameter for Lanczos is described in Normalization Options , 153.
Option Selection
Real symmetric eigenvalue analysis offers several normalization, frequency and mode, and performance
options.
Normalization Options
The methods accessed by the EIGR entry have several normalization  options. They are:
The Lanczos method (using EIGRL) has MASS or MAX normalization capability only.
The following options are valid for the Lanczos method only unless otherwise stated.
Frequency and Mode Options
At present, based on the F1, F2, and ND combinations, the following options are supported in the Lanczos
algorithm:NORM = MASS Mass normalization of eigenvectors (normalize to unit value of generalized
mass).
NORM = MAX Maximum normalization of eigenvectors (maximum component of vectors
is unity).
NORM = POINT A selected point normalized to unity. The point is specified by G (grid point
number) and C (components 1 through 6).
Table 6-3&lt;br&gt;
F1 F2 ND Option
Given Given Given Lowest ND or all in range
Given Given Blank All in range
Given Blank Given Lowest ND in range (F1, + )
Given Blank Blank Lowest one in range (F1, + )&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection154
Main Index
Performance Options
Space Saver .  Another option available in Lanczos method is called the space saver option. This option is
selected by setting SYSTEM (229) = 1 (default = 0) and results in significant reduction in scratch space  usage
by not preserving factor matrices for later use in the case both F1, F2 are given. However, CPU-time may
increase.
Sparse Solver in Lanczos .  For faster CPU execution, the Lanczos method by default executes sparse
matrix operations. The memory available for the sparse solver inside the Lanczos module can be controlled
by setting SYSTEM (146) to greater than 1. The larger the number the larger the area reserved for the factor.
Recommended values are 2, 3, 4. SYSTEM(146) is equivalent to the FBSMEM keyword. This increased
memory space is taken from the space available for the eigenvectors; consequently, the user must find a
satisfactory compromise.
Low Level Parallel Lanczos .  Low level shared memory parallel execution is supported in the shift
(decomposition) and solve steps of the Lanczos method. These methods can be specifically selected by setting
the keyword SMPARALLEL = ncpu and deselected by setting SMPARALLEL = 7168 + ncpu, where ncpu
= the number of CPUs.
I/O Reduction Options .  Other performance-enhancing options which control the I/O to CPU time
ratio are described in Table 6-4:Blank Blank Given Lowest ND in (- , + )
Blank Blank Blank Lowest one in (- , + )
Blank Given Given Lowest ND below F2
Blank Given Blank All below F2
Table 6-4  I/O Reduction Options
System Performance Option
(199) = lSet memory for incore mass matrix multiply to 2   l  BUFFSIZE
(default: l = 1)
0Automatically fits the mass matrix in core if sufficient memory is available
(193) = 0Save
result of mass matrix multiply (default = 0)
1Do not saveTable 6-3&lt;br&gt;
F1 F2 ND Option&lt;/p&gt;
&lt;p&gt;155 Chapter 6: Real Symmetric Eigenvalue Analysis
Option Selection
Main Index
Accuracy Options .  The user may reset the Lanczos iteration tolerance by:
where k is the exponent of the Lanczos tolerance criterion if it is negative, or it is the exponent of the
maximum matrix factor diagonal ratio tolerated by Lanczos if it is positive.
It is also possible to reset the pivot criterion for the decomposition inside the READ module by SYSTEM(89)
= -k, resulting in  used.
Frequency Segment Option .  The frequency segment option is controlled as follows.
The number of frequency segments may be specified on the EIGRL entry (NUMS) or on the NASTRAN
entry (NUMSEG). In the case both are given, NUMS is set to NUMSEG. The intermediate frequencies may
be directly given (f1 &amp;hellip; f15) by the user on the EIGRL entry. It is also possible to specify ALPH on the EIGRL
entry and, in this case, the intermediate frequencies are automatically calculated by the formula shown in
Table 6-5.
Miscellaneous Options
In the READ module, a new shift logic was introduced for the Lanczos method. If you wish to use the old
method (delivered via the REIGL module), SYSTEM(273) must be set to a non-zero value. The default is
zero.
Incompatible DAR and KAA sizes: If the DAR matrix has less rows than KAA, an approximate size identity
matrix is generated inside READ and merged into DAR to produce a DAA matrix. This assures that the rigid
body modes included will have the same size as the flexible modes computed. This operation requires the
presence of the VACOMP data block. If VACOMP is purged, a fatal error exit is produced.
Parallel OptionsSYSTEM(123) = k
Table 6-5  Frequency Segment Specification
PARAMETERS Definition
NUMS Number of frequency spectrum subsegments
ALPH
Subsegment boundary:
Frequency domain: dmp = n on submittal
domain solver fdmodes (numdom = n) in executive deck
The numdom value in fdmodes is equivalent to SYSTEM(197) (also known as NUMSEG).10k–
fifmaxfmin– 1ALPH
100&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-i
–
1ALPH
100&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-NUMS
–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Option Selection156
Main Index
Mass Matrix Analysis Options
Indefinite Test
The mass matrix MAA is examined for all real vibration (non-buckling) eigenvalue problems. First and
foremost, MAA is checked to determine if it is indefinite. Under no circumstances will an eigenvalue analysis
proceed with an indefinite MAA. If MAA is indefinite, UFM 4646 is printed, which includes a User Action
suggesting the use of SYSTEM(304) to perturb the mass matrix, which may render it positive semi-definite.
If SYSTEM(304) is activated, a small value is added to the diagonal of MAA, prior to the indefinite test.
Then, if the indefinite test passes, the perturbed MAA is used in the subsequent eigenvalue analysis regardless
of extraction method.
Rank Test
A rank test is performed if the indefinite test is selected. The rank of MAA (NRANK) is determined in order
to detect the presence of infinite roots. The number of eigenvectors requested is restricted to min
(N,NRANK) for the tridiagonal methods (HOU,GIV). The rank test is controlled by SYSTEM(313), which
defines a maximum ratio of MAA diagonals to MAA-factor diagonals.
Density Control
None of the above tests is performed if it is estimated that the tests themselves would require too much time.
A test is made to determine if the density of MAA exceeds some threshold (default 10%). If MAA exceeds
the threshold, it is deemed &amp;ldquo;dense&amp;rdquo;; therefore, its factorization might be nontrivial, and these tests are
bypassed. The threshold is controlled by SYSTEM(314). If N (the size problem) is less than or equal to 100,
the density check is bypassed.
These analyses are summarized in Table 6-6:Geometry domain: dmp = n on submittal
domain solver gdmodes (numdom = n) in executive deck
The numdom value in gdmodes is equivalent to SYSTEM(349).
The default for both numdom values is SYSTEM(231) (also known as dmp(arallel)).&lt;/p&gt;
&lt;p&gt;157 Chapter 6: Real Symmetric Eigenvalue Analysis
Real Symmetric Eigenvalue Diagnostics
Main Index
QL Householder Option
If sufficient memory is available, a modern (QL iteration based) version of Householder methods (AHOU,
MHOU, or HOU) is automatically used. This is also used in place of Lanczos when the problem size is
smaller than the value of the NE parameter (default = 20). Setting SYSTEM(359) = 0 will turn off the QL
Householder option (default SYSTEM(359)=1).
Real Symmetric Eigenvalue Diagnostics
The diagnostics of the eigenvalue methods can be categorized as execution diagnostics, numerical diagnostics,
and error messages.
Execution Diagnostics
A certain level of execution diagnostics of the READ module is requested by DIAG 16. For the Lanczos
method, MSGLVL = 1, 2, or 3 in the EIGRL entry gives different levels of execution diagnostics. These
diagnostics pages are elaborate and are described in Lanczos Diagnostics .
The following two tables are printed only when PRTSUM = TRUE (default)
Table of Shifts
The table of shifts shows the sequence of shifts taken, the Sturm counts, and the number of modes computed
at each shift for the Lanczos method. It appears as follows:Table 6-6  Mass Matrix Analyses
System Cell Comment Description Default Value
303 Indefinite Test =0 cutoff=1.0e-6
&amp;lt;0 cutoff=10**sys303&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bypass test0
(Do the T est)
304 M Perturbation =0 perturb=1.0e-6
&amp;lt;0 perturb=10&lt;strong&gt;sys304
0 bypass perturbation+1
(Do not Perturb)
313 Rank Test =0 rank ratio=1.0e+7
0 rank ratio=10&lt;/strong&gt;sys304
&amp;lt;0 bypass test0
(Do the T est)
314 Density
Threshold=0 thresh=0.01
0 thresh=sys314/10000.
&amp;lt;0 do not check density0
(10% Threshold)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Numerical Methods User’s Guide
Real Symmetric Eigenvalue Diagnostics158
Main Index
Execution Summary .  The execution summary table for the Lanczos method is as follows:
EIGENVALUE ANALYSIS SUMMARY (REAL LANCZOS METHOD)
The TEXT of the termination message can be any of the following:
REQUIRED NUMBER OF EIGENVALUES FOUND
ALL EIGENVALUES FOUND IN RANGE
NOT ALL EIGENVALUES FOUND IN RANGE
INSUFFICIENT TIME TO FIND MORE EIGENVALUES
The run may also finish with the following message:
USER FATAL MESSAGE 5405, ERROR X OCCURRED DURING ITERATIONTABLE OF SHIFTS  (REIGL)
SHIFT # SHIFT
VALUEFREQUENCY,
CYCLES# EIGENVALUES
BELOW# NEW EIGENVALUES
FOUND
X1 X2 X3 X4 X5
X1 The shift number
X2 The shift value in eigenvalue units
X3 The shift value in frequency units (typically Hertz)
X4 The number of modes below this shift (the Sturm count)
If X4 is “FACTOR ERROR” then this shift is rejected because the MAXRATIO
is too large. T o override this, the user may set SYSTEM(166) = 4 and the factor
will be used despite the high MAXRATIO.
X5 The number of modes found by the algorithm at this shift
BLOCK SIZE USED X
NUMBER OF DECOMPOSITIONS X
NUMBER OF ROOTS FOUND X
NUMBER OF SOLVES REQUIRED X
TERMINATION MESSAGE: TEXT
where X can be equal to - 31: INSUFFICIENT WORKSPACE&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;32:  QL ALGORITHM DID NOT CONVERGE&lt;/li&gt;
&lt;li&gt;33:  MORE EIGENVALUES FOUND THAN EXIST&lt;/li&gt;
&lt;li&gt;34:  FILE I/O ERROR&lt;/li&gt;
&lt;li&gt;35:  SVD ALGORITHM DID NOT CONVERGE&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;159 Chapter 6: Real Symmetric Eigenvalue Analysis
Real Symmetric Eigenvalue Diagnostics
Main Index
Numerical Diagnostics
UWM 3034 :
ORTHOGONALITY TEST FAILED, LARGEST TERM = X NUMBER FAILED = PAIR = X, EPSILON
=X.
This message is printed when the eigenvector accuracy is in doubt (up to a certain numerical limit). This
message is given for all methods.
SFM 3034.2:
INTERNAL FAILURE IN THE LANCZOS PROCEDURE: M-ORTHOGONAL QR PROCEDURE
FAILED TO CONVERGE. PROBABLE CAUSE: MASS MATRIX IS INDEFINITE (MODES) OR
STIFFNESS MATRIX IS INDEFINITE (BUCKLING).
Indicates that the mass/stiffness matrix is indefinite or badly scaled.
UIM 5010:
STURM SEQUENCE DATA FOR EIGENVALUE EXTRACTION TRIAL EIGENVALUE = X,
CYCLES = X, NUMBER OF EIGENVALUES BELOW THIS VALUE = X.
This information is very important in establishing the number of roots existing in certain subregions of the
frequency region.
UFM 4646 :
THE MASS MATRIX IS NOT POSITIVE DEFINITE USING HOUSEHOLDER OR GIVENS
METHOD.
UFM 4645 :
THE SHIFTED STIFFNESS MATRIX IS NOT POSITIVE DEFINITE IN MGIVENS OR
MHOUSEHOLDER METHOD.
UFM 4648 :
THE MODAL MASS MATRIX IS NOT POSITIVE DEFINITE.
These messages report problems from the reduction methods.
UWM 5411 :
NEGATIVE TERM ON DIAGONAL OF MASS (NORMAL MODES) OR STIFFNESS (BUCKLING)
MATRIX.
This is a Lanczos diagnostic message for information only.
Error Diagnostics
UFM 5429 :
INSUFFICIENT TIME TO START LANCZOS ITERATION.
UFM 5400 :
INCORRECT RELATIONSHIP BETWEEN FREQUENCY LIMITS.
This means the upper frequency limit has a lower value than the lower frequency limit.
SFM 5401:
LANCZOS METHOD IS UNABLE TO FIND ALL EIGENVALUES IN RANGE. ACCEPTED
EIGENVALUES AND ADDITIONAL ERROR MESSAGES MAY BE LISTED ABOVE. A POSSIBLE&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Real Symmetric Eigenvalue Diagnostics160
Main Index
CAUSE IS THE OCCURENCE OF HIGH MAXRATIOS. CHECK MODEL FOR MECHANISMS IF
HIGH MAXRATIOS EXIST. USER ACTION: RERUN WITH ANOTHER METHOD OR
ANOTHER SETTING ON EIGRL ENTRY.
UFM 5402 :
THE PROBLEM HAS NO STIFFNESS MATRIX.
UFM 4683 :
MASS (OR STIFFNESS) MATRIX NEEDED FOR EIGENVALUE ANALYSIS.
UWM 6243 (REIG) :
THE DOF REQUESTED FOR POINT NORMALIZATION HAS NOT BEEN SPECIFIED ON THE
EIGR OR EIGB ENTRY.
SFM 5299 :
FINITE INTERVAL ANALYSIS ERROR or
STARTING BLOCK COMPUTATION ERROR or
INSUFFICIENT STORAGE FOR LANCZOS or
FACTORIZATION ERROR ON THREE CONSECUTIVE SHIFTS or
RESTORATION OF VECTORIZATION ERROR or
IMPROPER PARAMETER SPECIFICATION FOR LANCZOS or
TRUST REGION OVERFLOW IN LANCZOS or
UNRECOVERABLE TERMINATION FROM LANCZOS ITERATION
These messages issued under 5299 are from the Lanczos method. The first one, the finite interval analysis
error, is the one most frequently encountered. This error indicates a high matrix-factor diagonal ratio at the
F1 or F2 shifts which can be caused by a modeling error or matrix singularity.
SFM 5407 :
INERTIA (STURM SEQUENCE) COUNT DISAGREES WITH THE NUMBER OF MODES
ACTUALLY COMPUTED IN AN INTERVAL.
This message flags a serious problem, i.e., spurious modes were found in the Lanczos method.
UWM 5406 :
NO CONVERGENCE IN SOLVING THE TRIDIAGONAL PROBLEM.
This message signals the abortion of the reduction methods.
Performance Diagnostics
UIM 5403:
BREAKDOWN OF CPU USAGE DURING LANCZOS ITERATIONS
Eigenvalue analysis can be computationally expensive and may dominate overall CPU time. T o help assess
numerical performance, this message shows how much time the primary operations (forward-backward
substitution, matrix-vector multiplication, and matrix summation and decomposition) consume during
Lanczos iterations. The last entry, “LANCZOS RUN”, refers to the duration of a complete set of Lanczos
cycles at one shift and contains within it all FBS and matrix multiplication times, but not the shift and factor&lt;/p&gt;
&lt;p&gt;161 Chapter 6: Real Symmetric Eigenvalue Analysis
Real Symmetric Eigenvalue Diagnostics
Main Index
times. The sum of total times for “SHIFT AND FACTOR” and “LANCZOS RUN” should then
approximate the total time taken by the REIGL or LANCZOS modules.
UIM 2141:
GIVENS (OR HOUSEHOLDER) TIME ESTIMATE IS X SECONDS. SPILL WILL OCCUR FOR
THIS CORE AT A PROBLEM SIZE OF X.
The reduction type methods are fairly predictable (not a characteristic of other eigenvalue methods). The
CPU time and storage estimate are given in this message.
Lanczos  Diagnostics
Since the Lanczos method is the most robust and modern of the eigenvalue extraction methods, its execution
diagnostics are described here in greater detail.
The printing of diagnostic information is controlled by the MSGLVL parameter. When left at its default
value of zero, only the Table of Shifts and the Execution Summary block are printed in addition to the
eigensolution. MSGLVL values of 1, 2, 3, or 4 yield increasingly more detailed diagnostic information about
the Lanczos operations.
The MSGLVL=1 diagnostics are organized into four major sections. Section I reports on the original problem
specification and the setting of module parameters. In this section an interval analysis check is also shown to
establish the number of eigenvalues in the range set by the user.
Most of the detailed diagnostics are self explanatory. Some of the parameter values are detailed below:
The LEFT and RIGHT END POINTS are the F1, F2 values set on the EIGRL entry. The center frequency
is the center (not necessarily the arithmetic center) of the interval.
ACCURACY  REQUIRED is a value automatically set by the program.*** USER INFORMATION MESSAGE 5403 (LNNRIGL)
BREAKDOWN OF CPU USAGE DURING LANCZOS ITERATIONS:
OPERATION               REPETITIONS  AVERAGE      TOTAL
FBS (BLOCK SIZE=&amp;hellip;)        &amp;hellip;. &amp;hellip;&amp;hellip;..   &amp;hellip;&amp;hellip;..
MATRIX-VECTOR MULTIPLY      &amp;hellip;. &amp;hellip;&amp;hellip;..   &amp;hellip;&amp;hellip;..
SHIFT AND FACTOR            &amp;hellip;.                       &amp;hellip;&amp;hellip;..   &amp;hellip;&amp;hellip;..
LANCZOS RUN &amp;hellip;.                       &amp;hellip;&amp;hellip;..   &amp;hellip;&amp;hellip;..
MODE FLAG =1 Vibration problem
2 Buckling problem
PROBLEM TYPE =1 Lowest ND roots in interval
2 Highest ND roots
3 All roots in interval
4 ND roots nearest to center frequency&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Real Symmetric Eigenvalue Diagnostics162
Main Index
The CP TIME ALLOWED is the remainder of the time left to complete the run using the limit set on the
TIME entry.
The SHIFTING  SCALE is an estimate of the smallest (in magnitude) nonzero eigenvalue in the spectrum.
This estimate can be specified by the user or automatically calculated by the program. The INERTIA values
at the specific locations are Sturm numbers.
Section II provides diagnostics on the memory usage, the setting of the working array sizes based on the
memory available, and the maximum memory allocated. The term RITZ VECTORS means the approximate
eigenvectors. A TRUST REGION is a segment of the frequency spectrum, bounded by two shifts where all
the eigenvalues are found. By the nature of the automatic shift algorithm, there can be several of these
segments.
Section III is the Lanczos run section. The text shown in this section can be repeated for each new shift. This
section also prints occasional user information messages (for example, 5010 and 4158) that report the results
from the decomposition module. This section is further expanded when the user requests additional
diagnostics from the Lanczos run by setting MSGLVL = 2 or 3.
Section IV reports the conclusion of the Lanczos run. The most frequently occurring warning flag settings
are listed below:
MSGLVL = 2 provides detailed information about the shift logic, the spectrum distribution, and the cause
of the termination of a Lanczos run. This TERMINATION CRITERION  may have the following values:COMPLETION FLAG =- 99 ND roots nearest to center frequency&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;26 Starting block computation error&lt;/li&gt;
&lt;li&gt;25 Restoration of factorization error&lt;/li&gt;
&lt;li&gt;24 Illegal parameters error&lt;/li&gt;
&lt;li&gt;23 Core structure error&lt;/li&gt;
&lt;li&gt;22 Internal factorization error&lt;/li&gt;
&lt;li&gt;21 Insufficient storage&lt;/li&gt;
&lt;li&gt;20 Factorization error at a boundary shift&lt;/li&gt;
&lt;li&gt;5 Incorrect frequency range&lt;/li&gt;
&lt;li&gt;2 No eigenvalues found in range
0 Required number of roots found
1 All roots in interval found
2 Not all roots in interval found
3 Insufficient time to finish
4 - 7 Same as 1- 3; however, the inertia count error
(see SFM 5407) occurred during the iteration&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>User&#39;s Manual P9</title>
      <link>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_009/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>//uildDrafts:1313/docs/msc_nastran_2024.1_numerical_methods_user_guide/msc_nastran_2024.1_numerical_methods_user_guide_009/</guid>
      <description>
        
        
        &lt;p&gt;163 Chapter 6: Real Symmetric Eigenvalue Analysis
Real Lanczos Estimates and Requirements
Main Index
Finally, MSGLVL = 3 describes the Lanczos run including norms, condition numbers, convergence criterion,
shifted eigenvalues, and their residuals.
Real Lanczos Estimates and Requirements
The time estimates  for the Lanczos method are the following:
Shifting time (sec):
Recursion time (sec):
Ortho-normalization time (sec):
Packing time (sec):
where:0 Preset maximum number of Lanczos steps reached
1 Cost of eigenvalue calculation increasing
2 Number of needed eigenvalues was found
3 Shift is too close to an eigenvalue
4 Lanczos block is singular
5 Running out of time&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 Insufficient workspace&lt;/li&gt;
&lt;li&gt;2 QL algorithm does not converge&lt;/li&gt;
&lt;li&gt;3 Too many eigenvalues found&lt;/li&gt;
&lt;li&gt;4 File I/O error&lt;/li&gt;
&lt;li&gt;5 Singular value decomposition error
=number of shifts
=number of Lanczos steps
=block size used
=number of modes desiredTdNshifts
NstepsNshifts2TMTs+  
2 NshiftNsteps2M 
2 NdesNsteps+ NP
Nshifts
Nsteps
B
Ndes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Numerical Methods User’s Guide
References164
Main Index
The minimum storage requirements are as follows:
where  is the factor disk space requirement.
The number of shifts in most Lanczos runs is 2, occasionally 1, sometimes 3 or more. The number of Lanczos
steps is 10 on the average.
References
Givens, W. Numerical Computation of the Characteristic Values of a Real Symmetric Matrix . Oak Ridge
National Lab., ORNL-1574, 1954.
Grimes, R. G., et al. A Shifted Block Lanczos Algorithm for Solving Sparse Symmetric Generalized
Eigenproblems . SIAM, J. Mat. Analysis Appl., 13, 1992.
Householder, A.S.; Bauer, F .L. On Certain Methods for Expanding the Characteristic Polynomial.
Numerische Mathematik, Volume 1, 1959, pp. 29-37.
Lanczos, C. An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and
Integral Operators.  Journal of the Research of the National Bureau of Standards., Volume 45, 1950, pp. 255-
282.
Lewis, J. G.; Grimes, R. G. Practical Lanczos Algorithms for Solving Structural Engineering Eigenvalue
Problems. Sparse Matrices, and Their Uses, edited by I. E.Duff, Academic Press, London, 1981.
MacNeal, R. H.; Komzsik, L. Speeding Up the Lanczos Process.  RILEM, Kosice, Slovakia, 1995.
Ortega, J. M.; Kaiser, H. F . The LR and QR Methods for Symmetric Tridiagonal Matrices.  The Computer
Journal, Volume 6, No. 1, Jan. 1963, pp. 99-101.
Parlett, B. N. The Symmetric Eigenvalue Problem . Prentice Hall, Englewood Cliffs, 1980.
Smith, B. T . et al. Matrix Eigensystem Routines - EISPACK Guide.  Springer Verlag, 1974.
Wilkinson, J. H. The Algebraic Eigenvalue Problem . Oxford University Press, 1965.
Wilkinson, J. H. The Calculation of the Eigenvectors of Codiagonal Matrices.  The Computer Journal,
Volume 1, 1958, p. 90.=decomposition time (see Decomposition Estimates and Requirements , 71 for details)
=solution time (see FBS Estimates and Requirements, 82  for details)
=matrix multiply time (see MPYAD Estimates and Requirements , 55 for details)
Disk:
Memory:Td
Ts
TM
NdesNIPRECDfactor+ 
2NstepsB 2IPREC4 BNIPREC  +
Dfactor&lt;/p&gt;
&lt;p&gt;Main Index
Chapter 7: Complex Eigenvalue Analysis
7 Complex Eigenvalue Analysis
Damped Models
Theory of Complex Eigenvalue Analysis
Solution Method Characteristics
User Interface
Method Selection
Option Selection
Complex Eigenvalue Diagnostics
Complex Lanczos Estimates and Requirements&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Damped Models166
Main Index
Damped Models
The solution of complex eigenvalue  problems is important for damped models. The solution method is
applied when either viscous or structural (or both) damping is used.
The basic equation of the complex eigenvalue analysis is
(7-1)
where:
The  matrix may be purged and the  and  matrices may be real or complex, symmetric or
unsymmetric.
The eigenvalue  is given by
(7-2)
The complex eigenvalue and eigenvector solution is of the form:
(7-3)
Theory of Complex Eigenvalue Analysis
Canonical Transformation to Mathematical Form
The complex eigenvalue analysis problem in general is a quadratic eigenvalue problem in the form:
(7-4)
where  is the displacement vector.  is the acceleration of the grid points, i.e., the second time derivative
of .  refers to the velocity or first time derivative. The solution of this homogeneous system (the free, but
damped vibrations) is of the form
(7-5)
where  is a vector of complex numbers and the  eigenvalue is also complex in general. By substituting
(7-5) into (7-4) we get:
(7-6)=mass matrix
=damping matrix
=stiffness matrixMu··Bu·Ku0= + +
M
B
K
B M K

 ai+=
uet=
Mu··Bu·Ku0= + +
u u··
uu·
u et  =
 
M2BK + +   0=&lt;/p&gt;
&lt;p&gt;167 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
In order to solve the quadratic eigenvalue problem, first a linearization transformation is executed. This
transformation converts the original quadratic problem to a linear problem of twice the size.
It is obtained by simply rewriting (7-6) as a 2  2 block matrix equation:
(7-7)
where:
(7-8)
This equation is now linear; however, there are shortcomings. Independently of the eigenvalue solution
method, one would need to invert both the mass and damping matrices and an unsymmetric, indefinite
matrix built from the damping and stiffness matrices, in order to reach a solution. Although the explicit
inverses are not needed, the numerical decompositions on either of these matrices may not be very well
defined.
An advancement is possible by executing a spectral transformation, i.e., introducing an appropriate shift as:
(7-9)
With the shift (whose appropriate selection in the complex case is rather heuristic) the linear equation may
be rewritten as:
(7-10)
Another recommended improvement is to invert the problem by introducing:
(7-11)
By substituting and reordering we get:
(7-12)
The latter equation is a canonical form of
(7-13)
where:M0
0I·
BK
I–0·
0= +
·
=
0+ =
BM0–– K–
I 0I–·
M 0
0I·
=
1
&amp;mdash;=
·
BM0–– K–
I 0I–1–
M 0
0I·
=
x Ax=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis168
Main Index
(7-14)
The form allows the singularity of the mass matrix. However, the zero subspaces of , , and  may not
coincide. This is a much lighter and more practical restriction. This is the formulation used in the Lanczos
methods.
If  is nonsingular, then a simplified formulation of
(7-15)
also results in a canonical form of
(7-16)
where:
This is the formulation used in the Hessenberg methods.
When  is null, then the double-size representation  can be avoided.
The problem then becomes
(7-17)
Using the following:
(7-18)
We can write
(7-19)
T o premultiply (7-19), we use
(7-20)
Therefore, the  dynamic matrix  for the case of nonsingular  matrix is as follows:ABM0–– K–
I 0I–1–
M 0
0I=
KB M
M
M1–B0I+  – M1–K –
I 0I–·
·
=
Axx=
x·
=
B
M2K+  0=
20+ =
M0K+  M =
M1–K0I+   – =
A M&lt;/p&gt;
&lt;p&gt;169 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
(7-21)
Because of the unsymmetric nature, the following so-called left-handed solution also exists:
(7-22)
For the special case of the structural damping problem, the left-handed solution of form:
(7-23)
also exists.
In (7-22) and (7-23), the superscript  stands for complex conjugate transpose.
The left-handed eigenvectors of the problems will only be found in the Lanczos and QZ Hessenberg
methods. The left-handed eigenvectors are useful in establishing convergence quality.
The physical eigenvalues may easily be recovered from the back substitution of the shift and invert operations:
(7-24)
In order to find the relationship between the mathematical and physical eigenvectors, let us rewrite (7-7) in
the following block notation:
(7-25)
where again
(7-26)
The block matrices are simply:
(7-27)
and         when B0=AM1–B0 I + M1–K –
I0 I   –
M1–K0 I +  –
=         when B0
H2MBK + +   0=
H2M K+  0=
H
1
&amp;mdash;-0+ =
M K+ x0=
x·
=
KBK
I–0=&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis170
Main Index
(7-28)
Substituting (7-24) into (7-25) and reordering yields:
(7-29)
which is the same as (7-11) in the block notation. This proves that the right eigenvectors are invariant under
the shifted, inverted transformations, i.e., the right physical eigenvectors are the same as their mathematical
counterparts, apart from the appropriate partitioning.
For the left-handed problem of (7-22), we rewrite (7-22) using the block notation:
(7-30)
with a left-handed physical eigenvector of:
(7-31)
Substituting (7-24) again and by factoring:
(7-32)
By grouping:
(7-33)
which is equivalent to:
(7-34)
In this case, the mathematical problem we solve is:
(7-35)
This means that the left-handed physical eigenvectors are not invariant under the transformation. Comparing
gives the following relationship:
(7-36)
or expanding into the solution termsMM0
0I=
K0+ M 1–MI+   x0=
yHM K+ 0=
yH·HH =
yHK0 M + K0 M + 1–MI+   0=
K0 M + Hy  H
MK0 M +  + 0=
K0 M + Hy  H
K0 M + 1–MI+   0=
yHK0 M + 1–MI+   0=
yHK0 M + Hy  H&lt;/h1&gt;&lt;p&gt;171 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
(7-37)
and finally
(7-38)
The cost of this back-transformation is not very large since the factors of the dynamic matrix are available
and we need a forward-backward substitution only. The importance of this back-transformation is rather
academic since there is no physical meaning associated with the left physical eigenvectors. On the other hand,
these are the eigenvectors output in the DMAP data block PSI, and they are not orthogonal to PHI unless
properly converted.
Dynamic Matrix Multiplication
In any eigenvalue solution method, the  dynamic matrix times a vector (block) multiplication is the most
time consuming. From (7-13), the  matrix is
(7-39)
From its structure it is clear that the matrix does not need to be built explicitly. In the following, the implicit
execution of the dynamic matrix multiplication in both the transpose and non-transpose case is detailed.
In the non-transpose case, any  operation in the recurrence will be identical to the
(7-40)
solution of systems of equations. Let us consider the partitioning of the vectors according to the A matrix
partitions:
(7-41)
Developing the first row:yB –  M0– K–
I 0I–H
y –=
yB–  M0– K–
I 0I–1–H
y –=
A
A
AB  M– 0– K–
I 0I–1–M 0
0 I=
z Ax=
B  M– 0– K–
I0I–zM 0
0 Ix =
B  M– 0– K–
I0I–z1
z2M 0
0 Ix1
x2=&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis172
Main Index
(7-42)
Developing the second row, we have:
(7-43)
Substituting the latter into the first row, we get:
(7-44)
Some reordering yields:
(7-45)
The latter formulation has significant advantages. Besides avoiding the explicit formulation of , the
decomposition of the 2N size problem is also avoided.
It is important that the transpose operation be executed without any matrix transpose at all. In this case, any
operation in the recurrence will be identical to the
(7-46)
operation. Let us introduce an intermediate vector :
(7-47)
Now partitioning these vectors according to the matrix partitions and transforming we obtain:
(7-48)
Developing the first row:
(7-49)
Developing the second row, we have:
(7-50)B– M 0–  z1K z2M x1= –
z10 z2x2+ =
B– M 0–  0 z2x2+ K z2M x1= –
K0 B02 M + +   –  z2M x1B0 M +  x2+ =
A
yTxTA =
yTxTB  M– 0– K–
I0I–1–M 0
0 I=
z
zTxTB  M– 0– K–
I0I–1–&lt;/h1&gt;&lt;p&gt;x1
x2B  M– 0– K–
I0 I –Tz1
z2=
x1B  M– 0– Tz1z2+ =
x2KTz1– 0 z2– =&lt;/p&gt;
&lt;p&gt;173 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Expressing  from the first equation, substituting into the second and reordering yields:
(7-51)
or in the solution form:
(7-52)
The lower part of the z vector is recovered from (7-49) as:
(7-53)
Finally
(7-54)
Physical Solution Diagnosis
From the eigenvalue solution it will be guaranteed that the left and right mathematical eigenvectors are bi-
orthonormal:
(7-55)
where  is an identity matrix in essence with computational zeroes as off diagonal terms.
Based on the physical eigenvalues recovered by the shift formulae and the physical eigenvectors, another
orthogonality criterion can be formed. Using the left and right solutions, the following equations are true for
the problem:
(7-56)
(7-57)
By appropriate pre- and postmultiplications, we get:
(7-58)
(7-59)
A subtraction yields:z2
x20 x1KT0 BT02 MT+ +  z1–= +
z1KT0 BT02 MT+ +  1–x20 x1+  –=
z2x1B0 M + Tz1+ =
yTzTM 0
0 I=
YHX I XYH= =
I
Mi2BiK + +  i0=
jHMj2BjK + +   0=
jHMi2BiK + +  i0=
jHMj2BjK + +  i0=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis174
Main Index
(7-60)
Assuming  we can shorten and obtain a mass orthogonality criterion as:
(7-61)
The  matrix has (computational) zeroes as off diagonal terms (when ) and nonzero (proportional to
) diagonal terms.
By premultiplying (7-56) by , postmultiplying (7-57) by  and subtracting them we obtain:
(7-62)
By expanding and cancelling we get:
(7-63)
Assuming again that  we can shorten and obtain another orthogonality condition recommended
mainly for the structural damping option as:
(7-64)
This orthogonality matrix will also have zero off-diagonal terms, but nonzero (proportional to ) diagonal
terms.
Hessenberg Method
The method utilized in MSC Nastran uses the  Householder reduction to upper Hessenberg form followed
by Francis’s QR steps and a direct eigenvector generation scheme. To distinguish from the QZ method, this
is called the QR Hessenberg method.
Householder Transformations .  An  by  matrix with the  following form:
(7-65)
is introduced in Theory of Real Eigenvalue Analysis , 106 as a Householder transformation or reflection. These
matrices are symmetric and orthogonal and capable of zeroing out specified entries or any block of vector
components. We generate a Householder matrix for a given nonzero vector :jHMii2j2–jHBiij–0= +
ji
O1jHMiij+jH+ Bi=
O1ij
2i
j jHi i
jjHMj2BiK + +  ijH– Mj2BjK + +   i i0=
ij+jHMi i jji+j Ki0= +
ji
O2j jHMi i jHK i– =
i2
n n
P I2vvT
vTv&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; –=
x&lt;/p&gt;
&lt;p&gt;175 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
(7-66)
so that for  the elements from  to  of the transformed vector are  zero; i.e.,
(7-67)
The procedure is as follows. We calculate
(7-68)
and build
(7-69)
It can be proven that if  is defined as in (7-65), then
(7-70)
which is the form described in (7-67)Equation (7-67). Similarly the matrix update:
(7-71)
It is shown that  need not be formed explicitly if  and  are available.
Implicit Matrix Update .  For given values of  , , and , the following algorithm in MSC Nastran
overwrites  with , where
For&lt;br&gt;
For
End loop .
End loop .xTx1xn  =
1kjn k1+ j
Px x1xk00xj1+xn  T=
a2xk2 xj2+ + =
vT00xka+  sign xk xk1+xj00  +  =
P
Px x1xk1– sign –   xka00xj1+xn   =T
PA IvvT– A A vATvT– = =
P v 2vTv=
Av
A PA
P IvvT–  =
p 1n =
s vkAkp vjAjp + + =
ss=
ikj =
Aip Aips vi – =
i
p&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis176
Main Index
Householder Reduction to the Hessenberg Form .  Let  be the general matrix on which the
transformation must be executed. Consider the following transformation:
(7-72)
where:
The elements of  are chosen so that  has zeroes in the positions 1 through  in the r-th row. The
configuration and partitioning of the  can be shown as:
where:
The transformation matrix can be partitioned as follows:Note:    above. An analogous algorithm is used for the  update. vT0vkvj00    = AP&lt;/h1&gt;&lt;p&gt;=
=1
=a submatrix of order
=of upper Hessenberg form
=a square matrix order  (part of original matrix)
=a vector having  componentsA
ArPr Ar1– Pr=
A0A
PrI2wr wtT–
wrTwr
wrArr2–
Ar1–
r
nr–xxxxxx
xxxxxx
xxxxx
xxxx
xxxx
xxxxHr1– Cr1–
0br1–Br1–= Ar1– =
Cr1–r
Hr1–
Br1–nr–
br1–nr–
rI 0
0Qr I 0
0I2vrvrT–= Pr =
nr–&lt;/p&gt;
&lt;h1&gt;177 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
where  is a unit vector of order .
By executing the multiplication given in the right-hand side of (7-72), the following  is obtained:
where .
If  is chosen so that  is null except for its first component, then  of the order  takes the
Hessenberg form.
In-Memory Algorithm .  This formulation can be  developed by writing  as follows:
(7-73)
where  is a unit vector of the order  with zeroes as the first  elements. By further changes:
(7-74)
where:
Because of the lack of symmetry, the pre- and postmultiplications in (7-72) must be considered separately. The
premultiplication takes the following form:
(7-75)
The postmultiplication is as follows:=&lt;/h1&gt;&lt;p&gt;=
=vrnr–
Ar
rHr1–    Cr1–Qr
0 crQrBr1–QrTAr =
nr–
crQr br1–=
vrcrHrr1+
Pr
PrI2wrwrT–=
wrn r
PrIururT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –=
uir0i12r  =
ur1r+arr1+St
uirairir2n +=
2Kr2Sr2  ar1+r Sr
PrAr1–IururT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –

Ar1–Ar1–ururAr1–
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;Fr= – = =&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis178
Main Index
(7-76)
Because  has zero elements in the 1 through  positions, it is seen that the premultiplication leaves the
first  rows of  unchanged, while the postmultiplication leaves the first  columns unchanged. By
introducing new intermediate vectors, the following may be written:
(7-77)
where  has its first  elements as zero. This results in
(7-78)
For the postmultiplication, the  vector is introduced
(7-79)
which has no zero components. Finally,
(7-80)
Two-Level Storage (Spill) Algorithm .  If the execution of memory transfers is a requirement, the
following formulation is more convenient. The vector  is defined as in (7-77)
(7-81)
However,  is as follows:
(7-82)
By scaling elements of , the following is obtained:
(7-83)ArFr PrFrIur urT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- –
    
= FrFr ururT
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; – = = =
urr
r Ar1–r
urTAr1–prT=
prTr1–
FrAr1–ur
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;-
prT– =
qr
Fr urqr=
ArFrqrur
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;-
T
– =
pr
prTurT Ar1–=
qr
qrAr1– ur=
ur
vrur
2Kr2&amp;mdash;&amp;mdash;&amp;mdash;- =&lt;/p&gt;
&lt;p&gt;179 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Finally, introducing the scalar  as follows:
(7-84)
(7-72) now can be written in the following form:
(7-85)
QR Iteration Using the Householder Matrices
It is proven (Wilkinson, 1965 ) that the general  matrix can be factorized into the product of a unitary
matrix  and an upper triangular matrix . The algorithm at the r-th stage is as follows:
(7-86)
By including the successive iterates,
(7-87)
From the following form of (7-86):
(7-88)
it is seen that  is unitarily similar to . In general,  tends to take the upper triangular form. The
matrix  can be the product of  elementary unitary transformations necessary to reduce  to the
upper triangular form . The transformation matrices can be Givens rotations or Householder reflections.
In MSC Nastran the latter Householder reflections are used. The shift of origin may be incorporated into the
single  logic as follows:
(7-89)
If the matrix has  complex conjugate eigenvalues, the most economical way is to execute two steps (double
step). For example, the first  double shift with shifts  and  can be written as follows:ar
arprTvr=
arAr1–vr prT– qr vrT prTvrur vrT+ – =
Ar1–vr prT– qrar ur–  vrT– =
A
Q R
ArQr Rr=
Ar1+QrHAr QrQrH Qr Rr QrRr Qr= = =
Ar1+QrHAr QrQrHQi1–HQ1HA1 Q1Q2Qr      = =
Q1Q2Qr   Ar1+A1Q1Q2Qr   =
ArA1Ar
Qrn1– Ar
Rr
QR
Arkr IQr Rr= –
Ar1+Rr Qrkr I + =
k1k2&lt;/p&gt;
&lt;h1&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis180
Main Index
(7-90)
and it follows that
(7-91)
By introducing
(7-92)
the following is obtained:
(7-93)
or
(7-94)
which means that  is the orthogonal matrix that reduces  to the upper triangular
form.
Implicit QR Step .  Using the Householder matrices again,
(7-95)
where
The derivation of  can be produced as follows. First we create a Householder matrix which provides the
following:
(7-96)
where:&lt;/h1&gt;&lt;p&gt;=
=A1k1 IQ1 R1= –
A2k2 IQ2 R2= –R1 Q1k1 IA2= +
R2 Q2k2 IA3= –
A3Q1 Q2HA1 Q1 Q2=
Q  Q1 Q2=
R R1 R2=
QR A1k1 I – A1k2 I –  =
QTA1k1 I – A1k2 I –  R=
QTA1k1 I – A1k2 I – 
PrI2wrwrT–=
wr = 000xx   
A3
P1 x ke1=
xTx1y1z100   
x1a11k1–a11k2–a12 a21+
y1a21a11k2–a22k1–a21+&lt;/p&gt;
&lt;p&gt;181 Chapter 7: Complex Eigenvalue Analysis
Theory of Complex Eigenvalue Analysis
Main Index
Then we compute the following:
(7-97)
(See the update with Householder matrices in Theory of Real Eigenvalue Analysis , 106.)
The  matrix no longer takes the Hessenberg form. Therefore, this matrix must be reduced by new
Householder matrices:
(7-98)
Now the question is how to formulate the  matrices. The nonzero elements of the  Householder
matrices are determined by  for  and by  for . Convenient
representation of  (see also Theory of Real Eigenvalue Analysis , 106 on Householder method) for the current
case is as follows:
(7-99)
Then
(7-100)
For the update, the algorithm described in Theory of Real Eigenvalue Analysis , 106 is used.=
=first unit vectorz1a32 a21
e1
C1P1 A1 P1=
C1
Pn2–P2P1A1P1P2Pn2– A3=
PrPr
x1y1z1 P1xryrzr P2Pn2–
Pr
PrI2vrvrT
vrTvr&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; –=
vrT001urvr00    =
uryr
xr  ar&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
vrzr
xr  ar&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- =
2
vrTvr&amp;mdash;&amp;mdash;&amp;mdash;-2
1ur2vr2+ + &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;-r= =
PrIvrr vrT –=&lt;/p&gt;
&lt;p&gt;Numerical Methods User’s Guide
Theory of Complex Eigenvalue Analysis182
Main Index
Eigenvector Computation
The Hessenberg form is convenient for generating eigenvectors when the eigenvalues are known. The k-th
eigenvector of the Hessenberg matrix  can be found by solving the following:
(7-101)
where  is the k-th eigenvalue.
This equation in detailed form is as follows:
(7-102)
The matrix of the system is singular, but the minor corresponding to the upper right corner element  is
not. Therefore, we normalize the  vector so that the last element in it is equal to 1.
(7-103)
The next to the last element in this case is given by solving the following:
(7-104)
which yields the following:
(7-105)
The other elements can be evaluated in recursive form from:
(7-106)
Then the -th element,  is as follows:
(7-107)H
Hk I – yk0=
k
h11k– h12h13h1n
h21h22k– h23h2n
hii1–hiik– hin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  hnn1–hnnk–yk1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;yki
ykn1–
ykn0=
&amp;hellip;
h1n
yk
ykn 1=
hnn1– yk n1–hnnyk–k n0= +
yk n1–1
hnn1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash;khnn–  =
hii1– yk i1–hiik–  yk i hil ykl
li1+=n
= + +
i1– in1n22 ––=
yki1–1
hii1–&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;khii–  yk i hil ykl
li1+=n
– =&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
